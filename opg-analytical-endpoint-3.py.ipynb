{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69b5b253-5e7f-48b0-a413-e7ad5b5b95bc",
   "metadata": {},
   "outputs": [],
   "source": [
<<<<<<< HEAD
<<<<<<< HEAD
<<<<<<< HEAD
    "#!pip install pydbtools\n",
    "#!pip install lovely_logger "
=======
=======
>>>>>>> main
=======
>>>>>>> 460ae4060501c8bed7396e0dd0f17b383b42546c
    "!pip install pydbtools\n",
    "!pip install lovely_logger "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a4de770-8bef-418f-a6f7-f725ffa55597",
   "metadata": {},
   "outputs": [],
   "source": [
    "#==============================================================================\n",
    "# OPG Sirius Data Processing (OPG Analytical)\n",
    "# @author: Richard Ingley\n",
    "# MoJ Data Science Hub\n",
    "#==============================================================================\n",
    "\n",
    "# OPG Analytical have specified a schema that satisfies most of their \n",
    "# requirements for Sirius data cuts. This code to pull that data from Athena, \n",
    "# chunk, export to CSV + feather, and push to dedicated warehouse bucket(s). \n",
    "\n",
    "import os\n",
    "import sys\n",
    "import shutil\n",
    "import psutil\n",
    "import pydbtools\n",
    "import boto3\n",
    "import numpy\n",
    "import pandas\n",
    "import datetime\n",
    "import uuid\n",
    "import lovely_logger as log\n",
    "from arrow_pd_parser import reader, writer\n",
    "\n",
    "# Unique identifier to tag files\n",
    "uuid_tag = str(uuid.uuid4())\n",
    "timestring = str(datetime.datetime.today()).replace(' ','_').replace(':','-').split('.')[0]\n",
    "name = 'opg-analytical'\n",
    "test_mode = True\n",
    "\n",
    "# Clear a tmp directory\n",
    "shutil.rmtree('tmp',ignore_errors=True)\n",
    "os.makedirs('tmp', exist_ok=True)\n",
    "\n",
    "#==============================================================================\n",
    "# Log\n",
    "#==============================================================================\n",
    "\n",
    "try:\n",
    "    logfile\n",
    "except NameError:\n",
    "    # Initialise log\n",
    "    logfile = 'tmp/{}_{}.log'.format(name,uuid_tag)\n",
    "    # log.FILE_FORMAT = \"[%(asctime)s] [%(levelname)-8s] - %(message)s (%(filename)s:%(lineno)s)\"\n",
    "    log.FILE_FORMAT = \"[%(asctime)s] [%(levelname)-8s] %(message)s\"\n",
    "    # log.CONSOLE_FORMAT = \"[%(levelname)-8s] - %(message)s (%(filename)s:%(lineno)s)\"\n",
    "    log.CONSOLE_FORMAT = \"[%(levelname)-8s] %(message)s\"\n",
    "    log.DATE_FORMAT = '%Y-%m-%d %H:%M:%S.uuu%z'\n",
    "    log.init(logfile)\n",
    "    log.info(\"*** Hello from {}.py ***\".format(name))\n",
    "    log.info(\"uuid: our tag is [{}]\".format(uuid_tag))\n",
    "    \n",
    "#==============================================================================\n",
    "# Pipeline \n",
    "#==============================================================================\n",
    "\n",
    "# Make an array of new local files we're creating\n",
    "files = []\n",
    "\n",
    "# Keep track of files successfully written to S3\n",
    "news3files = []\n",
    "\n",
    "# For testing limit the number of records pulled (negative number for no limit)\n",
    "sql_limit = -1\n",
    "\n",
    "# Leave data where?\n",
    "bucket_name = 'alpha-opg-analytical'\n",
    "s3dir = 'sirius_data_cuts_3/'\n",
    "\n",
    "# Return data for first how many attorneys?\n",
    "max_attorneys = 4\n",
    "\n",
    "# Once a multi-field column has been converted to separate columns, shall we keep the original?\n",
    "keep_parsed_array_fields = False\n",
    "\n",
    "# For console debugging set data frame display options\n",
    "pandas.set_option('display.min_rows', 50)\n",
    "pandas.set_option('display.max_rows', 20)\n",
    "pandas.set_option('display.width', 1000)\n",
    "pandas.set_option('display.max_columns', 500)\n",
    "\n",
    "pandas.options.display.float_format = '{:.1f}'.format\n",
    "\n",
    "# Resource and buckets\n",
    "s3c = boto3.client('s3')\n",
    "s3r = boto3.resource('s3')\n",
    "\n",
    "#==============================================================================\n",
    "# \n",
    "#==============================================================================\n",
    "\n",
    "# Report memory footprint of a dataframe and the python process\n",
    "def memuse(df): \n",
    "    mem_df = round(df.memory_usage(deep=True).sum()/(1024**3),1)\n",
    "    mem_py = round(psutil.Process(os.getpid()).memory_info().rss/(1024**3),1)\n",
    "    report = 'memory: dataframe is {}GB, process is {}GB'.format(mem_df,mem_py)\n",
    "    return(report)\n",
    "\n",
    "# For a SIRIUS nested field in a dataframe, what are the unnested columns? \n",
    "# Name them sensibly. \n",
    "def split_attorney_fields_2(data, field, max_attorneys):\n",
    "    \n",
    "    log.info(\"[p{}] split_attorney_fields: parsing [{}] array field, first [{}] attorneys, [{}] records\".format(page,field,max_attorneys,len(data)))\n",
    "    \n",
    "    # e.g. we want 'replacement_attorney_postcodes' to become\n",
    "    # 'replacement_attorney_1_postcode', 'replacement_attorney_2_postcode', etc.\n",
    "    new_field_name_head = '{}attorney_'.format(field.split('attorney_',1)[0])\n",
    "    new_field_name_tail = '_{}'.format(field.split('attorney_',1)[1].replace('s_','_').rstrip('s'))\n",
    "\n",
    "    # Remove leading and trailing brackets\n",
    "    df = data[field].str.slice(\n",
    "        1, -1\n",
    "        \n",
    "    # Split into separate columns for each attorney\n",
    "    ).str.split(\n",
    "        pat=', ', n = -1, expand=True\n",
    "        \n",
    "    # Return only the first N columns. Note this gives 'None' if we run out of attorneys.   \n",
    "    # An empty string means there was an attorney, but data was missing. \n",
    "    ).iloc[\n",
    "        :, : max_attorneys\n",
    "        \n",
    "    # Rename the columns (e.g.: \"A1_postcode\", \"A2_postcode\", ... etc.)\n",
    "    ].rename(\n",
    "        columns=dict(\n",
    "            zip(numpy.arange(0,max_attorneys,1), \n",
    "                [new_field_name_head + n + new_field_name_tail for n in numpy.arange(1,max_attorneys+1,1).astype(str)])\n",
    "        )\n",
    "    )\n",
    "    \n",
    "    return(df)\n",
    "\n",
    "# What do the codes in an NSPL column translate to?\n",
    "def translate_nspl_codes(data, column, dict_s3bucket, dict_s3key, dict_from_col, dict_to_col, page): \n",
    "\n",
    "    # Read translation keys and values from S3 document to data frame\n",
    "    log.info(\"translate_nspl_codes: [{}] reading {} to {} dictionary\".format(page, dict_from_col,dict_to_col))\n",
    "#     replace_df = pandas.read_csv('s3://{}/{}'.format(dict_s3bucket,dict_s3key), \n",
    "#                          usecols=[dict_from_col, dict_to_col], \n",
    "#                          low_memory=False, \n",
    "#                          dtype=str)\n",
    "    replace_df = reader.read(f's3://{dict_s3bucket}/{dict_s3key}', \n",
    "                            usecols = [dict_from_col, dict_to_col], \n",
    "                            low_memory=False)\n",
    "\n",
    "    # Convert to dict for series replace\n",
    "    replace_dict = dict(zip(replace_df[dict_from_col], replace_df[dict_to_col]))\n",
    "\n",
    "    log.info(\"translate_nspl_codes: [{}] translating {} codes to names\".format(page, dict_from_col))\n",
    "    names = data[column].replace(to_replace=replace_dict)\n",
    "\n",
    "    return(names)\n",
    "\n",
    "\n",
    "# File export handler\n",
    "def my_export(df, fileformat, filename, page='?'):\n",
    "    log.info('[p{}] export-data: Writing {} file [{}]'.format(page,fileformat,filename))\n",
    "    if fileformat == \"csv\": \n",
    "        df.to_csv(filename, sep=',', index = False)\n",
    "    elif fileformat == \"feather\":\n",
    "        feather.write_dataframe(df=df, dest=filename)\n",
    "    elif fileformat == \"xlsx\": \n",
    "        writer = pandas.ExcelWriter(filename, engine='xlsxwriter')\n",
    "        df.to_excel(writer, sheet_name='Cases {} (N={})'.format(df['reg_year'].iloc[0],len(df)), index=False)\n",
    "        writer.save()\n",
    "    statinfo = os.stat(filename)\n",
    "    log.info(\"[p{}] export-data: Done ({}MB)\".format(page,round(statinfo.st_size/1024**2)))    \n",
    "    return(statinfo)\n",
    "\n",
    "def categorise_fields(df, fields): \n",
    "    '''\n",
    "    Report memory usage while categorising\n",
    "    '''\n",
    "    # Drop any fields not in the data\n",
    "    fields = list(set(fields).intersection(list(df.columns)))\n",
    "    for field in fields: \n",
    "        MB_to_save = round((df[field].memory_usage(deep=True) - \n",
    "                            df[field].astype('category').memory_usage(deep=True))/1024**2)\n",
    "        log.info('categorise_fields: converting {} to categorical to save {}MB'.format(field, MB_to_save))\n",
    "        df[field] = df[field].astype(\"category\")\n",
    "    return(df)\n",
    "\n",
    "#==============================================================================\n",
    "# NSPL region data\n",
    "#==============================================================================\n",
    "\n",
    "## Read NSPL data\n",
    "\n",
    "nspl_bucket_name = 'alpha-opg-data-processing'\n",
    "nspl_bucket = s3r.Bucket(nspl_bucket_name)\n",
    "nspl_key = 'ons/NSPL_FEB_2021_UK/Data/NSPL_FEB_2021_UK.csv'\n",
    "\n",
    "usecols = ['pcds','oa11','cty','laua','ctry','rgn','lsoa11','msoa11','ccg','ru11ind','oac11','imd']\n",
    "\n",
    "log.info(\"pandas: reading NSPL database\")\n",
    "#nspl = pandas.read_csv('s3://' + nspl_bucket_name + '/' + nspl_key, usecols=usecols, low_memory=False)\n",
    "nspl = reader.read(f's3://{nspl_bucket_name}/{nspl_key}', file_format='csv', usecols=usecols, low_memory=False)\n",
    "\n",
    "log.info(\"pandas: renaming NSPL fields\")\n",
    "cols_old = list(nspl.columns)\n",
    "cols_new = ['donor_nspl_' + col for col in cols_old]\n",
    "cols_dic = dict(zip(cols_old, cols_new))\n",
    "nspl = nspl.rename(columns = cols_dic)\n",
    "\n",
    "log.info(\"pandas: squashing NSPL postcodes\")\n",
    "nspl['pcdsquish'] = nspl['donor_nspl_pcds'].str.replace(\" \",\"\")\n",
    "\n",
    "#==============================================================================\n",
    "# Stats Wales WIMD data\n",
    "#==============================================================================\n",
    "\n",
    "wimd_bucket_name = 'alpha-opg-data-processing'\n",
    "wimd_bucket = s3r.Bucket(wimd_bucket_name)\n",
    "wimd_key = 'govwales/welsh-index-multiple-deprivation-2019-index-and-domain-ranks-by-small-area.ods'\n",
    "usecols = ['LSOA Code','LSOA Name (Eng)','WIMD 2019 Overall Decile']\n",
    "\n",
    "log.info(\"pandas: reading WIMD data\")\n",
    "# wimd = pandas.read_excel('s3://' + wimd_bucket_name + '/' + wimd_key,  \n",
    "#                          engine=\"odf\", \n",
    "#                          sheet_name = 'Deciles_quintiles_quartiles', \n",
    "#                          usecols = ['LSOA Code','LSOA Name (Eng)','WIMD 2019 Overall Decile'])\n",
    "s3r.Object(wimd_bucket_name, wimd_key).download_file(f\"tmp/{wimd_key.split('/')[1]}\")\n",
    "wimd = pandas.read_excel(f\"tmp/{wimd_key.split('/')[1]}\", sheet_name = 'Deciles_quintiles_quartiles', usecols = usecols)\n",
    "\n",
    "log.info(\"pandas: renaming WIMD fields\")\n",
    "cols_old = list(wimd.columns)\n",
    "cols_new = ['donor_wimd_lsoa', 'donor_wimd_lsoa_name', 'donor_wimd_decile']\n",
    "cols_dic = dict(zip(cols_old, cols_new))\n",
    "wimd = wimd.rename(columns = cols_dic)\n",
    "\n",
    "#==============================================================================\n",
    "# ONS English IMD data\n",
    "#==============================================================================\n",
    "\n",
    "imd_bucket_name = 'alpha-opg-data-processing'\n",
    "imd_bucket = s3r.Bucket(nspl_bucket_name)\n",
    "imd_key = 'ons/English indices of deprivation 2019 /File_7_-_All_IoD2019_Scores__Ranks__Deciles_and_Population_Denominators_3-1.csv'\n",
    "usecols = ['LSOA code (2011)','LSOA name (2011)','Index of Multiple Deprivation (IMD) Decile (where 1 is most deprived 10% of LSOAs)']\n",
    "\n",
    "log.info(\"pandas: reading IMD data\")\n",
    "# imd = pandas.read_csv('s3://' + imd_bucket_name + '/' + imd_key,  \n",
    "#                        usecols = ['LSOA code (2011)','LSOA name (2011)','Index of Multiple Deprivation (IMD) Decile (where 1 is most deprived 10% of LSOAs)'])\n",
    "imd = reader.read(f's3://{imd_bucket_name}/{imd_key}', file_format='csv', usecols=usecols, low_memory=False)\n",
    "log.info(\"pandas: renaming IMD fields\")\n",
    "cols_old = list(imd.columns)\n",
    "cols_new = ['donor_imd_lsoa', 'donor_imd_lsoa_name', 'donor_imd_decile']\n",
    "cols_dic = dict(zip(cols_old, cols_new))\n",
    "imd = imd.rename(columns = cols_dic)\n",
    "\n",
    "#==============================================================================\n",
    "# TransUnion CAMEO data\n",
    "#==============================================================================\n",
    "\n",
    "cameo_bucket_name = 'alpha-opg-data-processing'\n",
    "cameo_bucket = s3r.Bucket(cameo_bucket_name)\n",
    "cameo_key = 'transunion/cameo/CAMEO Analysis.csv'\n",
    "\n",
    "log.info(\"pandas: reading CAMEO data\")\n",
    "usecols = [\"Pcd\", \"CAMEO_UKP\", \"CAMEO_UKPG\", \"Age_Score\", \"Age_Band\", \n",
    "           \"Tenr_Score\", \"Tenr_Band\", \"Comp_Score\", \"Comp_Band\", \n",
    "           \"Econ_Score\", \"Econ_Band\", \"Life_Score\", \"Life_Band\", \n",
    "           \"CAMEOINTL\"]\n",
    "# cameo = pandas.read_csv('s3://' + cameo_bucket_name + '/' + cameo_key,  \n",
    "#                        usecols = usecols)\n",
    "cameo = reader.read(f's3://{cameo_bucket_name}/{cameo_key}', file_format='csv', usecols=usecols, low_memory=False)\n",
    "log.info(\"pandas: squashing CAMEO postcodes\")\n",
    "cameo['pcdsquish'] = cameo['Pcd'].str.replace(\" \",\"\")\n",
    "\n",
    "log.info(\"pandas: renaming CAMEO fields\")\n",
    "cols_old = list(cameo.columns)\n",
    "cols_new = ['cameo_' + col for col in cols_old]\n",
    "cols_dic = dict(zip(cols_old, cols_new))\n",
    "cameo = cameo.rename(columns = cols_dic)\n",
    "\n",
    "#==============================================================================\n",
    "# Select glue exports\n",
    "#==============================================================================\n",
    "\n",
    "# Database sirius_stacked has all historical exports piled on top of \n",
    "# each other, discerned by glueexporteddate for each table. \n",
    "\n",
    "# Choose a version of the source data \n",
    "# target_date = '2022-01-28' # YYYY-MM-DD, or: \n",
    "\n",
    "# Choose the most recent available source data\n",
    "target_date = pydbtools.read_sql_query('SELECT max(glueexporteddate) AS glueexporteddate FROM opg_sirius_prod.cases')['glueexporteddate'][0]\n",
    "# target_date = target_date.split('T')[0]\n",
    "log.info('pydbtools: Last glue export date seems to be {}'.format(target_date))\n",
    "\n",
    "# What is the most recent glue export datetime for each table of interest?\n",
    "glueexporteddates = {}\n",
    "tables = ['cases', 'persons', 'addresses', 'person_caseitem']\n",
    "log.info('pydbtools: looking for contemporary glue exports of {} tables ...'.format(len(tables)))\n",
    "for table in tables: \n",
    "    query = f\"SELECT MAX(glueexporteddate) AS glueexporteddate FROM opg_sirius_prod.{table} WHERE glueexporteddate <= DATE('{target_date}')\"\n",
    "    glueexporteddate = pydbtools.read_sql_query(query)['glueexporteddate'][0]\n",
    "    log.info(\"pydbtools: using '{}' table glue export from {} in Sirius\".format(table, glueexporteddate))\n",
    "    glueexporteddates[table] = glueexporteddate\n",
    "\n",
    "    \n",
    "# For testing these dates will work \n",
    "example_working_glueexporteddates = {'cases': datetime.date(2024, 2, 1),\n",
    "                                     'persons': datetime.date(2024, 2, 1),\n",
    "                                     'addresses': datetime.date(2024, 2, 1),\n",
    "                                     'person_caseitem': datetime.date(2024, 2, 1)}\n",
    "# glueexporteddates = example_working_glueexporteddates\n",
    "\n",
    "#==============================================================================\n",
    "# Select attorney fields\n",
    "#==============================================================================\n",
    "\n",
    "## Which attorney/replacement attorney data do we want?\n",
    "\n",
    "# SQL query varies depending on how many attorneys we want to recover data for \n",
    "# (max_attorneys), and which fields have been requested. Dictionary 'sql_subs' \n",
    "# below contains substrings of SQL to pass to Athena e.g. attorney ids in as many \n",
    "# fields as needed. \n",
    "\n",
    "# Make a dictionary with values containing SQL substrings which can be dropped \n",
    "# into our Athena query, e.g. TRY(<table>.attorney_dobs[N]) for N = 1 to 4). \n",
    "# We need TRY() in case there aren't that many attorneys for any specific case. \n",
    "# We cast to varchar to stop crazy dates in Sirius breaking Athena. Max \n",
    "# attorneys is set at top in pipeline config. \n",
    "\n",
    "attorney_indices = list(range(1, max_attorneys + 1))       # e.g. [1,2,3,4]\n",
    "sql_subs = {}\n",
    "\n",
    "# Attorney data \n",
    "for feature in ['dobs', 'postcodes']: \n",
    "    sql_subs['a_{}'.format(feature)] = \", \".join(['TRY(CAST(attorneys.{}[{}] as varchar)) AS A{}_{}'.format(feature,n,n,feature[:-1]) for n in attorney_indices])\n",
    "\n",
    "# Replacement attorney data \n",
    "for feature in ['dobs']: \n",
    "    sql_subs['r_{}'.format(feature)] = \", \".join(['TRY(CAST(replacement_attorneys.{}[{}] as varchar)) AS R{}_{}'.format(feature,n,n,feature[:-1]) for n in attorney_indices])\n",
    "\n",
    "#==============================================================================\n",
    "# Request to add cert provider data\n",
    "#==============================================================================\n",
    "\n",
    "## Use opg_sirius_prod temp tables\n",
    "\n",
    "def create_temp_table_ranked_person_address(glueexportdate): \n",
    "    \n",
    "    '''\n",
    "    Some (~30K) persons have more than one address attached to them. Sirius\n",
    "    only uses the first one. Rank addresses for each person in the addresses \n",
    "    table.\n",
    "    '''\n",
    "    \n",
    "    name = f\"temp_ranked_addresses\"\n",
    "    \n",
    "    q = (f\"\"\"\n",
    "        SELECT\n",
    "            ROW_NUMBER() OVER (PARTITION BY addresses.person_id ORDER BY addresses.id ASC) AS rank,\n",
    "            addresses.*\n",
    "        FROM opg_sirius_prod.addresses\n",
    "        WHERE addresses.glueexporteddate = DATE('{glueexportdate}')\n",
    "    \"\"\")\n",
    "    \n",
    "    q = ' '.join(q.split())\n",
    "    \n",
    "    pydbtools.create_temp_table(q, name)\n",
    "    \n",
    "    return name\n",
    "\n",
    "def create_temp_table_ranked_uid_actor(actor_type, glueexportdate): \n",
    "    \n",
    "    '''\n",
    "    Rank certificate providers for each case in the person_caseitem table\n",
    "    '''\n",
    "    \n",
    "    name = f\"temp_ranked_{actor_type}\"\n",
    "    \n",
    "    q = (f\"\"\"\n",
    "        SELECT\n",
    "        cases.uid as case_uid, \n",
    "        ROW_NUMBER() OVER (PARTITION BY cases.uid ORDER BY persons.id ASC) AS rank,\n",
    "        persons.*\n",
    "\n",
    "        FROM opg_sirius_prod.cases\n",
    "\n",
    "        LEFT JOIN opg_sirius_prod.person_caseitem\n",
    "        ON person_caseitem.glueexporteddate = DATE('{glueexportdate}')\n",
    "        AND person_caseitem.caseitem_id = cases.id\n",
    "\n",
    "        RIGHT JOIN opg_sirius_prod.persons\n",
    "        ON persons.glueexporteddate = DATE('{glueexportdate}')\n",
    "        AND persons.id = person_caseitem.person_id\n",
    "        AND persons.type = '{actor_type}'\n",
    "\n",
    "        WHERE cases.glueexporteddate = DATE('{glueexportdate}')\n",
    "    \"\"\")\n",
    "    \n",
    "    q = ' '.join(q.split())\n",
    "    \n",
    "    pydbtools.create_temp_table(q, name)\n",
    "    \n",
    "    return name\n",
    "\n",
    "# Rank addresses for each person in the addresses table\n",
    "# gdate = glueexporteddates['cases'].split('T')[0]\n",
    "gdate = glueexporteddates['cases'].strftime('%Y-%m-%d')\n",
    "temp_table_adr_ranked = create_temp_table_ranked_person_address(gdate)\n",
    "# Rank certificate providers for each case in the person_caseitem table\n",
    "temp_cp_table = create_temp_table_ranked_uid_actor('lpa_certificate_provider', gdate)\n",
    "    \n",
    "#==============================================================================\n",
    "# Configure SQL paging\n",
    "#==============================================================================\n",
    "\n",
    "## All the data at once exceeds our RAM limit. Page everything from here \n",
    "## until we can move all logic into Athena\n",
    "\n",
    "# How many pages shall we divide work over?\n",
    "n_pages = 10\n",
    "\n",
    "# Treat SIRIUS UID as primary key. Categorise UIDs by page number.\n",
    "log.info('pydbtools: requesting SIRIUS UIDs ...')\n",
    "query = (f\"\"\"\n",
    "         SELECT uid \n",
    "         FROM opg_sirius_prod.cases \n",
    "         WHERE cases.glueexporteddate = DATE('{glueexporteddates['cases']}')\n",
    "         \"\"\")\n",
    "query = ' '.join(query.split())\n",
    "uids = pydbtools.read_sql_query(query)\n",
    "log.info('page_config: discretising {} UIDs into {} pages'.format(len(uids),n_pages))\n",
    "uids['uid'] = pandas.to_numeric(uids['uid'])\n",
    "uids['page'] = pandas.qcut(uids['uid'], n_pages, labels = False)\n",
    "\n",
    "for page in range(n_pages): \n",
    "    uid_min = uids.loc[uids['page'] == page, 'uid'].min()\n",
    "    uid_max = uids.loc[uids['page'] == page, 'uid'].max()\n",
    "    log.info('page_config: starting page {} UIDs {} - {} (N={})'.format(page,uid_min,uid_max,len(uids.loc[uids['page'] == page])))\n",
    "    \n",
    "    #==============================================================================\n",
    "    # Read Athena data\n",
    "    #==============================================================================\n",
    "\n",
    "    query = (f\"\"\"\n",
    "\n",
    "    WITH \n",
    "        attorneys AS \n",
    "        (SELECT \n",
    "            person_caseitem.caseitem_id AS caseitem_id, \n",
    "            COALESCE(CARDINALITY(ARRAY_AGG(persons.id)),0) AS count, \n",
    "            ARRAY_AGG(persons.dob) AS dobs, \n",
    "            ARRAY_AGG(addresses.postcode) AS postcodes\n",
    "        FROM opg_sirius_prod.person_caseitem person_caseitem\n",
    "        INNER JOIN opg_sirius_prod.persons persons\n",
    "            ON persons.id = person_caseitem.person_id\n",
    "            AND persons.type = 'lpa_attorney'\n",
    "            AND persons.glueexporteddate = DATE('{glueexporteddates['persons']}')\n",
    "        LEFT JOIN opg_sirius_prod.addresses addresses\n",
    "            ON addresses.person_id = persons.id\n",
    "            AND addresses.glueexporteddate = DATE('{glueexporteddates['addresses']}')\n",
    "        WHERE person_caseitem.glueexporteddate = DATE('{glueexporteddates['person_caseitem']}')\n",
    "        GROUP BY person_caseitem.caseitem_id), \n",
    "\n",
    "        replacement_attorneys AS\n",
    "        (SELECT \n",
    "            person_caseitem.caseitem_id AS caseitem_id, \n",
    "            COALESCE(CARDINALITY(ARRAY_AGG(persons.id)),0) AS count, \n",
    "            ARRAY_AGG(persons.dob) AS dobs\n",
    "        FROM opg_sirius_prod.person_caseitem person_caseitem\n",
    "        INNER JOIN opg_sirius_prod.persons persons\n",
    "            ON persons.id = person_caseitem.person_id\n",
    "            AND persons.type = 'lpa_replacement_attorney'\n",
    "            AND persons.glueexporteddate = DATE('{glueexporteddates['persons']}')\n",
    "        WHERE person_caseitem.glueexporteddate = DATE('{glueexporteddates['person_caseitem']}')\n",
    "        GROUP BY person_caseitem.caseitem_id)\n",
    "\n",
    "    SELECT\n",
    "        CAST(cases.uid AS varchar) AS uid, \n",
    "        cases.type AS type, \n",
    "        cases.casesubtype AS casesubtype, \n",
    "        cases.status AS status, \n",
    "        CAST(cases.receiptdate AS varchar) AS receiptdate, \n",
    "        CAST(cases.registrationdate AS varchar) AS registrationdate,\n",
    "        CAST(cases.lpadonorsignaturedate AS varchar) AS lpadonorsignaturedate, \n",
    "        CASE WHEN cases.applicationtype = 0 THEN 'Classic' ELSE 'Online' END AS applicationtype, \n",
    "        cases.repeatapplication,     \n",
    "        cases.attorneyactdecisions, \n",
    "\n",
    "        CASE\n",
    "            WHEN cases.haveappliedforfeeremission = 0 THEN 'Not Set'\n",
    "            WHEN cases.haveappliedforfeeremission = 1 THEN 'False'\n",
    "            WHEN cases.haveappliedforfeeremission = 2 THEN 'True'\n",
    "            ELSE null END AS haveappliedforfeeremission, \n",
    "\n",
    "        CASE\n",
    "            WHEN cases.paymentremission = 0 THEN 'Not Set' \n",
    "            WHEN cases.paymentremission = 1 THEN 'False'\n",
    "            WHEN cases.paymentremission = 2 THEN 'True'\n",
    "            ELSE null END AS paymentremission, \n",
    "\n",
    "        CASE\n",
    "            WHEN cases.paymentexemption = 0 THEN 'Not Set'\n",
    "            WHEN cases.paymentexemption = 1 THEN 'False'\n",
    "            WHEN cases.paymentexemption = 2 THEN 'True'\n",
    "            ELSE null END AS paymentexemption, \n",
    "\n",
    "        CASE \n",
    "            WHEN cases.paymentbycheque = 0 THEN 'Not Set' \n",
    "            WHEN cases.paymentbycheque = 1 THEN 'False' \n",
    "            WHEN cases.paymentbycheque = 2 THEN 'True' \n",
    "            ELSE null END AS paymentbycheque, \n",
    "\n",
    "        cases.lifesustainingtreatment, \n",
    "\n",
    "        CASE \n",
    "            WHEN cases.caseattorneysingular AND NOT cases.caseattorneyjointly AND NOT cases.caseattorneyjointlyandseverally AND NOT cases.caseattorneyjointlyandjointlyandseverally THEN 'S' \n",
    "            WHEN cases.caseattorneyjointly AND NOT cases.caseattorneysingular AND NOT cases.caseattorneyjointlyandseverally AND NOT cases.caseattorneyjointlyandjointlyandseverally THEN 'J' \n",
    "            WHEN cases.caseattorneyjointlyandseverally AND NOT cases.caseattorneysingular AND NOT cases.caseattorneyjointly AND NOT cases.caseattorneyjointlyandjointlyandseverally THEN 'JS' \n",
    "            WHEN cases.caseattorneyjointlyandjointlyandseverally AND NOT cases.caseattorneysingular AND NOT cases.caseattorneyjointly AND NOT cases.caseattorneyjointlyandseverally THEN 'JJS' \n",
    "            ELSE null END AS how_attorneys_appointed, \n",
    "\n",
    "        CASE \n",
    "            WHEN cases.applicationhasguidance AND NOT cases.applicationhasrestrictions THEN 'PREF' \n",
    "            WHEN cases.applicationhasrestrictions AND NOT cases.applicationhasguidance THEN 'INST' \n",
    "            WHEN cases.applicationhasguidance AND cases.applicationhasrestrictions THEN 'BOTH' \n",
    "            WHEN NOT cases.applicationhasguidance AND NOT cases.applicationhasrestrictions THEN 'NONE' \n",
    "            ELSE null END AS instructions_or_preferences,  \n",
    "\n",
    "        CAST(donors.dob AS varchar) AS donor_dob,\n",
    "        LOWER(donors.salutation) AS donor_salutation, \n",
    "        add_d.postcode AS donor_postcode,\n",
    "\n",
    "--        CASE WHEN \n",
    "--            LOWER(add_c.address_lines) LIKE '% solicitor%' \n",
    "--                OR LOWER(add_c.address_lines) LIKE '% llp%'  \n",
    "--                OR LOWER(add_c.address_lines) LIKE '% ltd%' \n",
    "--                OR LOWER(add_c.address_lines) LIKE '% limited%' \n",
    "--                OR LOWER(add_c.address_lines) LIKE '% and co%' \n",
    "--                OR LOWER(add_c.address_lines) LIKE '% & co%' \n",
    "--                OR LOWER(add_c.address_lines) LIKE '% partners%' \n",
    "--                OR LOWER(add_c.address_lines) LIKE '% associates%' \n",
    "--                OR LOWER(add_c.address_lines) LIKE '% legal%' \n",
    "--                OR LOWER(add_c.address_lines) LIKE '% law %' \n",
    "--                OR correspondents.companyname != ''\n",
    "--            THEN True\n",
    "--        ELSE False END AS professional_correspondent, \n",
    "\n",
    "        COALESCE(attorneys.count,0) AS attorney_count, \n",
    "        COALESCE(replacement_attorneys.count,0) AS replacement_attorney_count, \n",
    "\n",
    "        {', '.join(sql_subs.values())}, \n",
    "        \n",
    "        cases.glueexporteddate AS cases_glueexporteddate, \n",
    "        \n",
    "        TRY(LOWER(cp1.firstname)) AS cp1_firstname, \n",
    "        TRY(LOWER(cp1.surname)) AS cp1_surname, \n",
    "        LOWER(TRY(cp1adr.address_lines[1])) AS cp1_adr_lines_1,\n",
    "        LOWER(TRY(cp1adr.address_lines[2])) AS cp1_adr_lines_2,\n",
    "        LOWER(TRY(cp1adr.address_lines[3])) AS cp1_adr_lines_3, \n",
    "        cp1adr.postcode AS cp1_pcode,\n",
    "        \n",
    "        TRY(LOWER(cp2.firstname)) AS cp2_firstname, \n",
    "        TRY(LOWER(cp2.surname)) AS cp2_surname, \n",
    "        LOWER(TRY(cp2adr.address_lines[1])) AS cp2_adr_lines_1,\n",
    "        LOWER(TRY(cp2adr.address_lines[2])) AS cp2_adr_lines_2,\n",
    "        LOWER(TRY(cp2adr.address_lines[3])) AS cp2_adr_lines_3, \n",
    "        cp2adr.postcode AS cp2_pcode\n",
    "        \n",
    "    FROM opg_sirius_prod.cases\n",
    "\n",
    "    LEFT JOIN opg_sirius_prod.persons donors \n",
    "        ON donors.id = cases.donor_id\n",
    "        AND donors.glueexporteddate = DATE('{glueexporteddates['persons']}')\n",
    "    LEFT JOIN opg_sirius_prod.persons correspondents\n",
    "        ON correspondents.id = cases.correspondent_id\n",
    "        AND correspondents.glueexporteddate = DATE('{glueexporteddates['persons']}')\n",
    "    LEFT JOIN opg_sirius_prod.addresses add_d\n",
    "        ON add_d.person_id = cases.donor_id\n",
    "        AND add_d.glueexporteddate = DATE('{glueexporteddates['addresses']}')\n",
    "    LEFT JOIN opg_sirius_prod.addresses add_c\n",
    "        ON add_c.person_id = cases.correspondent_id \n",
    "        AND add_c.glueexporteddate = DATE('{glueexporteddates['addresses']}')\n",
    "    LEFT JOIN attorneys\n",
    "        ON attorneys.caseitem_id = cases.id\n",
    "    LEFT JOIN replacement_attorneys\n",
    "        ON replacement_attorneys.caseitem_id = cases.id\n",
    "        \n",
    "    LEFT JOIN {temp_cp_table} cp1\n",
    "    ON cp1.rank = 1\n",
    "    AND cp1.case_uid = cases.uid\n",
    "    \n",
    "    LEFT JOIN {temp_cp_table} cp2\n",
    "    ON cp2.rank = 2\n",
    "    AND cp2.case_uid = cases.uid\n",
    "    \n",
    "    LEFT JOIN {temp_table_adr_ranked} cp1adr\n",
    "    ON cp1adr.rank = 1\n",
    "    AND cp1adr.person_id = cp1.id\n",
    "    \n",
    "    LEFT JOIN {temp_table_adr_ranked} cp2adr\n",
    "    ON cp2adr.rank = 1\n",
    "    AND cp2adr.person_id = cp2.id\n",
    "\n",
    "    WHERE cases.glueexporteddate = DATE('{glueexporteddates['cases']}')\n",
    "        AND CAST(cases.uid AS BIGINT) BETWEEN {uid_min} AND {uid_max} \n",
    "        AND cases.type IN ('lpa','epa')\n",
    "\n",
    "    {'LIMIT ' + str(sql_limit) if sql_limit >= 0 else ''}\n",
    "    \"\"\")\n",
    "\n",
    "    # Query whitespace\n",
    "    # query = \" \".join(query.split())\n",
    "\n",
    "    log.info(\"[p{}] pydbtools: our SQL query is: '{}'\".format(page,query))\n",
    "\n",
    "    # Send query to Athena, capture response in a pandas data frame\n",
    "    if \"LIMIT\" in query: log.warning(\"pydbtools: *** SQL LIMIT IMPOSED [{}] ***\".format(sql_limit))\n",
    "    log.info(\"[p{}] pydbtools: requesting data ... \".format(page))\n",
    "    cases = pydbtools.read_sql_query(query) \n",
    "    log.info(\"[p{}] pydbtools: received {} records\".format(page,cases.index.max()))\n",
    "\n",
    "    #==============================================================================\n",
    "    # Cleaning: Categorise\n",
    "    #==============================================================================\n",
    "\n",
    "    categoricals = ['type','casesubtype','status', 'applicationtype', \n",
    "                    'attorneyactdecisions', 'haveappliedforfeeremission', \n",
    "                    'paymentremission', 'paymentexemption', 'paymentbycheque', \n",
    "                    'lifesustainingtreatment', 'how_attorneys_appointed', \n",
    "                    'instructions_or_preferences']\n",
    "\n",
    "    cases = categorise_fields(cases, categoricals)\n",
    "\n",
    "    #==============================================================================\n",
    "    # Donor gender tagging \n",
    "    #==============================================================================\n",
    "\n",
    "    log.info('[p{}] pandas: tagging donor genders'.format(page))\n",
    "\n",
    "    cases['donor_salutation'] = cases['donor_salutation'].str.lower()\n",
    "\n",
    "    # Specify gender specific titles to tag with a gender\n",
    "    titles_f = ['mrs','miss','ms','missus','mistress','baroness', 'bness', 'countess', 'dame', 'lady', 'mm', 'mme', 'madam', 'madame', 'sister', 'viscountess', 'rev mrs']\n",
    "    titles_m = ['mr','master','baron', 'earl', 'esq', 'father', 'his honour', 'lord', 'sir','viscount']\n",
    "\n",
    "    # Tag everything as unknown initially, then match customer desired tags\n",
    "    cases['donor_gender'] = 'Other'\n",
    "    cases.loc[cases['donor_salutation'].isin(titles_m), 'donor_gender'] = 'Male'\n",
    "    cases.loc[cases['donor_salutation'].isin(titles_f), 'donor_gender'] = 'Female'\n",
    "    cases.loc[cases['donor_salutation'].isnull(), 'donor_gender'] = numpy.NaN\n",
    "    cases.loc[cases['donor_salutation'] == '', 'donor_gender'] = numpy.NaN\n",
    "\n",
    "    # Convert to categorical\n",
    "    cases['donor_gender'] = cases['donor_gender'].astype('category')\n",
    "    \n",
    "    # Drop salutations\n",
    "    cases = cases.drop(columns = 'donor_salutation')\n",
    "\n",
    "    #==============================================================================\n",
    "    # Cleaning: Dates\n",
    "    #==============================================================================\n",
    "\n",
    "    for field in [\"receiptdate\",\"registrationdate\",\"lpadonorsignaturedate\"]: \n",
    "        log.info(\"[p{}] pandas: converting {} to datetimes\".format(page, field))\n",
    "        cases[field] = pandas.to_datetime(cases[field], errors = 'coerce')\n",
    "\n",
    "    for field in [\"cases_glueexporteddate\"]:\n",
    "        log.info(\"[p{}] pandas: converting {} to dates\".format(page, field))\n",
    "        cases[field] = pandas.to_datetime(cases[field], errors = 'coerce').dt.date\n",
    "      \n",
    "    #==============================================================================\n",
    "    # Merge with NSPL data\n",
    "    #==============================================================================\n",
    "\n",
    "    log.info(\"[p{}] pandas: squishing SIRIUS postcodes\".format(page))\n",
    "    cases['pcdsquish'] = cases['donor_postcode'].str.replace(\" \",\"\")\n",
    "\n",
    "    log.info(\"[p{}] pandas: merging NSPL and SIRIUS databases\".format(page))\n",
    "    cases = cases.merge(nspl, on='pcdsquish', how='left')\n",
    "    n_no_nspl_match = len(cases.loc[cases['donor_nspl_pcds'].isnull()])\n",
    "    log.info('[p{}] pandas: failed to match {} records ({}%) to NSPL data'.format(page, n_no_nspl_match, round(100*n_no_nspl_match/len(cases),1)))\n",
    "    cases = cases.drop(['donor_nspl_pcds'], axis = 1)\n",
    "    \n",
    "    # Make new fields categorical\n",
    "    categoricals = ['donor_nspl_rgn','donor_nspl_laua','donor_nspl_ctry',\n",
    "                    'donor_nspl_ccg','donor_nspl_oac11']\n",
    "    cases = categorise_fields(cases, categoricals)\n",
    "\n",
    "    # Regions and country pseudo-regions\n",
    "    cases['donor_nspl_rgn_name'] = translate_nspl_codes(data = cases, \n",
    "                                                        column = 'donor_nspl_rgn', \n",
    "                                                        dict_s3bucket = nspl_bucket_name, \n",
    "                                                        dict_s3key = 'ons/NSPL_FEB_2021_UK/Documents/Region names and codes EN as at 12_10 (GOR).csv', \n",
    "                                                        dict_from_col = 'GOR10CD', dict_to_col = 'GOR10NM', page = page)\n",
    "\n",
    "    # Local authority / unitary authority\n",
    "    cases['donor_nspl_laua_name'] = translate_nspl_codes(data = cases, \n",
    "                                                         column = 'donor_nspl_laua', \n",
    "                                                         dict_s3bucket = nspl_bucket_name, \n",
    "                                                         dict_s3key = 'ons/NSPL_FEB_2021_UK/Documents/LA_UA names and codes UK as at 04_20.csv', \n",
    "                                                         dict_from_col = 'LAD20CD', dict_to_col = 'LAD20NM', page = page)\n",
    "\n",
    "    # UK Country\n",
    "    cases['donor_nspl_ctry_name'] = translate_nspl_codes(data = cases, \n",
    "                                                         column = 'donor_nspl_ctry', \n",
    "                                                         dict_s3bucket = nspl_bucket_name, \n",
    "                                                         dict_s3key = 'ons/NSPL_FEB_2021_UK/Documents/Country names and codes UK as at 08_12.csv', \n",
    "                                                         dict_from_col = 'CTRY12CD', dict_to_col = 'CTRY12NM', page = page)\n",
    "\n",
    "    # Clinical commissioning group or national equivalent\n",
    "    cases['donor_nspl_ccg_name'] = translate_nspl_codes(data = cases, \n",
    "                                                        column = 'donor_nspl_ccg', \n",
    "                                                        dict_s3bucket = nspl_bucket_name, \n",
    "                                                        dict_s3key = 'ons/NSPL_FEB_2021_UK/Documents/CCG names and codes UK as at 04_20.csv', \n",
    "                                                        dict_from_col = 'CCG20CD', dict_to_col = 'CCG20NM', \n",
    "                                                        page = page)\n",
    "\n",
    "    # OAC11 supergroups, groups and subgroups\n",
    "    for to_col in ['Supergroup','Group','Subgroup']: \n",
    "        cases['donor_nspl_oac11_{}'.format(to_col.lower())] = translate_nspl_codes(data = cases, \n",
    "                                                             column = 'donor_nspl_oac11', \n",
    "                                                             dict_s3bucket = nspl_bucket_name, \n",
    "                                                             dict_s3key = 'ons/NSPL_FEB_2021_UK/Documents/2011 Census Output Area Classification Names and Codes UK.csv', \n",
    "                                                             dict_from_col = 'OAC11', dict_to_col = to_col, \n",
    "                                                                                   page = page)\n",
    "\n",
    "    # All new fields should be categories to save memory \n",
    "    categories = ['donor_nspl_oac11', 'donor_nspl_cty',\n",
    "                  'donor_nspl_laua', 'donor_nspl_ctry',\n",
    "                  'donor_nspl_rgn', 'donor_nspl_lsoa11',\n",
    "                  'donor_nspl_msoa11', 'donor_nspl_ccg',\n",
    "                  'donor_nspl_ru11ind', 'donor_nspl_oac11', \n",
    "                  'donor_nspl_rgn_name', 'donor_nspl_laua_name', \n",
    "                  'donor_nspl_ctry_name', 'donor_nspl_ccg_name'] + ['donor_nspl_oac11_{}'.format(to_col.lower()) for to_col in ['Supergroup','Group','Subgroup']]\n",
    "    cases = categorise_fields(cases, categories)\n",
    "\n",
    "\n",
    "    #==============================================================================\n",
    "    # Merge with WIMD dataframe\n",
    "    #==============================================================================\n",
    "\n",
    "    log.info(\"[p{}] pandas: merging SIRIUS and WIMD data\".format(page))\n",
    "    cases = cases.merge(wimd, left_on='donor_nspl_lsoa11', right_on = 'donor_wimd_lsoa', how='left')\n",
    "    n_wimd_match = len(cases.loc[~cases['donor_wimd_lsoa'].isnull()])\n",
    "    log.info('[p{}] pandas: matched {} records ({}%) to WIMD data'.format(page, n_wimd_match, \n",
    "                                                                    round(100*n_wimd_match/len(cases),1)))\n",
    "    cases = cases.drop(['donor_wimd_lsoa','donor_wimd_lsoa_name'], axis = 1)\n",
    "\n",
    "    # wip \n",
    "    \n",
    "    #==============================================================================\n",
    "    # Merge with IMD dataframe\n",
    "    #==============================================================================\n",
    "    \n",
    "    log.info(\"[p{}] pandas: merging SIRIUS and IMD data\".format(page))\n",
    "    cases = cases.merge(imd, left_on='donor_nspl_lsoa11', right_on = 'donor_imd_lsoa', how='left')\n",
    "    n_imd_match = len(cases.loc[~cases['donor_imd_lsoa'].isnull()])\n",
    "    log.info('[p{}] pandas: matched {} records ({}%) to IMD data'.format(page,\n",
    "                                                                          n_imd_match,\n",
    "                                                                    round(100*n_imd_match/len(cases),1)))\n",
    "    cases = cases.drop(['donor_imd_lsoa','donor_imd_lsoa_name'], axis = 1)\n",
    "    \n",
    "    #==============================================================================\n",
    "    # Merge with CAMEO dataframe\n",
    "    #==============================================================================\n",
    "    \n",
    "    log.info(\"[p{}] pandas: merging SIRIUS and CAMEO data\".format(page))\n",
    "    cases = cases.merge(cameo, left_on='pcdsquish', right_on = 'cameo_pcdsquish', how='left')\n",
    "    n_cameo_match = len(cases.loc[~cases['cameo_Pcd'].isnull()])\n",
    "    log.info('[p{}] pandas: matched {} records ({}%) to CAMEO data'.format(page,\n",
    "                                                                          n_cameo_match, \n",
    "                                                                          round(100*n_cameo_match/len(cases),1)))\n",
    "    \n",
    "    cases = cases.drop(['cameo_Pcd','cameo_pcdsquish'], axis = 1)\n",
    "    log.info(\"[p{}] {}\".format(page,memuse(cases)))\n",
    "    \n",
    "    # Translate field codes\n",
    "        \n",
    "    cases['cameo_CAMEO_UKP_name'] = translate_nspl_codes(data = cases, \n",
    "                                                    column = 'cameo_CAMEO_UKP', \n",
    "                                                    dict_s3bucket = cameo_bucket_name, \n",
    "                                                    dict_s3key = 'transunion/cameo/CAMEO_UKP_category_names.csv', \n",
    "                                                    dict_from_col = 'CAMEO UK Category Code', dict_to_col = 'CAMEO UK Type', \n",
    "                                                    page = page)\n",
    "    \n",
    "    log.info(\"[p{}] {}\".format(page,memuse(cases)))\n",
    "    \n",
    "    cases['cameo_CAMEO_UKPG_name'] = translate_nspl_codes(data = cases, \n",
    "                                                column = 'cameo_CAMEO_UKPG', \n",
    "                                                dict_s3bucket = cameo_bucket_name, \n",
    "                                                dict_s3key = 'transunion/cameo/CAMEO_UKPG_group_names.csv', \n",
    "                                                dict_from_col = 'CAMEO UK Group Code', dict_to_col = 'CAMEO UK Type', \n",
    "                                                page = page)\n",
    "    \n",
    "    log.info(\"[p{}] {}\".format(page,memuse(cases)))\n",
    "    \n",
    "    for to_col in ['group','type']: \n",
    "        cases['cameo_CAMEOINTL_{}'.format(to_col.lower())] = translate_nspl_codes(data = cases, \n",
    "                                                             column = 'cameo_CAMEOINTL', \n",
    "                                                             dict_s3bucket = cameo_bucket_name, \n",
    "                                                             dict_s3key = 'transunion/cameo/CAMEOINTL_group_type_names.csv', \n",
    "                                                             dict_from_col = 'CAMEO International Code', dict_to_col = to_col, page = page)\n",
    "        \n",
    "    log.info(\"[p{}] {}\".format(page,memuse(cases)))\n",
    "\n",
    "    # Rename with donor prefix in case we add matches for other actors later\n",
    "    cols_old = [col for col in list(cases.columns) if col.startswith('cameo_')]\n",
    "    cols_new = ['donor_' + col for col in cols_old]\n",
    "    cols_dic = dict(zip(cols_old, cols_new))\n",
    "    cases = cases.rename(columns = cols_dic)\n",
    "        \n",
    "    #==============================================================================\n",
    "    # Drop discolsive/unneeded fields\n",
    "    #==============================================================================\n",
    "        \n",
    "    cases = cases.drop(['pcdsquish'], axis = 1)\n",
    "    \n",
    "    #==============================================================================\n",
    "    # Export to local files\n",
    "    #==============================================================================\n",
    "\n",
    "    # Export data to required formats \n",
    "    \n",
    "    date_tag_cases = 'S' + glueexporteddates['cases'].strftime('%Y-%m-%d')\n",
    "    date_tag_pipeline = 'P' + datetime.datetime.now().strftime(\"%Y-%m-%d\")\n",
    "    \n",
    "    for fileformat in [\"csv\"]:      \n",
    "        filename = \"tmp/{}_cases_{}_{}_{}_part-{}.{}\".format(name,date_tag_pipeline, date_tag_cases, uuid_tag,str(page+1).zfill(2),fileformat)\n",
    "        my_export(cases, fileformat, filename, page)\n",
    "        files.append(filename)\n",
    "        \n",
    "    log.info(\"[p{}] {}\".format(page,memuse(cases)))\n",
    "    \n",
    "    log.info('page_config: finished page {} UIDs {} - {} (N={})'.format(page,uid_min,uid_max,len(uids.loc[uids['page'] == page])))\n",
    "    \n",
    "#==============================================================================\n",
    "# Push local files to bucket\n",
    "#==============================================================================\n",
    "\n",
    "# Send everything to analytical bucket. \n",
    "\n",
    "for filename in files:\n",
    "    s3file = s3dir + filename.split(\"tmp/\")[1]\n",
    "    log.info(\"s3upload: writing CSV to S3 [\" + bucket_name + '/' + s3file + \"]\")\n",
    "    \n",
    "    try: \n",
    "        response = s3r.Object(bucket_name,s3file).put(Body=open(filename, 'rb'))\n",
    "        if('ResponseMetadata' in response): \n",
    "            if('HTTPStatusCode' in response['ResponseMetadata']): \n",
    "                if(response['ResponseMetadata']['HTTPStatusCode'] == 200): \n",
    "                    log.info('s3upload: HTTPStatusCode was 200')\n",
    "                    s3filesize = s3r.Bucket(bucket_name).Object(s3file).content_length\n",
    "                    if (s3filesize == os.stat(filename).st_size): \n",
    "                        log.info('s3upload: S3 and local file sizes match, assuming good upload')\n",
    "                        news3files.append(s3file)\n",
    "                    else:\n",
    "                        log.error('s3upload: S3 and local file size mismatch')\n",
    "                        sys.exit(500)\n",
    "                else: \n",
    "                    log.error('s3upload: bad HTTPStatusCode')\n",
    "                    sys.exit(501)\n",
    "            else: \n",
    "                log.error('s3upload: No HTTPStatusCode')\n",
    "                sys.exit(502)\n",
    "        else: \n",
    "            log.error('s3upload: No ResponseMetadata')\n",
    "            sys.exit(503)\n",
    "    except: \n",
    "        log.error('s3upload: some unknown problem uploading file to S3')\n",
    "        sys.exit(504)\n",
    "\n",
    "\n",
    "#==============================================================================\n",
    "# Remove old files from bucket\n",
    "#==============================================================================\n",
    "\n",
    "if(len(news3files) > 0): \n",
    "    log.info(\"cleanup: Clearing deprecated files from '{}/{}'\".format(bucket_name, s3dir))\n",
    "    bucket = s3r.Bucket(bucket_name)\n",
    "    s3files = list(bucket.objects.filter(Prefix=s3dir))\n",
    "    log.info(\"cleanup: Checking {} files\".format(len(s3files)))\n",
    "    \n",
    "    for file in s3files:\n",
    "        if (file.key != s3dir and \n",
    "            file.key not in news3files): \n",
    "            log.info(\"cleanup: Deleting deprecated file '{}' from bucket\".format(file.key))\n",
    "            file.delete()\n",
    "        else:\n",
    "            log.info(\"cleanup: Leaving '{}' in bucket\".format(file.key))\n",
    "else: \n",
    "    log.warning('cleanup: No good uploads; not touching existing files')\n",
    "\n",
    "#==============================================================================\n",
    "# Sign off and upload log\n",
    "#==============================================================================\n",
    "s3logfile = s3dir + logfile.split(\"tmp/\")[1]\n",
    "log.info(\"boto3: writing log to S3 [\" + bucket_name + '/' + s3logfile + \"]\")\n",
    "log.info(\"*** Bye from {}.py ***\".format(name))\n",
    "s3r.Object(bucket_name,s3logfile).put(Body=open(logfile, 'rb'))\n",
    "\n",
    "#==============================================================================\n",
    "# Scratch ... delete before deploy\n",
    "#==============================================================================\n",
    "\n",
    "# ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d5b062c-b1e7-4c1e-b2a1-ecfc7f1e5a35",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
