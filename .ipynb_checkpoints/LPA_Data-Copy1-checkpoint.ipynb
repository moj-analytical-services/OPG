{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f9dce19a-0b8e-4890-a9d2-d22dc9a1aba4",
   "metadata": {},
   "source": [
    "# OPG : LPA Data Pre-processing and Cleaning tool"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7517bdf9-08ba-4cc1-8c2f-b01ffdf7971d",
   "metadata": {},
   "source": [
    "## Setup\n",
    "\n",
    "Before you can run this project, you need to install some Python packages using the terminal:\n",
    "\n",
    "\n",
    "### create and activate  a virtual environment\n",
    "cd OPG\n",
    "python3 -m venv venv\n",
    "source venv/bin/activate\n",
    "\n",
    "### install the python packages required\n",
    "pip install -r requirements.txt\n",
    "\n",
    "#pip install --upgrade pip\n",
    "\n",
    "### Updating your branch with main\n",
    "When working on your models it is likely that your branch will get out of date with the main branch. To update you branch with the latest changes from main open a terminal and run the following:\n",
    "\n",
    "Check your working tree, commit/push any changes if required\n",
    "\n",
    "git status\n",
    "Switch to the main branch and collect the latest changes, if any\n",
    "\n",
    "git switch main\n",
    "git fetch\n",
    "git pull\n",
    "Switch back to your branch and merge in the changes from main\n",
    "\n",
    "git switch <your initial>/model-a-development\n",
    "git merge main -m \"update branch with main\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e765f151-ee2c-44b2-8fcd-0a438f11633c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Un-comment and Run the below code if there is an error with packages:\n",
    "\n",
    "!pip install arrow_pd_parser\n",
    "!pip install pydbtools\n",
    "!pip install arrow_pd_parser\n",
    "!pip install pydbtools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83bed50b-9b72-4d69-9326-fd1f690522fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import sys\n",
    "print(sys.path)\n",
    "\n",
    "import os.path\n",
    "from os import path\n",
    "os.getcwd()\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "#import awswrangler as wr\n",
    "#import statsmodels.api as sm\n",
    "#import tensorflow as tf\n",
    "import boto3\n",
    "import getpass\n",
    "import pytz\n",
    "#import openpyxl\n",
    "#import matplotlib\n",
    "import csv\n",
    "from arrow_pd_parser import reader, writer\n",
    "\n",
    "import pydbtools as pydb\n",
    "import datetime as dt\n",
    "from datetime import timedelta\n",
    "from datetime import datetime\n",
    "from datetime import date\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "%matplotlib inline\n",
    "from dateutil.relativedelta import relativedelta\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "#from tensorflow import keras\n",
    "import matplotlib as mpl\n",
    "mpl.rcParams['agg.path.chunksize'] = 10000\n",
    "from matplotlib import rc\n",
    "import statsmodels.api as sm\n",
    "\n",
    "# consistent plot size wherever not specifiied\n",
    "from pylab import rcParams\n",
    "mpl.rcParams['figure.figsize'] = (15,8)\n",
    "mpl.rcParams['axes.grid'] = False\n",
    "rcParams['xtick.labelsize'] = 14\n",
    "rcParams['ytick.labelsize'] = 14\n",
    "rcParams['axes.labelsize'] = 14\n",
    "\n",
    "%matplotlib inline\n",
    "%config InlineBackend.figure_format='retina'\n",
    "\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib as mpl\n",
    "from matplotlib import rc\n",
    "import statsmodels.api as sm\n",
    "\n",
    "# consistent plot size wherever not specifiied\n",
    "from pylab import rcParams\n",
    "mpl.rcParams['figure.figsize'] = (15,8)\n",
    "mpl.rcParams['axes.grid'] = False\n",
    "rcParams['xtick.labelsize'] = 14\n",
    "rcParams['ytick.labelsize'] = 14\n",
    "rcParams['axes.labelsize'] = 14\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4208661-eff3-4123-920a-99ed7ce72fcb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a directory to store the dummy CSV files\n",
    "#os.makedirs(\"dummy_csv_files\", exist_ok=True)\n",
    "\n",
    "# Generate and save dummy CSV files\n",
    "#for i in range(1, 4):  # Create 3 dummy CSV files\n",
    "#    data = {\n",
    "#        'Column_A': np.random.randint(1, 100, 5),\n",
    "#        'Column_B': np.random.rand(5),\n",
    "#       'Column_C': np.random.choice(['Category1', 'Category2'], 5)\n",
    "#    }\n",
    "#    df = pd.DataFrame(data)\n",
    "\n",
    "#    csv_filename = f\"dummy_file_{i}.csv\"\n",
    "#    df.to_csv(os.path.join(\"dummy_csv_files\", csv_filename), index=False)\n",
    "    \n",
    "##replace dummy_csv_files with the path to your csv files using the following command:\n",
    "#os.chdir('/path/to/your/csv/files')\n",
    "\n",
    "\n",
    "# Getting the List of CSV Files\n",
    "csv_files = [f for f in os.listdir(\"dummy_csv_files\") if f.endswith('.csv')]\n",
    "print(csv_files)\n",
    "\n",
    "# Loading the CSV Files into a DataFrame\n",
    "dfs = []\n",
    "\n",
    "for csv in csv_files:\n",
    "    df = pd.read_csv(os.path.join(\"dummy_csv_files\", csv), usecols=[\"receiptdate\",\"cases_glueexporteddate\",\"uid\",\"type\",\"casesubtype\",\"status\",\"donor_dob\",\"donor_postcode\",\"donor_gender\"], encoding='utf-8', error_bad_lines=False, engine=\"python\")\n",
    "    dfs.append(df)\n",
    "    \n",
    "# Concatenating the DataFrames\n",
    "final_df = pd.concat(dfs, ignore_index=True)\n",
    "\n",
    "#pd.concat([x.ix[:, cols_to_keep] for x in pd.read_csv(..., chunksize=200)]) \n",
    "final_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93434d88-6814-4c5b-8d96-e33a7eaa70d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate data \n",
    "\n",
    "## Source copied from \"Raw Data\" tab in monthly monitor extracted for only family case types\n",
    "## \\\\dom1.infra.int\\data\\hq\\102PF\\Shared\\CJG\\FMDU\\Data Share Area\\OPGDOCS\\Forecasting Model\\Long_Term_Model_April_2024\\MODEL_INPUTS\\opg-analytical_cases_P2024-03-18_S.csv\n",
    "\n",
    "#LPA_data = reader.read(\n",
    "#   r'dom1.infra.int\\data\\hq\\102PF\\Shared\\CJG\\FMDU\\Data Share Area\\OPGDOCS\\Forecasting Model\\Long_Term_Model_April_2024\\MODEL_INPUTS\\opg-analytical_cases_P2024-03-18_S.csv', \n",
    "#   file_format=\"csv\", \n",
    "#   encoding='unicode_escape'\n",
    "#)\n",
    "#r'opg-analytical_cases_P2024-03-18_S.csv', \n",
    "\n",
    "#LPA_data = reader.read(\n",
    "#   r\"opg-analytical_cases_P2024-03-18_S.csv\", \n",
    "#   file_format=\"csv\", \n",
    "#   encoding='utf-8',\n",
    "#   low_memory=False\n",
    "#)\n",
    "\n",
    "# Set the date you want to extract data\n",
    "snapshot_end = final_df.values[7].astype(str)[7]\n",
    "#snapshot_end = final_df.astype(str)\n",
    "#snapshot_start = datetime.strptime(snapshot_end, '%Y-%m-%d') - relativedelta(months= 1)\n",
    "#snapshot_start = snapshot_start.strftime('%Y-%m-%d')\n",
    "snapshot_end"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2fc0e1c-b824-4822-a8e5-20041f465a65",
   "metadata": {},
   "source": [
    "# Meta data and Variable selection and Data Cleaning for the LPA data in Data Warehouse:\n",
    "\n",
    "## Goal: to work out how many people applied for lpa and recieved the power of atthorney and how many applications in a year/month/week by age group since 2007? \n",
    "\n",
    "## ages over 19 years old\n",
    "\n",
    "### Unique case reference for each donor = [donor_dob + donor_postcode + donor_gender]\n",
    "\n",
    "#### Sort by the unique id and count how many application\n",
    "\n",
    "##### and then dermine Whether the application type [casesubtype] is hw=health and welfare or pfa=property and finance\n",
    "\n",
    "##### how many certificate provider (cp) for each lpa application?\n",
    "\n",
    "##### Location based data and geographical data for the donor can be used to identify the financial situation and wherether they are located in England or Wales\n",
    "\n",
    "Select \n",
    "    uid, # unique identifier \n",
    "    type, # type od appliation # type = lpa (living power of atthorney)\n",
    "    casesubtype, # type of lpa (hw=health and welfare, pfa=property and finance)\n",
    "    status, # status = application should be Registered and stand by OPG and be used by or perfect, not other options like pending or withdrown\n",
    "    receiptdate, # NOT NULL # very important for indexing and time series # recieptdate = actual receipt date that the application recieved by OPG and might change slightly and may produce the volume \n",
    "    registrationdate, # registration date only if the application is registered/Perfect/Payment Pending/Pending/Withdrawn.\n",
    "    lpadonorsignaturedate, # donor signed the application # 2-3% of application are in this category and it is useful to track delay and identify top draw lpa as donor they dont want to pay for the power of athorney\n",
    "    applicationtype, # application type is either Online or Classic (physical form) and 70% is doing the application in the Classic paper form\n",
    "    repeatapplication, # Not Important and be ignored\n",
    "    attorneyactdecisions, # Not Important and be ignored\n",
    "    haveappliedforfeeremission, # Not Important and be ignored\n",
    "    paymentremission, # Not Important and be ignored\n",
    "    paymentexemption, # Not Important and be ignored\n",
    "    paymentbycheque, # Not Important and be ignored\n",
    "    lifesustainingtreatment, # Not Important and be ignored\n",
    "    how_attorneys_appointed, # donor can appoint more than one attorney  # = (JS (Joined attorneies), S (Solely attorney))\n",
    "    instructions_or_preferences, # each application might generate multiple OPG reciepts # = NONE/BOTH/INST/PREF\n",
    "    donor_dob, #  NOT NULL # KEY VARIABLE to create a unique key # Date of Birth of the donor\n",
    "    donor_postcode, #  NOT NULL # KEY VARIABLE to create a unique key # Postcode of the donor  \n",
    "    attorney_count, # How many atthornies a donor applied for\n",
    "    replacement_attorney_count, # if one of the attorneies died or withdrawns \n",
    "    a1_dob, # the first attorney DoB\n",
    "    a2_dob, # the second attorney DoB\n",
    "    a3_dob, # the third attorney DoB\n",
    "    a4_dob, # the fourth attorney DoB\n",
    "    a1_postcode, # the first attorney Postcode\n",
    "    a2_postcode, # the second attorney Postcode\n",
    "    a3_postcode, # the third attorney Postcode\n",
    "    a4_postcode, # the fourth attorney Postcode\n",
    "    r1_dob, # the first \n",
    "    r2_dob, # the second \n",
    "    r3_dob, # the third \n",
    "    r4_dob, # the fourth \n",
    "    cases_glueexporteddate, # date of lpa application cases extracted (e.g., 18/03/2024) # this useful to index the 10th file\n",
    "    cp1_firstname, # certificate provider (cp)'s first name # can be a doctor or family/neighbor of donor # when donor and attorney signed needed to be withnessed and the person is an cp = to sign the document and assert the attorny\n",
    "    cp1_surname, # certificate provider (cp)'s second name \n",
    "    cp1_adr_lines_1, # certificate provider (cp)'s the first line of address \n",
    "    cp1_adr_lines_2, # certificate provider (cp)'s the second line of address \n",
    "    cp1_adr_lines_3, # certificate provider (cp)'s the third line of address\n",
    "    cp1_pcode, # certificate provider (cp)'s postcode\n",
    "    donor_gender, #  NOT NULL # KEY VARIABLE to create a unique key # Gender of the donor # sex = female,male, other (Dr, prof, etc)\n",
    "    donor_nspl_oa11, # the geograpgical location of donor to measure of household finance and more useful information\n",
    "    donor_nspl_cty, # Location based data wherether in England or Wales\n",
    "    donor_nspl_laua, # \n",
    "    donor_nspl_ctry, # \n",
    "    donor_nspl_rgn, # \n",
    "    donor_nspl_lsoa11, # \n",
    "    donor_nspl_msoa11, # \n",
    "    donor_nspl_ccg, # \n",
    "    donor_nspl_ru11ind, # \n",
    "    donor_nspl_oac11, # \n",
    "    donor_nspl_imd, # \n",
    "    donor_nspl_rgn_name, # \n",
    "    donor_nspl_laua_name, # \n",
    "    donor_nspl_ctry_name, # \n",
    "    donor_nspl_ccg_name, # \n",
    "    donor_nspl_oac11_supergroup, # \n",
    "    donor_nspl_oac11_group, # \n",
    "    donor_nspl_oac11_subgroup, # \n",
    "    donor_wimd_decile, # \n",
    "    donor_imd_decile, # \n",
    "    donor_cameo_CAMEO_UKP, # \n",
    "    donor_cameo_CAMEO_UKPG, # \n",
    "    donor_cameo_Age_Score, # \n",
    "    donor_cameo_Age_Band, # \n",
    "    donor_cameo_Tenr_Score, # \n",
    "    donor_cameo_Tenr_Band, # \n",
    "    donor_cameo_Comp_Score, # \n",
    "    donor_cameo_Comp_Band, # \n",
    "    donor_cameo_Econ_Score, # \n",
    "    donor_cameo_Econ_Band, # \n",
    "    donor_cameo_Life_Score, # \n",
    "    donor_cameo_Life_Band, # \n",
    "    donor_cameo_CAMEOINTL, # \n",
    "    donor_cameo_CAMEO_UKP_name, # \n",
    "    donor_cameo_CAMEO_UKPG_name, # \n",
    "    donor_cameo_CAMEOINTL_group, # \n",
    "    donor_cameo_CAMEOINTL_type\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50385b9d-958f-4a48-958b-bee05777d4cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "## Identify the type of data set and pre-processing: \n",
    "## Import, manipulate, and clean the data and impute missing values\n",
    "\n",
    "## Import the dataset and read in the actual data\n",
    "#df = wr.s3.read_csv([path1_s3], sep = ',', parse_dates=True) #import divorce data\n",
    "#read data\n",
    "#def parser(s):\n",
    "#    return datetime.strptime(s, '%Y-%m-%d')\n",
    "#df = wr.s3.read_csv([path1_s3], parse_dates=[0], index_col=0, squeeze=True, date_parser=parser)\n",
    "## iterating the columns\n",
    "#for col in df.columns:\n",
    "#    print(col)\n",
    "\n",
    "\n",
    "#lpa=LPA_data[[\"receiptdate\",\"cases_glueexporteddate\",\"uid\",\"type\",\"casesubtype\",\"status\",\"donor_dob\",\"donor_postcode\",\"donor_gender\"]]\n",
    "\n",
    "\n",
    "\n",
    "# Filter the records:\n",
    "df_filtered = final_df\n",
    "\n",
    "# Convert the receipt date to date format \n",
    "df_filtered['receiptdate'] = \\\n",
    "pd.to_datetime(df_filtered['receiptdate'], errors = 'coerce').dt.date\n",
    "\n",
    "# Filter records between the selected dates\n",
    "df_filtered = df_filtered.loc[(df_filtered['receiptdate'] > pd.to_datetime('2007-12-31'))]\n",
    "df_filtered = df_filtered.loc[(df_filtered['receiptdate'] < pd.to_datetime('2020-01-01'))]\n",
    "\n",
    "# Filter the dataframe to select only lpa type\n",
    "df_filtered = df_filtered.loc[(df_filtered['type'] == 'lpa')]\n",
    "\n",
    "df = df_filtered[[\"receiptdate\",\"uid\",\"casesubtype\",\"status\",\"donor_dob\",\"donor_postcode\",\"donor_gender\"]]\n",
    "## Remove Null values and records\n",
    "df = df.dropna()\n",
    "\n",
    "\n",
    "## Set index\n",
    "#df['receiptdate'] = pd.to_datetime(df['receiptdate'])\n",
    "\n",
    "#df = df.set_index('receiptdate').asfreq('D')\n",
    "\n",
    "####df['receiptdate'] = df.set_index('receiptdate',inplace=True)\n",
    "\n",
    "#df.index = df.index.to_period('D')\n",
    "                            \n",
    "###print(df.head())\n",
    "###print(df.tail())\n",
    "\n",
    "## Select the appropriate variable to be forecasted\n",
    "lpa_df = df\n",
    "\n",
    "\n",
    "\n",
    "#lpa_df['age'] = pd.to_datetime(df['receiptdate'], errors = 'coerce').dt.date - pd.to_datetime(df['donor_dob'], errors = 'coerce').dt.date\n",
    "#lpa_df['receiptdate'] = pd.to_datetime(lpa_df['receiptdate']).dt.date#.apply(lambda x: x.strftime('%Y-%m-%d'))\n",
    "#print(lpa_df)#['receiptdate']\n",
    "#lpa_df\n",
    "\n",
    "\n",
    "lpa_df['donor_dob'] = \\\n",
    "pd.to_datetime(lpa_df['donor_dob'], errors = 'coerce').dt.date\n",
    "\n",
    "# Extract age by subtracting 'receiptdate' and 'donor_dob'\n",
    "lpa_df['age'] = pd.to_datetime(lpa_df['receiptdate'], errors = 'coerce').dt.day - pd.to_datetime(lpa_df['donor_dob'], errors = 'coerce').dt.day#) / 365.25)\n",
    "#lpa_df['age'] = relativedelta(date, dob).years\n",
    "\n",
    "\n",
    "# Convert the ‘receiptdate’ column to datetime format for proper plotting.\n",
    "# Convert 'receiptdate' to datetime format \n",
    "lpa_df['receiptdate'] = pd.to_datetime(lpa_df['receiptdate'], errors='coerce')\n",
    "\n",
    "# Extract year from 'receiptdate'\n",
    "lpa_df['year'] = lpa_df['receiptdate'].dt.year\n",
    "\n",
    "\n",
    "#print(lpa_df['age'])\n",
    "\n",
    "## infer the frequency of the data:\n",
    "###lpa_df = df\n",
    "\n",
    "#lpa_df = df.asfreq(pd.infer_freq(df.index))\n",
    "\n",
    "#lpa_df = lpa_df[start_date:end_date]\n",
    "\n",
    "#start_date_years = datetime.strptime(start_date, \n",
    "#                                     '%Y-%m-%d') + relativedelta(years = 0)\n",
    "#print(start_date_years)\n",
    "\n",
    "#start_date_formatted = start_date_years.date()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5fb709f1-4cfb-43c2-870f-a4e078b48057",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract the year from the receiptdate\n",
    "#receipt_year = birth_year        \n",
    "\n",
    "# Calculate the age of the person\n",
    "#age = receiptdate - birth_year\n",
    "#lpa_df['a'] = \n",
    "############(pd.to_datetime(lpa_df['receiptdate'], errors = 'coerce').dt.day - pd.to_datetime(lpa_df['donor_dob'], errors = 'coerce').dt.day) # / 365.25\n",
    "#lpa_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "015e48bf-b1c7-4d9c-8521-e4c8619c8327",
   "metadata": {},
   "source": [
    "# Visualisation of the time series\n",
    "## Virtualisation of the LPA Data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b87f746-f458-4ab7-ab2e-8ce7243fbcb7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot 'age' against 'receiptdate'\n",
    "\n",
    "## --------------------------------------------------------------------------------  ##\n",
    "# Create a scatter plot with ‘receiptdate’ as the x-axis and ‘age’ as the y-axis.\n",
    "# Display the plot with appropriate labels and a grid.\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.scatter(lpa_df['receiptdate'], lpa_df['age'], alpha=0.5)\n",
    "plt.title('Age vs Receipt Date')\n",
    "plt.xlabel('Receipt Date')\n",
    "plt.ylabel('Age')\n",
    "plt.grid(True)\n",
    "plt.show()\n",
    "\n",
    "## --------------------------------------------------------------------------------  ##\n",
    "# Create a histogram of the 'age' column\n",
    "\n",
    "# This code will produce a histogram that displays the frequency distribution of ages in your dataset. \n",
    "# The bins parameter determines the number of bins used in the histogram, and you can adjust this number\n",
    "# to change the granularity of your histogram.\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.hist(lpa_df['age'], bins=20, alpha=0.7, color='blue')\n",
    "plt.title('Age Distribution')\n",
    "plt.xlabel('Age')\n",
    "plt.ylabel('Frequency')\n",
    "plt.grid(True)\n",
    "plt.show()\n",
    "\n",
    "## --------------------------------------------------------------------------------  ##\n",
    "# Create a line chart of age against receipt date\n",
    "# Sort the DataFrame by 'receiptdate' to ensure the line chart is ordered\n",
    "lpa_df.sort_values('receiptdate', inplace=True)\n",
    "\n",
    "# Plot 'age' against 'receiptdate' using a line chart\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(lpa_df['receiptdate'], lpa_df['age'], marker='o', linestyle='-', color='blue')\n",
    "plt.title('Age vs Receipt Date')\n",
    "plt.xlabel('Receipt Date')\n",
    "plt.ylabel('Age')\n",
    "plt.grid(True)\n",
    "\n",
    "## --------------------------------------------------------------------------------  ##\n",
    "# Produce a line chart that displays the average age of individuals for each year based on the receipt dates in your dataset.\n",
    "# The data points are connected with a line, which helps in identifying any trends or patterns over the years.\n",
    "\n",
    "# Group the data by year and calculate the average age for each year\n",
    "age_by_year = lpa_df.groupby('year')['age'].mean().reset_index()\n",
    "\n",
    "# Plot 'age' against 'year' using a line chart\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(age_by_year['year'], age_by_year['age'], marker='o', linestyle='-', color='blue')\n",
    "plt.title('Average Age vs Year')\n",
    "plt.xlabel('Year')\n",
    "plt.ylabel('Average Age')\n",
    "plt.grid(True)\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0817f1a4-4782-4762-bd6e-715e96728f49",
   "metadata": {},
   "source": [
    "## Observations:\n",
    "The trend in the line chart indicates the changes in the average age of individuals over the years, \n",
    "based on the receipt dates from your dataset.\n",
    "Such a visualization can help identify patterns, \n",
    "such as whether the average age is increasing, decreasing, or remaining relatively stable over time.\n",
    "\n",
    "For example:\n",
    "An upward trend would suggest that the average age is increasing each year.\n",
    "A downward trend would indicate that the average age is decreasing.\n",
    "A flat line would imply that there is little to no change in the average age over the years.\n",
    "These trends can be influenced by various factors, such as the demographics of the population being studied, \n",
    "changes in policies, or other external factors that might affect the age distribution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9819b606-ffd9-4e8c-bcc6-ac54e81cf4cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Plot the Actuals\n",
    "\n",
    "# lpa_series = lpa_df['age']\n",
    "# #lpa_series = df.squeeze()\n",
    "# plt.figure(figsize=(28, 14))\n",
    "# plt.plot(lpa_series)\n",
    "# plt.title('UK Actual LPA Data', fontsize=20)\n",
    "# plt.ylabel('Age', fontsize=16)\n",
    "# plt.axvline(pd.to_datetime(df['receiptdate'], errors = 'coerce').dt.year, color = 'k', linestyle='--', alpha = 0.2)\n",
    "# # for year in range(min(pd.to_datetime(df['receiptdate'], errors = 'coerce').dt.year), \n",
    "# #     datetime.strptime(snapshot_end, '%Y-%m-%d').year):\n",
    "# #     #datetime.strptime(\"2024-03-18\", '%Y-%m-%d').year):\n",
    "# #     plt.axvline(pd.to_datetime(df['receiptdate'], errors = 'coerce'), color = 'k', linestyle='--', alpha = 0.2)\n",
    "# #     #plt.axvline(pd.to_datetime(str(year) + '-01-01'), color = 'k', \n",
    "# #     #print(year)\n",
    "# plt.legend()    \n",
    "# #plt.savefig('UK_Actual_LPA_Data.png', dpi=300, bbox_inches='tight')\n",
    "# plt.show()  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d9843c8-2905-43ae-9974-1c184aeba587",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Perform the GROUP BY operation and calculate the count\n",
    "#Cases_by_year_age = lpa_df.groupby(\n",
    "#    ['receiptdate', 'uid', 'type', 'casesubtype', 'status', 'donor_postcode', 'donor_gender', 'age']) \\\n",
    "#    .agg({'No_of_Cases': 'count'}) \\ #['donor_postcode', 'donor_gender', 'age']\n",
    "#    .reset_index()\n",
    "\n",
    "#agg_funcs = dict(No_of_Cases = 'count')\n",
    "#Cases_by_year_age = lpa_df.set_index(['receiptdate', 'uid', 'type', 'casesubtype', 'status', 'donor_postcode', 'donor_gender', 'age']) \\\n",
    "#    .stack() \\\n",
    "#    .groupby(level=0) \\\n",
    "#    .agg(agg_funcs)\n",
    "\n",
    "\n",
    "#Cases_by_year_age\n",
    "#lpa_by_year_age = lpa_df[['receiptdate', 'uid', 'type', 'casesubtype', 'status', 'donor_postcode', 'donor_gender', 'age']] \\\n",
    "#                    .groupby(['donor_postcode', 'donor_gender', 'age'])  \\\n",
    "#                    .agg('count')#.sum()\n",
    "#lpa_by_year_age.to_csv(r'lpa_by_year_age.csv')\n",
    "\n",
    "\n",
    "\n",
    "#lpa_df.to_csv(r'lpa_df.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0916770-95b6-4bff-8b14-b1d103bc585a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a unique identifier based on multiple columns:\n",
    "# lpa_unique_key = lpa_df\n",
    "\n",
    "\n",
    "# #df1.set_index(['donor_postcode', 'donor_gender', 'age']).index.factorize()[0]+1\n",
    "# lpa_unique_key.insert(loc = 0, column='ukey', value = lpa_unique_key.set_index(['donor_postcode', 'donor_gender', 'age']).index.factorize()[0]+1)\n",
    "# #lpa_unique_key\n",
    "\n",
    "# #(lpa_unique_key.fillna({'donor_postcode':'', 'donor_gender':'', 'age':''})\n",
    "# #   .groupby(['donor_postcode', 'donor_gender', 'age'],sort=False).ngroup()+1)\n",
    "\n",
    "# #lpa_unique_key.loc[lpa_unique_key['type']=='lpa','ukey'].agg(['nunique','count','size'])\n",
    "# #lpa_unique_key.query('type == \"lpa\"')['ukey'].agg(['nunique','count','size'])\n",
    "# #lpa_unique_key.query('casesubtype == \"hw\"')['ukey'].agg(['nunique','count','size'])\n",
    "# #lpa_unique_key.query('casesubtype == \"pfa\"')['ukey'].agg(['nunique','count','size'])\n",
    "# #lpa_unique_key.groupby(['ukey']).count()\n",
    "# #lpa_unique_key['count_ukey'] = lpa_unique_key['ukey'].value_counts()\n",
    "# #lpa_unique_key\n",
    "\n",
    "\n",
    "\n",
    "# lpa_unique_key['CountbyUkey'] = lpa_unique_key.groupby(['donor_postcode', 'donor_gender']).age.transform('count')\n",
    "# lpa_unique_key['CountbyAge'] = lpa_unique_key.groupby('year').age.transform('count').sum()\n",
    "\n",
    "# # Perform the GROUP BY operation and calculate the sum\n",
    "# lpa_age = lpa_unique_key.groupby(['donor_postcode', 'donor_gender', 'age']) \\\n",
    "#     .agg({'CountbyAge': 'sum'}) \\\n",
    "#     .reset_index()\n",
    "\n",
    "# print(lpa_age)\n",
    "# #lpa_unique_key['month'] = lpa_unique_key['ArrivalDate'].dt.month\n",
    "\n",
    "\n",
    "# # Cases_by_year_age\n",
    "\n",
    "# #lpa_by_year_age = lpa_unique_key[['receiptdate', 'uid', 'type', 'casesubtype', 'status', 'donor_postcode', 'donor_gender', 'age']] \\\n",
    "# #                    .groupby(['donor_postcode', 'donor_gender', 'age'])  \\\n",
    "# #                    .agg('count')#.sum()\n",
    "\n",
    "\n",
    "# DataFrame with the count of unique records for each combination of age and year. \n",
    "# Generate a unique key by combining age, donor_gender, and donor_postcode, \n",
    "# and then calculate the number of unique records by age and year.\n",
    "\n",
    "# Remove spaces from the donor postcodes\n",
    "lpa_df['donor_postcode'] = lpa_df['donor_postcode'].str.strip()\n",
    "lpa_df['donor_postcode'] = lpa_df['donor_postcode'].str.replace(' ', '')\n",
    "\n",
    "# Generate a unique key by combining age, donor_gender, and donor_postcode\n",
    "lpa_df['unique_key'] = lpa_df['donor_dob'].astype(str) + lpa_df['donor_gender'] + lpa_df['donor_postcode']\n",
    "# lpa_by_year_age = lpa_unique_key\n",
    "\n",
    "# lpa_by_year_age.to_csv(r'lpa_by_year_age.csv')\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ed8d0ee-d035-4c36-8a51-464c5ed4e872",
   "metadata": {},
   "outputs": [],
   "source": [
    "lpa_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d42c4dde-0614-45ab-91a1-254117ca8f41",
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove duplicate rows based on Id values(unique_key) and \n",
    "# keep only the row that don't have 0 value in all the fields.\n",
    "lpa_unique = lpa_df\n",
    "\n",
    "duplicateMask = lpa_unique.duplicated('unique_key', keep=False)\n",
    "\n",
    "lpa_unique = pd.concat([lpa_unique.loc[duplicateMask & lpa_unique[['donor_dob', 'donor_gender', 'donor_postcode']].ne(0).any(axis=1)],\n",
    "               lpa_unique[~duplicateMask]])\n",
    "\n",
    "#lpa_df['zero']=lpa_df.select_dtypes(['int','float']).eq(0).sum(axis=1)\n",
    "#df=df.sort_values(['zero','Id']).drop_duplicates(subset=['Id']).drop(columns='zero')df['zero']=df.select_dtypes(['int','float']).eq(0).sum(axis=1)\n",
    "#df=df.sort_values(['zero','Id']).drop_duplicates(subset=['Id']).drop(columns='zero')\n",
    "\n",
    "lpa_unique = lpa_unique.drop_duplicates(subset=\"unique_key\")\n",
    "lpa_unique"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17bc0e87-7c3d-4780-9eff-a84fe55b4bc2",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "# Sort the rows of dataframe by  'unique_key'  \n",
    "# column inplace\n",
    "\n",
    "lpa_df_index = lpa_unique.sort_values(lpa_unique.columns[9])\n",
    "\n",
    "\n",
    "#lpa_df_index = lpa_unique.sort_values['unique_key']\n",
    "#lpa_df_index = lpa_unique.sort_values(by = 'unique_key', axis = 1, inplace = True, ascending = True)\n",
    "#lpa_df_index = lpa_unique.reindex(sorted(lpa_unique.columns), axis=1)\n",
    "\n",
    "## Set index\n",
    "#df['receiptdate'] = pd.to_datetime(df['receiptdate'])\n",
    "\n",
    "#df = df.set_index('receiptdate').asfreq('D')\n",
    "#lpa_df_index['unique_key'] = \n",
    "\n",
    "lpa_df_index.set_index('unique_key', inplace = True)\n",
    "\n",
    "#df.index = df.index.to_period('D')\n",
    "                            \n",
    "###print(df.head())\n",
    "###print(df.tail())\n",
    "\n",
    "Missing_data = lpa_df_index[(lpa_df_index['age']<0)]\n",
    "print(Missing_data)\n",
    "\n",
    "# Extract and save data into a csv file\n",
    "lpa_data = lpa_df_index\n",
    "lpa_data.to_csv(r'lpa_data.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49ed0a3d-b058-41e4-8cbc-61cd51c67b72",
   "metadata": {},
   "source": [
    "# Missing Data Imputation:\n",
    "\n",
    "There are be some people in the LPA data with missing age (they are represented with negetive numbers in column age). \n",
    "So for missing data (age) imputation, his code is written to use age distribution of cases that they have age and\n",
    "apply this to the total number of doners in that year. \n",
    "Actually, we allocate proportionaly distributed age across each year of these missing ages. \n",
    "E.g., if we get 90% of age distribution for a particular year,\n",
    "we used this age distribution to be applied to the 100% of donors to get the total distribution. \n",
    "\n",
    "The code below: \n",
    "first, loads the data from the CSV file and replaces negative ages \n",
    "with NaN to represent missing data. \n",
    "\n",
    "It then calculates the age distribution for each year. \n",
    "\n",
    "For each year, it finds the indices of the missing ages and imputes \n",
    "them by randomly choosing from the age distribution of that year. \n",
    "\n",
    "The imputed ages are proportional to the age distribution \n",
    "of the donors that year. \n",
    "\n",
    "Finally, it saves the DataFrame with the imputed ages to a new CSV file.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b89a0e3-ef12-4915-9d53-f8e6ca6dd1e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "lpa_data_sample_imputed = lpa_data\n",
    "\n",
    "\n",
    "# Identify the rows with missing age (represented as negative numbers)\n",
    "missing_age = lpa_data_sample_imputed['age'] < 0\n",
    "\n",
    "# Replace negative ages with NaN\n",
    "lpa_data_sample_imputed.loc[missing_age, 'age'] = np.nan\n",
    "\n",
    "# Calculate the age distribution for each year excluding missing ages\n",
    "age_distribution = lpa_data_sample_imputed.loc[~missing_age].groupby('year')['age'].value_counts(normalize=True)\n",
    "\n",
    "# Calculate the age distribution for each year\n",
    "age_distribution_per_year = lpa_data_sample_imputed.groupby('year')['age'].value_counts(normalize=True)\n",
    "\n",
    "# # Apply the age distribution to the total number of donors in each year\n",
    "# for year in df['year'].unique():\n",
    "#     # Calculate the number of missing ages in the current year\n",
    "#     num_missing = missing_age & (df['year'] == year)\n",
    "    \n",
    "#     # If there are missing ages in the current year\n",
    "#     if num_missing.sum() > 0:\n",
    "#         # Generate ages according to the age distribution of the current year\n",
    "#         imputed_ages = np.random.choice(age_distribution[year].index, \n",
    "#                                         p=age_distribution[year].values, \n",
    "#                                         size=num_missing.sum())\n",
    "        \n",
    "#         # Assign the generated ages to the missing ages\n",
    "#         df.loc[num_missing, 'age'] = imputed_ages\n",
    "\n",
    "\n",
    "# Apply the age distribution to the missing ages\n",
    "for year in df['year'].unique():\n",
    "    missing_age_indices = lpa_data_sample_imputed[(lpa_data_sample_imputed['year'] == year) & (lpa_data_sample_imputed['age'].isna())].index\n",
    "    if not missing_age_indices.empty:\n",
    "        imputed_ages = np.random.choice(age_distribution_per_year[year].index, \n",
    "                                        p=age_distribution_per_year[year].values, \n",
    "                                        size=len(missing_age_indices))\n",
    "        lpa_data_sample_imputed.loc[missing_age_indices, 'age'] = imputed_ages\n",
    "\n",
    "      \n",
    "# Save the dataframe with imputed ages\n",
    "lpa_data_sample_imputed.to_csv('lpa_data_sample_imputed.csv', index=False)\n",
    "\n",
    "# Print a success message\n",
    "print(\"The missing age data has been successfully imputed and saved to lpa_data_sample_imputed.csv file.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94e0a4fd-f395-40b0-b785-c361f1b11029",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Function to calculate the number of unique records by age, year, gender, and postcode\n",
    "# #def calculate_unique_records_by_age_year_gender_postcode(records):\n",
    "# # Get the current year\n",
    "# #current_year = datetime.now().year  \n",
    "# #Create a dictionary to store counts for each age, year, gender, and postcode combination\n",
    "# age_year_gender_postcode_counts = {}\n",
    "\n",
    "# records = lpa_df\n",
    "    \n",
    "# # Iterate over each record\n",
    "# for record in records:         \n",
    "#     # Extract gender and postcode\n",
    "#     gender = record[\"donor_gender\"]\n",
    "#     postcode = record[\"donor_postcode\"]\n",
    "#     dob = record[\"donor_dob\"]\n",
    "    \n",
    "#     # Create a unique key combining age, gender, and postcode\n",
    "#     key = (dob, gender, postcode)\n",
    "        \n",
    "#     # Increment the count for the key\n",
    "#     age_year_gender_postcode_counts[key] = age_year_gender_postcode_counts.get(key, 0) + 1\n",
    "        \n",
    "# return age_year_gender_postcode_counts\n",
    "\n",
    "# # Call the function and print the results\n",
    "# unique_records_by_age_year_gender_postcode = calculate_unique_records_by_age_year_gender_postcode(records)\n",
    "\n",
    "# print(\"Number of unique records by age, year, gender, and postcode:\")\n",
    "\n",
    "# for key, count in unique_records_by_age_year_gender_postcode.items():\n",
    "#     dob, gender, postcode = key\n",
    "#     print(f\"Date of Birth (D.o.B): {dob}, Gender: {gender}, Postcode: {postcode}, Count: {count}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e12141e7-3cf6-4e44-9040-156be6692d2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract the year of reciept from the receiptdate\n",
    "#receipt_year = int(record[\"receiptdate\"].split(\"-\")[0])\n",
    "\n",
    "\n",
    "\n",
    "# Calculate the number of unique records by age and year\n",
    "count_unique_records = lpa_data_sample_imputed.groupby(['year', 'donor_gender', 'age'])['uid'].nunique().reset_index(name='count')\n",
    "\n",
    "\n",
    "#unique_records = df.groupby('unique_key').agg('count').reset_index()  #.groupby(['year'])['unique_key'].nunique().reset_index(name='count')\n",
    "count_unique_records = count_unique_records.rename(columns={\"count\": \"Count_of_CASEID\"})\n",
    "\n",
    "# Display the result\n",
    "print(count_unique_records)\n",
    "\n",
    "# Save the result into a csv file\n",
    "count_unique_records.to_csv(r'count_unique_records.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1ce1de0-088c-4447-a432-f0dce3facfee",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This code will output the number of unique records for each age in each year for each donor gender in each donor postcode.\n",
    "# It calculates the age based on the current year and the birth year of each person in the records.\n",
    "# Then, it creates a unique key combining age, year, gender, and postcode, and increments the count for each key.\n",
    "# Finally, it prints the results showing the count of unique records for each combination.\n",
    "\n",
    "#from datetime import datetime\n",
    "\n",
    "# Sample data representing records with donor gender, donor postcode, and date of birth\n",
    "#records = [\n",
    "#    {\"donor_gender\": \"Male\", \"donor_postcode\": \"AB12 3CD\", \"date_of_birth\": \"1999-05-15\"},\n",
    "#    {\"donor_gender\": \"Female\", \"donor_postcode\": \"XY34 5YZ\", \"date_of_birth\": \"1994-08-20\"},\n",
    "#    {\"donor_gender\": \"Male\", \"donor_postcode\": \"CD56 7EF\", \"date_of_birth\": \"1996-02-10\"},\n",
    "#    {\"donor_gender\": \"Male\", \"donor_postcode\": \"FG78 9HI\", \"date_of_birth\": \"2000-11-30\"},\n",
    "#    {\"donor_gender\": \"Female\", \"donor_postcode\": \"JK90 1LM\", \"date_of_birth\": \"1987-03-25\"},\n",
    "#    {\"donor_gender\": \"Male\", \"donor_postcode\": \"OP23 4QR\", \"date_of_birth\": \"1993-09-05\"}\n",
    "#]\n",
    "\n",
    "# Function to calculate the number of unique records by age, year, gender, and postcode\n",
    "#def calculate_unique_records_by_age_year_gender_postcode(records):\n",
    "    # Get the current year\n",
    "#    current_year = datetime.now().year\n",
    "    \n",
    "    # Create a dictionary to store counts for each age, year, gender, and postcode combination\n",
    "#    age_year_gender_postcode_counts = {}\n",
    "    \n",
    "    # Iterate over each record\n",
    "#    for record in records:\n",
    "        # Extract the year of birth from the date_of_birth\n",
    "#        birth_year = int(record[\"date_of_birth\"].split(\"-\")[0])\n",
    "        \n",
    "        # Calculate the age of the person\n",
    "#        age = current_year - birth_year\n",
    "        \n",
    "        # Extract the year from the date_of_birth\n",
    "#        year = birth_year\n",
    "        \n",
    "        # Extract gender and postcode\n",
    "#        gender = record[\"donor_gender\"]\n",
    "#        postcode = record[\"donor_postcode\"]\n",
    "        \n",
    "        # Create a unique key combining age, year, gender, and postcode\n",
    "#        key = (age, year, gender, postcode)\n",
    "        \n",
    "        # Increment the count for the key\n",
    "#        age_year_gender_postcode_counts[key] = age_year_gender_postcode_counts.get(key, 0) + 1\n",
    "        \n",
    "#    return age_year_gender_postcode_counts\n",
    "\n",
    "# Call the function and print the results\n",
    "#unique_records_by_age_year_gender_postcode = calculate_unique_records_by_age_year_gender_postcode(records)\n",
    "#print(\"Number of unique records by age, year, gender, and postcode:\")\n",
    "#for key, count in unique_records_by_age_year_gender_postcode.items():\n",
    "#    age, year, gender, postcode = key\n",
    "#    print(f\"Age: {age}, Year: {year}, Gender: {gender}, Postcode: {postcode}, Count: {count}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77c7b3db-0582-4c65-8ca5-f682932c572d",
   "metadata": {},
   "source": [
    "# Mortality Statistics\n",
    "## Source Data For Mortality Statistics and Modelled Age Specific Survival Rates (Model Input Set By Control Assumptions)\n",
    "\n",
    "# LPA Control Assumptions\n",
    "## Specific Key Assumptions that control expected demand , LPA market size and saturation.\n",
    "\n",
    "\n",
    "# Meta data and Variable selection and Data Cleaning for the Mortality statastics data based on population projections:\n",
    "\n",
    "## Goal: \n",
    "### What proportion of the UK population are likely to buy LPA and still alive?\n",
    "*How many people are still alive (Living Donors bought LPA)*\n",
    "*Based on ONS Data of Population of Engalnd and Wales, how many people are still alive and how many of them are dead?*\n",
    "*e.g., if there are 1000 people and 100 of them are still alive and bought LPA,\n",
    "so there are 900 of them still didn't buy LPA.\n",
    "\n",
    "\n",
    "\n",
    "**1. These rates are standardised to the 2013 European Standard Population, expressed per million population; \n",
    "they allow comparisons between populations with different age structures, including between males and females and over time. \n",
    "**2.  Deaths per 1,000 live births. \n",
    "**3.  Death figures are based on deaths registered rather than deaths occurring in a calendar year.\n",
    "\n",
    "### For information on registration delays for a range of causes, see: \n",
    "    https://webarchive.nationalarchives.gov.uk/ukgwa/20160106020016/http://www.ons.gov.uk/ons/guide-method/user-guidance/health-and-life-events/impact-of-registration-delays-on-mortality-statistics/index.html\n",
    "\n",
    "A limiting factor in modelling numbers of surving LPA holders aged 90+ has been the absence of single age specific mortality rates \n",
    "for this group. Estimates* suggested that previously applied mortality rates were too low increasing the apparent numbers of \n",
    "surviving LPA holder saged 90+ and therefore over-estimating the \"sauration of this market.\n",
    "\n",
    "For the 2018 LPA forecast , Age specific mortality rates for those aged 90+ have therefore been extrapolated based on \n",
    "a standard log power law that best fits existing mortality rates to age. \n",
    "\n",
    "*numbers of surviving LPA holders were estimated to exceed the total projected  population in each age group which was \n",
    "clearly not possible.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b287ad4-7294-4a75-a9e7-71d4f397b8da",
   "metadata": {},
   "source": [
    "# LPA SURVIVAL TABLES:\n",
    " LPA MODEL/LPA SURVIVAL TABLES\n",
    "percentage of people are died in one year\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8aa6b618-1161-4001-94c2-df536c5a7953",
   "metadata": {},
   "source": [
    "# if a 1000 40 years old male bought an LPA in 2008, what proportion of are still alove today?\n",
    "\n",
    "# The model taking each age categories (categorical variable) and assumed that they are \n",
    "# singe age-specifics in the age category 18 to 90 and provide figure what percentage of people for male died within one year?\n",
    "\n",
    "## e.g., in the 15-19 age category, 0.3 percent of males died within one year in the UK and 0.03 per 1000\n",
    "## e.g., in the 25-29 age category, 0.6 percent of males died within one year in the UK and 0.06 per 1000\n",
    "## e.g., in the 70-74 age category, 23.7 percent of males died within one year in the UK or 2.37 per 1000\n",
    "\n",
    "## if you started at age 18, 7 years and become 25 years old ahead, \n",
    "## as the ages goes up you will fall into a higher mortality category (from 0.3 to 0.6)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7819c209-792e-47bb-ba4e-570d923b1c91",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
