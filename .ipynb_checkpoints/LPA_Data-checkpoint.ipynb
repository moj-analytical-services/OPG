{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f9dce19a-0b8e-4890-a9d2-d22dc9a1aba4",
   "metadata": {},
   "source": [
    "# OPG: LPA Data Pre-processing, Cleaning, Manipulation, Analysis, and insight tool\n",
    "\n",
    "#==============================================================================\n",
    "# @author: Dr. Leila Yousefi \n",
    "# MoJ Modelling Hub\n",
    "#==============================================================================# OPG : LPA Data Pre-processing and Cleaning tool\n",
    "\n",
    "==============================================================================\n",
    "\n",
    " OPG Demand Forecast modelling for LPA\n",
    " \n",
    " @author: Leila Yousefi\n",
    " \n",
    " MoJ Modelling Hub\n",
    " \n",
    "============================================================================== "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7517bdf9-08ba-4cc1-8c2f-b01ffdf7971d",
   "metadata": {},
   "source": [
    "## Setup\n",
    "\n",
    "Before you can run this project, you need to install some Python packages using the terminal:\n",
    "\n",
    "\n",
    "### create and activate  a virtual environment\n",
    "1. cd OPG\n",
    "2. python3 -m venv venv\n",
    "3. source venv/bin/activate\n",
    "\n",
    "### install the python packages required\n",
    "4. pip install --upgrade pip\n",
    "5. pip install -r requirements.txt\n",
    "\n",
    "### Updating your branch with main\n",
    "When working on your models it is likely that your branch will get out of date with the main branch. To update you branch with the latest changes from main open a terminal and run the following:\n",
    "\n",
    "Check your working tree, commit/push any changes if required\n",
    "\n",
    "git status\n",
    "Switch to the main branch and collect the latest changes, if any\n",
    "\n",
    "git switch main\n",
    "git fetch\n",
    "git pull\n",
    "Switch back to your branch and merge in the changes from main\n",
    "\n",
    "git switch <your initial>/model-a-development\n",
    "git merge main -m \"update branch with main\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf48e8d4-54cd-4d6e-aa17-92621c1bef97",
   "metadata": {},
   "source": [
    "# Installing the required packages:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e765f151-ee2c-44b2-8fcd-0a438f11633c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Un-comment and Run the below code if there is an error with packages installation:\n",
    "\n",
    "!pip install pip update\n",
    "!pip install panda update\n",
    "!pip install arrow_pd_parser\n",
    "!pip install pydbtools\n",
    "!pip install xlsxwriter\n",
    "!pip install holidays\n",
    "!pip install openpyxl\n",
    "!pip install panda update\n",
    "!pip install pymc3\n",
    "#!pip install statsforecast\n",
    "##You can add lines to install the required packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac0f18d1-54b8-4701-9e05-fc712195d057",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install pip update\n",
    "!pip install panda update\n",
    "!pip install scipy update\n",
    "!pip install pymc3 update\n",
    "!pip install numba update\n",
    "!pip install chex update"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12e26117-beda-4db7-9201-b6251522f2b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install --upgrade --force-reinstall pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ad94f13-cc0d-40b2-9ab1-c86098c257a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# make sure you are in the home directory : cd ..\n",
    "\n",
    "!conda create -c conda-forge -n pymc_env \"pymc>=5\"\n",
    "!source activate base\n",
    "!conda init\n",
    "!conda activate pymc_env\n",
    "!conda install numpyro\n",
    "!conda install blackjax\n",
    "!conda install -c conda-forge nutpie\n",
    "#!conda update -n base conda"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62438500-01c6-4434-89b1-37ebddfa540b",
   "metadata": {},
   "source": [
    "# Importing the required packages:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83bed50b-9b72-4d69-9326-fd1f690522fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import sys\n",
    "\n",
    "print(sys.path)\n",
    "\n",
    "sys.path.append('/opt/conda/lib/python3.9/site-packages')\n",
    "import os.path\n",
    "from os import path\n",
    "os.getcwd()\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "#import awswrangler as wr\n",
    "#import statsmodels.api as sm\n",
    "#import tensorflow as tf\n",
    "import boto3\n",
    "import getpass\n",
    "import pytz\n",
    "#import openpyxl\n",
    "\n",
    "\n",
    "import pymc as pm\n",
    "import theano.tensor as tt\n",
    "import arviz as az\n",
    "\n",
    "#import matplotlib\n",
    "import csv\n",
    "from arrow_pd_parser import reader, writer\n",
    "import shutil\n",
    "import pydbtools as pydb\n",
    "import datetime as dt\n",
    "from datetime import timedelta\n",
    "from datetime import datetime\n",
    "from datetime import date\n",
    "#import statsforecast\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "%matplotlib inline\n",
    "from dateutil.relativedelta import relativedelta\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from io import StringIO\n",
    "\n",
    "#from tensorflow import keras\n",
    "import matplotlib as mpl\n",
    "mpl.rcParams['agg.path.chunksize'] = 10000\n",
    "from matplotlib import rc\n",
    "import statsmodels.api as sm\n",
    "\n",
    "# consistent plot size wherever not specifiied\n",
    "from pylab import rcParams\n",
    "mpl.rcParams['figure.figsize'] = (15,8)\n",
    "mpl.rcParams['axes.grid'] = False\n",
    "rcParams['xtick.labelsize'] = 14\n",
    "rcParams['ytick.labelsize'] = 14\n",
    "rcParams['axes.labelsize'] = 14\n",
    "\n",
    "%matplotlib inline\n",
    "%config InlineBackend.figure_format='retina'\n",
    "\n",
    "import xlsxwriter\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib as mpl\n",
    "from matplotlib import rc\n",
    "import statsmodels.api as sm\n",
    "\n",
    "# consistent plot size wherever not specifiied\n",
    "from pylab import rcParams\n",
    "mpl.rcParams['figure.figsize'] = (15,8)\n",
    "mpl.rcParams['axes.grid'] = False\n",
    "rcParams['xtick.labelsize'] = 14\n",
    "rcParams['ytick.labelsize'] = 14\n",
    "rcParams['axes.labelsize'] = 14\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd602cf8-4885-43a5-82a9-429f8ce730b0",
   "metadata": {},
   "source": [
    "# LPA Data Import from Dom1\n",
    "\n",
    "This was used previously before transfering data to AWS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07194ab2-9571-406a-85f9-af8d699de142",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ## Enter the corresponding bucket name\n",
    "# bucketName = \"alpha-opg-analytical\"\n",
    "\n",
    "# ##For Automation import getpass\n",
    "# #bucketName = getpass.getpass()\n",
    "\n",
    "\n",
    "# ## Select the folder\n",
    "# folderPath = \"sirius_data_cuts_3\"\n",
    "\n",
    "\n",
    "# ## Set the folder in which the final output will be uploade to in S3\n",
    "# #output_path = f\"s3://alpha-opg-analytical/\" + folderPath + \"/\"\n",
    "\n",
    "# ## Then create a new excel file and copy the previous record from the S3 buckets and add the newly copied raws\n",
    "# ## Finaly convert the excel file to csv and upload it in the following path:\n",
    "# ## s3://alpha-opg-analytical/sirius_data_cuts_3/\n",
    "\n",
    "\n",
    "# ## Explore the s3 bucket path\n",
    "# path_s3 = f\"s3://{bucketName}/{folderPath}/*.csv\"\n",
    "# print ([path_s3])\n",
    "\n",
    "\n",
    "# ## Listing CSV Files in an S3 Bucket Folder: \n",
    "# ### To list all CSV files in a specific folder within an S3 bucket, we can use the AWS CLI or the boto3 Python library. \n",
    "# ###list all files in a specific folder within an S3 bucket Using AWS CLI:\n",
    "# #aws s3 ls s3://your-bucket-name/your-folder-name/ --recursive\n",
    "\n",
    "# ### lists all CSV files in a specific folder within an S3 bucket using boto3:\n",
    "# def list_csv_files(bucketName, folderPath):\n",
    "#     s3 = boto3.resource('s3')\n",
    "#     bucket = s3.Bucket(bucketName)\n",
    "#     for obj in bucket.objects.filter(Prefix=folderPath):\n",
    "#         if obj.key.endswith('.csv'):\n",
    "#             print(obj.key)\n",
    "\n",
    "\n",
    "# ## list all csv in the current folder in s3 bucket:\n",
    "# ### Getting the List of CSV Files\n",
    "# csv_files = list_csv_files(bucketName, folderPath)\n",
    "# print([csv_files])\n",
    "\n",
    "\n",
    "# ## Check if the path exists:\n",
    "# s3 = boto3.resource('s3')\n",
    "# bucket = s3.Bucket(bucketName)\n",
    "\n",
    "# def IsObjectExists(path):\n",
    "#     for object_summary in bucket.objects.filter(Prefix=path):\n",
    "#         return True\n",
    "#     return False\n",
    "\n",
    "# for fileName in [csv_files]:\n",
    "#     if(IsObjectExists(f\"{folderPath}/{fileName}\")):\n",
    "#         print(\"Path for the actual LPA data exists\")\n",
    "#     else:\n",
    "#         print(\"Path for the actual LPA data doesn't exists\")\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19756c99-6632-4f3f-b123-5f9394198786",
   "metadata": {},
   "source": [
    "# S3 Bucket Data Extraction for LPA Data (actuals)\n",
    "\n",
    "These will be used when extracting the raw data from the S3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abb24022-7229-4377-828f-44437c18cf9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download and upload the LPA actual data into the S3 bucket\n",
    "\n",
    "## Enter the corresponding S3 bucket name\n",
    "bucketName = \"alpha-opg-analytical\"\n",
    "\n",
    "##For Automation import getpass\n",
    "#bucketName = getpass.getpass()\n",
    "\n",
    "\n",
    "## Select the corresponding folder includes new LPA data in S3 bucket:\n",
    "folderPath = \"sirius_data_cuts_3\"\n",
    "\n",
    "\n",
    "## Set the folder in which the final output will be uploade to in S3\n",
    "#output_path = f\"s3://alpha-opg-analytical/\" + folderPath + \"/\"\n",
    "\n",
    "## Then create a new excel file and copy the previous record from the S3 buckets and add the newly copied raws\n",
    "## Finaly convert the excel file to csv and upload it in the following path:\n",
    "## s3://alpha-opg-analytical/sirius_data_cuts_3/\n",
    "\n",
    "# Initialize the S3 client\n",
    "s3_client = boto3.client('s3')\n",
    "\n",
    "# Specify your bucket name and folder (prefix)\n",
    "bucket_name = bucketName\n",
    "folder_prefix = 'sirius_data_cuts_3/'\n",
    "\n",
    "# List objects in the specified folder\n",
    "response = s3_client.list_objects_v2(Bucket=bucket_name, Prefix=folder_prefix)\n",
    "\n",
    "# Extract the keys (file names) from the response\n",
    "file_keys = [obj['Key'] for obj in response.get('Contents', [])]\n",
    "\n",
    "# Filter out None values (if any)\n",
    "non_none_file_keys = [key for key in file_keys if key is not None]\n",
    "#print(non_none_file_keys)\n",
    "\n",
    "# Remove folder prefix from file keys\n",
    "file_names = [os.path.basename(key) for key in non_none_file_keys]\n",
    "#print(file_names)\n",
    "\n",
    "csv_extension = '.csv'\n",
    "filtered_file_names = [fn for fn in file_names if fn.lower().endswith(csv_extension)]\n",
    "\n",
    "print(filtered_file_names)\n",
    "\n",
    "# ## Explore the s3 bucket path\n",
    "# path_s3 = f\"s3://{bucketName}/{folderPath}/\"\n",
    "# print ([path_s3])\n",
    "\n",
    "# ## Check if the path exists:\n",
    "# s3 = boto3.resource('s3')\n",
    "# bucket = s3.Bucket(bucketName)\n",
    "\n",
    "# def IsObjectExists(path):\n",
    "#     for object_summary in bucket.objects.filter(Prefix=path):\n",
    "#         return True\n",
    "#     return False\n",
    "\n",
    "# if(IsObjectExists(path_s3)):\n",
    "#     print(\"Path for the actual data exists\")\n",
    "# else:\n",
    "#     print(\"Path for the actual data doesn't exists\")\n",
    "\n",
    "\n",
    "## Listing CSV Files in an S3 Bucket Folder: \n",
    "### To list all CSV files in a specific folder within an S3 bucket, we can use the AWS CLI or the boto3 Python library. \n",
    "###list all files in a specific folder within an S3 bucket Using AWS CLI:\n",
    "#aws s3 ls s3://your-bucket-name/your-folder-name/ --recursive\n",
    "\n",
    "# ### lists all CSV files in a specific folder within an S3 bucket using boto3:\n",
    "# def list_csv_files(bucketName, folderPath):\n",
    "#     s3 = boto3.resource('s3')\n",
    "#     bucket = s3.Bucket(bucketName)\n",
    "#     for obj in bucket.objects.filter(Prefix=folderPath):\n",
    "#         if obj.key.endswith('.csv'):\n",
    "#             print(obj.key)\n",
    "\n",
    "\n",
    "# ## list all csv in the current folder in s3 bucket:\n",
    "# ### Getting the List of CSV Files\n",
    "# csv_files = list_csv_files(bucketName, folderPath)\n",
    "# print([csv_files])\n",
    "\n",
    "\n",
    "# ## Opening CSV Files Based on Selected Column and Condition: \n",
    "# def process_csv_file(file_path, selected_column, condition):\n",
    "#     df = pd.read_csv(file_path)\n",
    "#     filtered_df = df[df[selected_column] == condition]\n",
    "#     # Do further processing with the filtered data\n",
    "\n",
    "# ## pre-process the csv file to only show the required columns (important variable for the BAU):\n",
    "# process_csv_file('path/to/your-csv-file.csv', 'column_name', 'desired_value')\n",
    "\n",
    "\n",
    "# # ## Enter your file name\n",
    "# # fileName = \"d24_2.csv\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3556dfa4-cdcc-49b2-be16-811d5528b39c",
   "metadata": {},
   "source": [
    "# Query the warehouse tables directly from Python/R"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6db194d-65ed-489c-b1d5-1e4f13a3a7aa",
   "metadata": {},
   "source": [
    "\n",
    "    \"\"\"\n",
    "    with events as (\n",
    "        select *\n",
    "        from \"dim_guardianship_dev_dbt\".\"fct_case_receipts\"\n",
    "        where extract_type = 'latest_extract'\n",
    "            and receipt_date >= date_parse('01-01-2008', '%d-%m-%Y')\n",
    "    ),\n",
    "    dates as (\n",
    "        select *\n",
    "        from \"common_lookup_dev_dbt\".\"dim_date\"\n",
    "    ),\n",
    "    donors as (\n",
    "        select *\n",
    "        from \"dim_guardianship_dev_dbt\".\"dim_donors\"\n",
    "    ),\n",
    "    cases as (\n",
    "        select *\n",
    "        from \"dim_guardianship_dev_dbt\".\"dim_cases\"\n",
    "    ),\n",
    "    attributes as (\n",
    "        select dates.calendar_year as receipt_year,\n",
    "            events.receipt_date,\n",
    "            cases.case_id,\n",
    "            cases.case_type,\n",
    "            cases.case_subtype,\n",
    "            cases.case_status,\n",
    "            cases.donor_age_at_receipt,\n",
    "            donors.gender,\n",
    "            donors.region_name,\n",
    "            events.extract_date\n",
    "        from events\n",
    "            left join dates on events.receipt_date = dates.date_name\n",
    "            left join cases on events.extract_case_id = ces.extract_case_id\n",
    "            left join donors on events.extract_donor_id = donors.extract_donor_id\n",
    "    )\n",
    "    select *\n",
    "    from attributes\n",
    "    \"\"\"\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e82badb8-20ff-494f-8f18-ea433d17f6a0",
   "metadata": {},
   "source": [
    "# Reading in Data\n",
    "\n",
    "This extracts a list of Power of Attorney receipts with the following columns: ['receiptdate', 'uid', 'type', 'casesubtype', 'status', 'donor_postcode', 'donor_gender', 'age']."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92662597-d37b-417e-b335-5bfb590c8b39",
   "metadata": {},
   "outputs": [],
   "source": [
    "# read_csv_files function for Reading in CSV Files in an S3 Bucket Folder\n",
    "\n",
    "def read_csv_files(bucket_name, file_names, selected_columns):\n",
    "    \"\"\"\n",
    "        This function is written to read in the data from all of CSV files in the corresponding directory in the S3 bucket\n",
    "        by using input variables:\n",
    "        the S3 bucket name,\n",
    "        file_names \n",
    "        and the selected_columns \n",
    "        The output are the CSV files in the list of dataframes: dfs \n",
    "    \"\"\"\n",
    "    dfs = {}  # Dictionary to store DataFrames\n",
    "\n",
    "    # Initialize the S3 client\n",
    "    s3_client = boto3.client('s3')\n",
    "\n",
    "    for file_name in file_names:\n",
    "        s3_path = f's3://{bucket_name}/{file_name}'\n",
    "        try:\n",
    "            # Read the CSV data into a Pandas DataFrame\n",
    "            csv_obj = s3_client.get_object(Bucket=bucket_name, Key=f'{folderPath}/{file_name}')\n",
    "            csv_string = csv_obj['Body'].read().decode('utf-8')\n",
    "            df = pd.read_csv(StringIO(csv_string))\n",
    "\n",
    "            # Select specific columns\n",
    "            df_selected = df[selected_columns]\n",
    "            dfs[file_name] = df_selected\n",
    "        except Exception as e:\n",
    "            print(f\"Error reading {file_name}: {e}\")\n",
    "\n",
    "    return dfs\n",
    "\n",
    "\n",
    "#bucket_name = bucketName\n",
    "#file_names = ['file1.csv', 'file2.csv']  # Replace with your actual file names\n",
    "\n",
    "## Filter the required variables from the datafarame:\n",
    "selected_columns = [\"receiptdate\",\"cases_glueexporteddate\",\"uid\",\"type\",\"casesubtype\",\"status\",\"donor_dob\",\"donor_postcode\",\"donor_gender\"]  # Replace with desired column names\n",
    "\n",
    "## The read_csv_files function for Reading in CSV Files in an S3 Bucket Folder:\n",
    "dataframes = read_csv_files(bucket_name, filtered_file_names, selected_columns)\n",
    "\n",
    "## Access individual DataFrames by file name\n",
    "for file_name, df_selected in dataframes.items():\n",
    "    print(f\"DataFrame for {file_name}:\")\n",
    "    print(df_selected.head())\n",
    "    \n",
    "## Concatenating DataFrames: \n",
    "### After reading all CSV files, you can concatenate the DataFrames using pd.concat:\n",
    "combined_df = pd.concat(dataframes, ignore_index=True)\n",
    "print(combined_df)\n",
    "\n",
    "## Writing Back to S3: Finally, write the combined DataFrame back to S3:\n",
    "#combined_data_encoded = combined_df.to_csv(None, index=False).encode('utf-8')\n",
    "#combined_file_name = 'combined_data.csv'  # Choose a suitable file name\n",
    "#s3_client.put_object(Body=combined_data_encoded, Bucket=bucket_name, Key=combined_file_name\n",
    "\n",
    "## Identify the type of data set and pre-processing: \n",
    "## Import, manipulate, and clean the data and impute missing values\n",
    "\n",
    "## Column renaming:\n",
    "#df1.rename(columns={'old_col1': 'common_col1', 'old_col2': 'common_col2'}, inplace=True)\n",
    "\n",
    "## Handling Data Mismatch:\n",
    "###Be cautious when combining data with different structures. If a column has incompatible data types (e.g., mixing strings and numbers), you may need to convert or handle them appropriately.\n",
    "#combined_df['numeric_col'] = pd.to_numeric(combined_df['numeric_col'], errors='coerce')\n",
    "\n",
    "## Aggregating Data:\n",
    "###If the DataFrames have different structures, consider aggregating them based on a common identifier (e.g., date or unique ID).\n",
    "#combined_df = df1.groupby('product_id').sum()  # Aggregate by product ID\n",
    "\n",
    "## merge DataFrames based on a common identifier:\n",
    "#merged_df = pd.merge(df1, df2, on='ID', how='inner')\n",
    "#print(merged_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f991a0c6-dc43-4c09-83e6-773c016d3d7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#     ## Select Date\n",
    "#     start_date = '2018-06-01' # start date for the train set\n",
    "#     start_prediction ='2023-02-01' # The end date for the train set\n",
    "#     end_prediction ='2024-02-01' # test / Validation set\n",
    "\n",
    "\n",
    "\n",
    "## Import the dataset and read in the actual data\n",
    "#df = wr.s3.read_csv([path1_s3], sep = ',', parse_dates=True) #import divorce data\n",
    "#read data\n",
    "#def parser(s):\n",
    "#    return datetime.strptime(s, '%Y-%m-%d')\n",
    "#df = wr.s3.read_csv([path1_s3], parse_dates=[0], index_col=0, squeeze=True, date_parser=parser)\n",
    "## iterating the columns\n",
    "#for col in df.columns:\n",
    "#    print(col)\n",
    "\n",
    "\n",
    "#lpa=LPA_data[[\"receiptdate\",\"cases_glueexporteddate\",\"uid\",\"type\",\"casesubtype\",\"status\",\"donor_dob\",\"donor_postcode\",\"donor_gender\"]]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5268ef2b-4e72-4080-a918-e7bfddc40822",
   "metadata": {},
   "source": [
    "# Automating the input dates to forecast LPAs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93434d88-6814-4c5b-8d96-e33a7eaa70d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the date you want to extract data based on the latest date extrated LPA data\n",
    "\n",
    "## Grab part of filename\n",
    "fist_CSV_fileName = filtered_file_names[1]\n",
    "snapshot_end = fist_CSV_fileName.split('opg-analytical_cases_P')[1].lstrip().split('_S')[0]\n",
    "#snapshot_end\n",
    "\n",
    "#snapshot_end = final_df.values[7].astype(str)[7]\n",
    "\n",
    "# Automating the input dates to forecast\n",
    "\n",
    "## Select Start Date\n",
    "p = getpass.getpass(prompt='Do you want to change the starting date for forecasting? (Choose Yes=Y OR No=N)')\n",
    " \n",
    "if (p.lower() == 'n') | (p.lower() == ''): #defult start date\n",
    "    snapshot_start = '2006-12-31'\n",
    "    print('You have not choosen to change the start date, the default start date is: ' + snapshot_start)\n",
    "    ## The first date to be considered:\n",
    "else:    \n",
    "    ## Select Start Date\n",
    "    print('You have choosen to change the start date.')\n",
    "    snapshot_start = input('Enter the period_start date (for training): e.g., \"2006-12-31\" (in single qoutes)')\n",
    "    print('snapshot_start: ' + snapshot_start)\n",
    "\n",
    "## Select End Date    \n",
    "d = getpass.getpass(prompt='Do you want to change the ending date for forecasting? (Choose Yes=Y OR No=N)')\n",
    " \n",
    "if (d.lower() == 'n') | (d.lower() == ''): #defult start date\n",
    "    snapshot_end = snapshot_end # '2024-06-31'\n",
    "    print('You have not choosen to change the end date, the default end date is: ' + snapshot_end)\n",
    "    ## The end date to be considered:\n",
    "else:    \n",
    "    ## Select Date\n",
    "    print('You have choosen to change the end date.')\n",
    "    snapshot_end = input('Enter the period_end date (for training): e.g., \"2006-12-31\" (in single qoutes)')\n",
    "    print('snapshot_end: ' + snapshot_end)    \n",
    "# LPA historical data: '2018-01-01'\n",
    "# Covid data: '2021-01-01'\n",
    "# Post Covid data: '2022-01-01'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "197c09c2-54bc-42df-b55b-ea188f2babb5",
   "metadata": {},
   "source": [
    "# Data pre-processing and cleaning - data engineering"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2fc0e1c-b824-4822-a8e5-20041f465a65",
   "metadata": {},
   "source": [
    "## Meta data and Variable selection and Data Cleaning for the LPA data in Data Warehouse:\n",
    "\n",
    "Goal: to work out how many people applied for lpa and recieved the power of atthorney and how many applications in a year/month/week by age group since 2007? \n",
    "\n",
    "### ages over 19 years old\n",
    "\n",
    "#### Unique case reference for each donor = [donor_dob + donor_postcode + donor_gender]\n",
    "\n",
    "##### Sort by the unique id and count how many application\n",
    "\n",
    "###### and then dermine Whether the application type [casesubtype] is hw=health and welfare or pfa=property and finance\n",
    "\n",
    "###### how many certificate provider (cp) for each lpa application?\n",
    "\n",
    "###### Location based data and geographical data for the donor can be used to identify the financial situation and wherether they are located in England or Wales\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50385b9d-958f-4a48-958b-bee05777d4cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Filter the records:\n",
    "df_filtered = combined_df\n",
    "\n",
    "## Convert the receipt date to date format \n",
    "df_filtered['receiptdate'] = pd.to_datetime(df_filtered['receiptdate'], errors = 'coerce') #.dt.date\n",
    "\n",
    "## Filter records between the selected dates\n",
    "df_filtered = df_filtered.loc[(df_filtered['receiptdate'] > pd.to_datetime(snapshot_start))]\n",
    "df_filtered = df_filtered.loc[(df_filtered['receiptdate'] < pd.to_datetime(snapshot_end))]\n",
    "\n",
    "## Filter the dataframe to select only lpa type records\n",
    "df_filtered = df_filtered.loc[(df_filtered['type'] == 'lpa')]\n",
    "\n",
    "# Create a dataframe of the selected columns\n",
    "## Select the appropriate variable to be forecasted\n",
    "df = df_filtered[[\"receiptdate\",\"uid\",\"casesubtype\",\"status\",\"donor_dob\",\"donor_postcode\",\"donor_gender\"]]\n",
    "\n",
    "## Remove Null values and records\n",
    "lpa_df = df.dropna()\n",
    "\n",
    "# Extract age by subtracting 'receiptdate' and 'donor_dob'\n",
    "lpa_df['age'] = pd.to_datetime(lpa_df['receiptdate'], errors = 'coerce').dt.year - pd.to_datetime(lpa_df['donor_dob'], errors = 'coerce').dt.year\n",
    "#lpa_df['age'] = relativedelta(date, dob).years\n",
    "\n",
    "# Convert the donor_dob column to a datatime format\n",
    "lpa_df['donor_dob'] = pd.to_datetime(lpa_df['donor_dob'], errors = 'coerce').dt.date\n",
    "\n",
    "# Convert the ‘receiptdate’ column to datetime format for proper plotting.\n",
    "# Convert 'receiptdate' to datetime format \n",
    "lpa_df['receiptdate'] = pd.to_datetime(lpa_df['receiptdate'], errors='coerce')\n",
    "\n",
    "# Extract year from 'receiptdate'\n",
    "lpa_df['year'] = lpa_df['receiptdate'].dt.year\n",
    "\n",
    "## Set index\n",
    "#df['receiptdate'] = pd.to_datetime(df['receiptdate'])\n",
    "\n",
    "#df = df.set_index('receiptdate').asfreq('D')\n",
    "\n",
    "####df['receiptdate'] = df.set_index('receiptdate',inplace=True)\n",
    "\n",
    "#df.index = df.index.to_period('D')\n",
    "                            \n",
    "print(lpa_df.head())\n",
    "print(lpa_df.tail())\n",
    "\n",
    "\n",
    "#lpa_df['age'] = pd.to_datetime(df['receiptdate'], errors = 'coerce').dt.date - pd.to_datetime(df['donor_dob'], errors = 'coerce').dt.date\n",
    "#lpa_df['receiptdate'] = pd.to_datetime(lpa_df['receiptdate']).dt.date#.apply(lambda x: x.strftime('%Y-%m-%d'))\n",
    "#print(lpa_df)#['receiptdate']\n",
    "#lpa_df\n",
    "\n",
    "#print(lpa_df['age'])\n",
    "\n",
    "## infer the frequency of the data:\n",
    "###lpa_df = df\n",
    "\n",
    "#lpa_df = df.asfreq(pd.infer_freq(df.index))\n",
    "\n",
    "#lpa_df = lpa_df[start_date:end_date]\n",
    "\n",
    "#start_date_years = datetime.strptime(start_date, \n",
    "#                                     '%Y-%m-%d') + relativedelta(years = 0)\n",
    "#print(start_date_years)\n",
    "\n",
    "#start_date_formatted = start_date_years.date()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "015e48bf-b1c7-4d9c-8521-e4c8619c8327",
   "metadata": {},
   "source": [
    "# Visualisation of the time series\n",
    "## Virtualisation of the LPA Data:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "232c0736-d6df-4b90-9ad4-5f33b5f5bd1e",
   "metadata": {},
   "source": [
    "# Plot 'age' against 'receiptdate'\n",
    "\n",
    "## --------------------------------------------------------------------------------  ##\n",
    "- Create a scatter plot with ‘receiptdate’ as the x-axis and ‘age’ as the y-axis.\n",
    "- Display the plot with appropriate labels and a grid.\n",
    "\n",
    "plt.figure(figsize=(20, 10))\n",
    "plt.scatter(lpa_df['receiptdate'], lpa_df['age'], alpha=0.5)\n",
    "plt.title('Age vs Receipt Date')\n",
    "plt.xlabel('Receipt Date')\n",
    "plt.ylabel('Age')\n",
    "plt.grid(True)\n",
    "plt.show()\n",
    "\n",
    "## --------------------------------------------------------------------------------  ##\n",
    "# Create a histogram of the 'age' column\n",
    "\n",
    "- This code will produce a histogram that displays the frequency distribution of ages in your dataset. \n",
    "- The bins parameter determines the number of bins used in the histogram, and you can adjust this number\n",
    "- to change the granularity of your histogram.\n",
    "\n",
    "plt.figure(figsize=(20, 10))\n",
    "plt.hist(lpa_df['age'], bins=20, alpha=0.7, color='blue')\n",
    "plt.title('Age Distribution')\n",
    "plt.xlabel('Age')\n",
    "plt.ylabel('Frequency')\n",
    "plt.grid(True)\n",
    "plt.show()\n",
    "\n",
    "## --------------------------------------------------------------------------------  ##\n",
    "# Create a line chart of age against receipt date\n",
    "- Sort the DataFrame by 'receiptdate' to ensure the line chart is ordered\n",
    "lpa_df.sort_values('receiptdate', inplace=True)\n",
    "\n",
    "- Plot 'age' against 'receiptdate' using a line chart\n",
    "plt.figure(figsize=(20, 10))\n",
    "plt.plot(lpa_df['receiptdate'], lpa_df['age'], marker='o', linestyle='-', color='blue')\n",
    "plt.title('Age vs Receipt Date')\n",
    "plt.xlabel('Receipt Date')\n",
    "plt.ylabel('Age')\n",
    "plt.grid(True)\n",
    "\n",
    "## --------------------------------------------------------------------------------  ##\n",
    "- Produce a line chart that displays the average age of individuals for each year based on the receipt dates in your dataset.\n",
    "- The data points are connected with a line, which helps in identifying any trends or patterns over the years.\n",
    "\n",
    "- Group the data by year and calculate the average age for each year\n",
    "age_by_year = lpa_df.groupby('year')['age'].mean().reset_index()\n",
    "\n",
    "- Plot 'age' against 'year' using a line chart\n",
    "plt.figure(figsize=(20, 10))\n",
    "plt.plot(age_by_year['year'], age_by_year['age'], marker='o', linestyle='-', color='blue')\n",
    "plt.title('Average Age vs Year')\n",
    "plt.xlabel('Year')\n",
    "plt.ylabel('Average Age')\n",
    "plt.grid(True)\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0817f1a4-4782-4762-bd6e-715e96728f49",
   "metadata": {},
   "source": [
    "## Observations:\n",
    "The trend in the line chart indicates the changes in the average age of individuals over the years, \n",
    "based on the receipt dates from your dataset.\n",
    "Such a visualization can help identify patterns, \n",
    "such as whether the average age is increasing, decreasing, or remaining relatively stable over time.\n",
    "\n",
    "For example:\n",
    "An upward trend would suggest that the average age is increasing each year.\n",
    "A downward trend would indicate that the average age is decreasing.\n",
    "A flat line would imply that there is little to no change in the average age over the years.\n",
    "These trends can be influenced by various factors, such as the demographics of the population being studied, \n",
    "changes in policies, or other external factors that might affect the age distribution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9819b606-ffd9-4e8c-bcc6-ac54e81cf4cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Plot the Actuals\n",
    "\n",
    "# lpa_series = lpa_df['age']\n",
    "# #lpa_series = df.squeeze()\n",
    "# plt.figure(figsize=(28, 14))\n",
    "# plt.plot(lpa_series)\n",
    "# plt.title('UK Actual LPA Data', fontsize=20)\n",
    "# plt.ylabel('Age', fontsize=16)\n",
    "# plt.axvline(pd.to_datetime(df['receiptdate'], errors = 'coerce').dt.year, color = 'k', linestyle='--', alpha = 0.2)\n",
    "# # for year in range(min(pd.to_datetime(df['receiptdate'], errors = 'coerce').dt.year), \n",
    "# #     datetime.strptime(snapshot_end, '%Y-%m-%d').year):\n",
    "# #     #datetime.strptime(\"2024-03-18\", '%Y-%m-%d').year):\n",
    "# #     plt.axvline(pd.to_datetime(df['receiptdate'], errors = 'coerce'), color = 'k', linestyle='--', alpha = 0.2)\n",
    "# #     #plt.axvline(pd.to_datetime(str(year) + '-01-01'), color = 'k', \n",
    "# #     #print(year)\n",
    "# plt.legend()    \n",
    "# #plt.savefig('UK_Actual_LPA_Data.png', dpi=300, bbox_inches='tight')\n",
    "# plt.show()  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d9843c8-2905-43ae-9974-1c184aeba587",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Perform the GROUP BY operation and calculate the count\n",
    "#Cases_by_year_age = lpa_df.groupby(\n",
    "#    ['receiptdate', 'uid', 'type', 'casesubtype', 'status', 'donor_postcode', 'donor_gender', 'age']) \\\n",
    "#    .agg({'No_of_Cases': 'count'}) \\ #['donor_postcode', 'donor_gender', 'age']\n",
    "#    .reset_index()\n",
    "\n",
    "#agg_funcs = dict(No_of_Cases = 'count')\n",
    "#Cases_by_year_age = lpa_df.set_index(['receiptdate', 'uid', 'type', 'casesubtype', 'status', 'donor_postcode', 'donor_gender', 'age']) \\\n",
    "#    .stack() \\\n",
    "#    .groupby(level=0) \\\n",
    "#    .agg(agg_funcs)\n",
    "\n",
    "\n",
    "#Cases_by_year_age\n",
    "#lpa_by_year_age = lpa_df[['receiptdate', 'uid', 'type', 'casesubtype', 'status', 'donor_postcode', 'donor_gender', 'age']] \\\n",
    "#                    .groupby(['donor_postcode', 'donor_gender', 'age'])  \\\n",
    "#                    .agg('count')#.sum()\n",
    "#lpa_by_year_age.to_csv(r'lpa_by_year_age.csv')\n",
    "\n",
    "\n",
    "\n",
    "#lpa_df.to_csv(r'lpa_df.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49ed0a3d-b058-41e4-8cbc-61cd51c67b72",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Missing Data Imputation:\n",
    "\n",
    "There are be some people in the LPA data with missing age (they are represented with negetive numbers in column age). \n",
    "So for missing data (age) imputation, his code is written to use age distribution of cases that they have age and\n",
    "apply this to the total number of doners in that year. \n",
    "Actually, we allocate proportionaly distributed age across each year of these missing ages. \n",
    "E.g., if we get 90% of age distribution for a particular year,\n",
    "we used this age distribution to be applied to the 100% of donors to get the total distribution. \n",
    "\n",
    "The code below: \n",
    "first, loads the data from the CSV file and replaces negative ages \n",
    "with NaN to represent missing data. \n",
    "\n",
    "It then calculates the age distribution for each year. \n",
    "\n",
    "For each year, it finds the indices of the missing ages and imputes \n",
    "them by randomly choosing from the age distribution of that year. \n",
    "\n",
    "The imputed ages are proportional to the age distribution \n",
    "of the donors that year. \n",
    "\n",
    "Finally, it saves the DataFrame with the imputed ages to a new CSV file.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94e0a4fd-f395-40b0-b785-c361f1b11029",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Function to calculate the number of unique records by age, year, gender, and postcode\n",
    "# #def calculate_unique_records_by_age_year_gender_postcode(records):\n",
    "# # Get the current year\n",
    "# #current_year = datetime.now().year  \n",
    "# #Create a dictionary to store counts for each age, year, gender, and postcode combination\n",
    "# age_year_gender_postcode_counts = {}\n",
    "\n",
    "# records = lpa_df\n",
    "    \n",
    "# # Iterate over each record\n",
    "# for record in records:         \n",
    "#     # Extract gender and postcode\n",
    "#     gender = record[\"donor_gender\"]\n",
    "#     postcode = record[\"donor_postcode\"]\n",
    "#     dob = record[\"donor_dob\"]\n",
    "    \n",
    "#     # Create a unique key combining age, gender, and postcode\n",
    "#     key = (dob, gender, postcode)\n",
    "        \n",
    "#     # Increment the count for the key\n",
    "#     age_year_gender_postcode_counts[key] = age_year_gender_postcode_counts.get(key, 0) + 1\n",
    "        \n",
    "# return age_year_gender_postcode_counts\n",
    "\n",
    "# # Call the function and print the results\n",
    "# unique_records_by_age_year_gender_postcode = calculate_unique_records_by_age_year_gender_postcode(records)\n",
    "\n",
    "# print(\"Number of unique records by age, year, gender, and postcode:\")\n",
    "\n",
    "# for key, count in unique_records_by_age_year_gender_postcode.items():\n",
    "#     dob, gender, postcode = key\n",
    "#     print(f\"Date of Birth (D.o.B): {dob}, Gender: {gender}, Postcode: {postcode}, Count: {count}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1ce1de0-088c-4447-a432-f0dce3facfee",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This code will output the number of unique records for each age in each year for each donor gender in each donor postcode.\n",
    "# It calculates the age based on the current year and the birth year of each person in the records.\n",
    "# Then, it creates a unique key combining age, year, gender, and postcode, and increments the count for each key.\n",
    "# Finally, it prints the results showing the count of unique records for each combination.\n",
    "\n",
    "#from datetime import datetime\n",
    "\n",
    "# Sample data representing records with donor gender, donor postcode, and date of birth\n",
    "#records = [\n",
    "#    {\"donor_gender\": \"Male\", \"donor_postcode\": \"AB12 3CD\", \"date_of_birth\": \"1999-05-15\"},\n",
    "#    {\"donor_gender\": \"Female\", \"donor_postcode\": \"XY34 5YZ\", \"date_of_birth\": \"1994-08-20\"},\n",
    "#    {\"donor_gender\": \"Male\", \"donor_postcode\": \"CD56 7EF\", \"date_of_birth\": \"1996-02-10\"},\n",
    "#    {\"donor_gender\": \"Male\", \"donor_postcode\": \"FG78 9HI\", \"date_of_birth\": \"2000-11-30\"},\n",
    "#    {\"donor_gender\": \"Female\", \"donor_postcode\": \"JK90 1LM\", \"date_of_birth\": \"1987-03-25\"},\n",
    "#    {\"donor_gender\": \"Male\", \"donor_postcode\": \"OP23 4QR\", \"date_of_birth\": \"1993-09-05\"}\n",
    "#]\n",
    "\n",
    "# Function to calculate the number of unique records by age, year, gender, and postcode\n",
    "#def calculate_unique_records_by_age_year_gender_postcode(records):\n",
    "    # Get the current year\n",
    "#    current_year = datetime.now().year\n",
    "    \n",
    "    # Create a dictionary to store counts for each age, year, gender, and postcode combination\n",
    "#    age_year_gender_postcode_counts = {}\n",
    "    \n",
    "    # Iterate over each record\n",
    "#    for record in records:\n",
    "        # Extract the year of birth from the date_of_birth\n",
    "#        birth_year = int(record[\"date_of_birth\"].split(\"-\")[0])\n",
    "        \n",
    "        # Calculate the age of the person\n",
    "#        age = current_year - birth_year\n",
    "        \n",
    "        # Extract the year from the date_of_birth\n",
    "#        year = birth_year\n",
    "        \n",
    "        # Extract gender and postcode\n",
    "#        gender = record[\"donor_gender\"]\n",
    "#        postcode = record[\"donor_postcode\"]\n",
    "        \n",
    "        # Create a unique key combining age, year, gender, and postcode\n",
    "#        key = (age, year, gender, postcode)\n",
    "        \n",
    "        # Increment the count for the key\n",
    "#        age_year_gender_postcode_counts[key] = age_year_gender_postcode_counts.get(key, 0) + 1\n",
    "        \n",
    "#    return age_year_gender_postcode_counts\n",
    "\n",
    "# Call the function and print the results\n",
    "#unique_records_by_age_year_gender_postcode = calculate_unique_records_by_age_year_gender_postcode(records)\n",
    "#print(\"Number of unique records by age, year, gender, and postcode:\")\n",
    "#for key, count in unique_records_by_age_year_gender_postcode.items():\n",
    "#    age, year, gender, postcode = key\n",
    "#    print(f\"Age: {age}, Year: {year}, Gender: {gender}, Postcode: {postcode}, Count: {count}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07154148-04e8-4096-918b-625a4fed04b8",
   "metadata": {},
   "source": [
    "# Missing age imutation\n",
    "\n",
    "There are two issues with the age:\n",
    "\n",
    "1. The donor_gender might be missing or entered incorrectly\n",
    "\n",
    "2. The derieved age might be higher than 126 years old\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68ed109d-7ce6-4e76-a6f2-0712c10f1cc2",
   "metadata": {},
   "outputs": [],
   "source": [
    "lpa_data_sample_imputed = lpa_df\n",
    "\n",
    "# Filter rows with negative or greater than 126 age values\n",
    "criteria = lpa_data_sample_imputed[(lpa_data_sample_imputed['age'] < 0) | (lpa_data_sample_imputed['age'] > 126)]\n",
    "\n",
    "# Display the filtered rows\n",
    "print(criteria)\n",
    "\n",
    "# Replace age values with NULL (NaN) in the filtered rows\n",
    "lpa_data_sample_imputed.loc[criteria.index, 'age'] = np.nan #None\n",
    "\n",
    "# Display the updated DataFrame\n",
    "print(lpa_data_sample_imputed)\n",
    "\n",
    "# Group by year and count age groups\n",
    "age_distribution = lpa_data_sample_imputed.groupby('year')['age'].value_counts()\n",
    "\n",
    "# Fill missing ages with the most common age for each year\n",
    "most_common_age = lpa_data_sample_imputed.groupby('year')['age'].apply(lambda x: x.mode().iloc[0])\n",
    "lpa_data_sample_imputed['age'] = lpa_data_sample_imputed.apply(lambda row: most_common_age[row['year']] if pd.isna(row['age']) else row['age'], axis=1)\n",
    "\n",
    "# Display the age distribution after filling missing ages\n",
    "print(\"\\nAge distribution by year (including filled missing ages):\")\n",
    "print(age_distribution)\n",
    "\n",
    "# Display the final DataFrame\n",
    "print(\"\\nFinal DataFrame:\")\n",
    "#print(lpa_data_sample_imputed)\n",
    "\n",
    "# Save the dataframe with imputed ages\n",
    "#lpa_data_sample_imputed.to_csv('lpa_data_sample_imputed.csv', index=False)\n",
    "\n",
    "# Print a success message\n",
    "print(\"The missing age data has been successfully imputed and saved to lpa_data_sample_imputed.csv file.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5dedfc27-b664-4e22-b06a-ef0bf64c9765",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# lpa_data_sample_imputed = lpa_df\n",
    "\n",
    "\n",
    "# # Identify the rows with missing age (represented as negative numbers)\n",
    "# ## 1. The donor_gender might be missing or entered incorrectly:  < 0\n",
    "# ## 2. The derieved age might be higher than 126 years old > 126\n",
    "# lpa_data_sample_imputed['missing_age'] = (lpa_data_sample_imputed['age'] < 0) | (lpa_data_sample_imputed['age'] > 126)\n",
    "\n",
    "# # Replace negative ages with NaN\n",
    "# lpa_data_sample_imputed.loc[missing_age, 'age'] = np.nan\n",
    "\n",
    "# # Calculate the age distribution for each year excluding missing ages\n",
    "# age_distribution = lpa_data_sample_imputed.loc[~missing_age].groupby('year')['age'].value_counts(normalize=True)\n",
    "\n",
    "# # Calculate the age distribution for each year\n",
    "# age_distribution_per_year = lpa_data_sample_imputed.groupby('year')['age'].value_counts(normalize=True)\n",
    "\n",
    "# # Apply the age distribution to the total number of donors in each year\n",
    "# for year in df['year'].unique():\n",
    "#     # Calculate the number of missing ages in the current year\n",
    "#     num_missing = missing_age & (df['year'] == year)\n",
    "    \n",
    "#     # If there are missing ages in the current year\n",
    "#     if num_missing.sum() > 0:\n",
    "#         # Generate ages according to the age distribution of the current year\n",
    "#         imputed_ages = np.random.choice(age_distribution[year].index, \n",
    "#                                         p=age_distribution[year].values, \n",
    "#                                         size=num_missing.sum())\n",
    "        \n",
    "#         # Assign the generated ages to the missing ages\n",
    "#         df.loc[num_missing, 'age'] = imputed_ages\n",
    "\n",
    "\n",
    "# # Apply the age distribution to the missing ages\n",
    "# for year in lpa_data_sample_imputed['year'].unique():\n",
    "#     missing_age_indices = lpa_data_sample_imputed[(lpa_data_sample_imputed['year'] == year) & (lpa_data_sample_imputed['age'].isna())].index\n",
    "#     if not missing_age_indices.empty:\n",
    "#         imputed_ages = np.random.choice(age_distribution_per_year[year].index, \n",
    "#                                         p=age_distribution_per_year[year].values, \n",
    "#                                         size=len(missing_age_indices))\n",
    "#         lpa_data_sample_imputed.loc[missing_age_indices, 'age'] = imputed_ages\n",
    "\n",
    "      \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0916770-95b6-4bff-8b14-b1d103bc585a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a unique identifier based on multiple columns:\n",
    "# lpa_unique_key = lpa_df\n",
    "\n",
    "\n",
    "# #df1.set_index(['donor_postcode', 'donor_gender', 'age']).index.factorize()[0]+1\n",
    "# lpa_unique_key.insert(loc = 0, column='ukey', value = lpa_unique_key.set_index(['donor_postcode', 'donor_gender', 'age']).index.factorize()[0]+1)\n",
    "# #lpa_unique_key\n",
    "\n",
    "# #(lpa_unique_key.fillna({'donor_postcode':'', 'donor_gender':'', 'age':''})\n",
    "# #   .groupby(['donor_postcode', 'donor_gender', 'age'],sort=False).ngroup()+1)\n",
    "\n",
    "# #lpa_unique_key.loc[lpa_unique_key['type']=='lpa','ukey'].agg(['nunique','count','size'])\n",
    "# #lpa_unique_key.query('type == \"lpa\"')['ukey'].agg(['nunique','count','size'])\n",
    "# #lpa_unique_key.query('casesubtype == \"hw\"')['ukey'].agg(['nunique','count','size'])\n",
    "# #lpa_unique_key.query('casesubtype == \"pfa\"')['ukey'].agg(['nunique','count','size'])\n",
    "# #lpa_unique_key.groupby(['ukey']).count()\n",
    "# #lpa_unique_key['count_ukey'] = lpa_unique_key['ukey'].value_counts()\n",
    "# #lpa_unique_key\n",
    "\n",
    "\n",
    "\n",
    "# lpa_unique_key['CountbyUkey'] = lpa_unique_key.groupby(['donor_postcode', 'donor_gender']).age.transform('count')\n",
    "# lpa_unique_key['CountbyAge'] = lpa_unique_key.groupby('year').age.transform('count').sum()\n",
    "\n",
    "# # Perform the GROUP BY operation and calculate the sum\n",
    "# lpa_age = lpa_unique_key.groupby(['donor_postcode', 'donor_gender', 'age']) \\\n",
    "#     .agg({'CountbyAge': 'sum'}) \\\n",
    "#     .reset_index()\n",
    "\n",
    "# print(lpa_age)\n",
    "# #lpa_unique_key['month'] = lpa_unique_key['ArrivalDate'].dt.month\n",
    "\n",
    "\n",
    "# # Cases_by_year_age\n",
    "\n",
    "# #lpa_by_year_age = lpa_unique_key[['receiptdate', 'uid', 'type', 'casesubtype', 'status', 'donor_postcode', 'donor_gender', 'age']] \\\n",
    "# #                    .groupby(['donor_postcode', 'donor_gender', 'age'])  \\\n",
    "# #                    .agg('count')#.sum()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "607f2311-c45a-4016-ac0e-f2ca2a88c588",
   "metadata": {},
   "source": [
    "# Generate a Unique key by combining age, donor_gender, and donor_postcode\n",
    "\n",
    "For ages over 19 years old:\n",
    "Unique case reference for each donor = [donor_dob + donor_postcode + donor_gender]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d42c4dde-0614-45ab-91a1-254117ca8f41",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# DataFrame with the count of unique records for each combination of age and year. \n",
    "# Generate a unique key by combining age, donor_gender, and donor_postcode, \n",
    "# and then calculate the number of unique records by age and year.\n",
    "\n",
    "lpa_unique = lpa_data_sample_imputed\n",
    "\n",
    "# Remove spaces from the donor postcodes\n",
    "lpa_unique['donor_postcode'] = lpa_unique['donor_postcode'].str.strip()\n",
    "lpa_unique['donor_postcode'] = lpa_unique['donor_postcode'].str.replace(' ', '')\n",
    "\n",
    "# Generate a unique key by combining age, donor_gender, and donor_postcode\n",
    "lpa_unique['unique_key'] = lpa_unique['donor_dob'].astype(str) \\\n",
    "+ lpa_unique['donor_gender'] + lpa_unique['donor_postcode']\n",
    "\n",
    "# lpa_by_year_age = lpa_unique_key\n",
    "\n",
    "# lpa_by_year_age.to_csv(r'lpa_by_year_age.csv')\n",
    "\n",
    "# remove duplicate rows based on Id values(unique_key) and \n",
    "# keep only the row that don't have 0 value in all the fields.\n",
    "\n",
    "\n",
    "duplicateMask = lpa_unique.duplicated('unique_key', keep=False)\n",
    "\n",
    "lpa_unique = pd.concat([lpa_unique.loc[duplicateMask & lpa_unique[['age', 'donor_gender', 'donor_postcode']].ne(0).any(axis=1)], \\\n",
    "               lpa_unique[~duplicateMask]])\n",
    "\n",
    "#lpa_df['zero']=lpa_df.select_dtypes(['int','float']).eq(0).sum(axis=1)\n",
    "#df=df.sort_values(['zero','Id']).drop_duplicates(subset=['Id']).drop(columns='zero')df['zero']=df.select_dtypes(['int','float']).eq(0).sum(axis=1)\n",
    "#df=df.sort_values(['zero','Id']).drop_duplicates(subset=['Id']).drop(columns='zero')\n",
    "\n",
    "#lpa_unique = lpa_unique.drop_duplicates(subset=\"unique_key\")\n",
    "lpa_unique"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c2235a2-7a56-4d54-b10d-e0b17c6aa8ff",
   "metadata": {},
   "source": [
    "# Save the LPA data with new unique keys (as a unique ID)\n",
    "\n",
    "Sort by the unique id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51f4fb88-82cb-4844-bcae-72657302a02e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sort the rows of dataframe by  'unique_key'  \n",
    "## column inplace\n",
    "\n",
    "#lpa_df_index = lpa_unique.sort_values(lpa_unique.columns[9])\n",
    "\n",
    "# Extract month letter and year \n",
    "lpa_unique['month_year'] = lpa_unique['receiptdate'].dt.strftime('%b-%y')\n",
    "\n",
    "## Sort by 'unique_key' column in ascending order\n",
    "lpa_df_index = lpa_unique.sort_values(by=['unique_key','receiptdate'])\n",
    "\n",
    "\n",
    "#lpa_df_index = lpa_unique.sort_values['unique_key']\n",
    "#lpa_df_index = lpa_unique.sort_values(by = 'unique_key', axis = 1, inplace = True, ascending = True)\n",
    "#lpa_df_index = lpa_unique.reindex(sorted(lpa_unique.columns), axis=1)\n",
    "\n",
    "## Set index\n",
    "#df['receiptdate'] = pd.to_datetime(df['receiptdate'])\n",
    "\n",
    "#df = df.set_index('receiptdate').asfreq('D')\n",
    "#lpa_df_index['unique_key'] = \n",
    "\n",
    "## Set the unique key as an ID (index)\n",
    "lpa_df_index.set_index('unique_key', inplace = True)\n",
    "\n",
    "#df.index = df.index.to_period('D')\n",
    "                            \n",
    "###print(df.head())\n",
    "###print(df.tail())\n",
    "\n",
    "#Missing_data = lpa_df_index[(lpa_data_sample_imputed['age'] < 0 | lpa_data_sample_imputed['age'] > 126)]\n",
    "#print(Missing_data)\n",
    "\n",
    "\n",
    "\n",
    "# Extract and save data into a csv file\n",
    "lpa_data = lpa_df_index\n",
    "\n",
    "##for year in range(min(pd.to_datetime(count_unique_receipts_dailydf['receiptdate'], errors = 'coerce').dt.year), \n",
    "#                  max(pd.to_datetime(count_unique_receipts_daily['receiptdate'], errors = 'coerce').dt.year)):\n",
    "#lpa_data.to_csv(r'lpa_data.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a38d5c6-bff5-4f78-9c6a-f5c4dec2a30b",
   "metadata": {},
   "source": [
    "# count the average daily demand and multiply it by the number of working days in each year \n",
    "- Daily Demand Calculation: We group the data by receiptdate to calculate the demand count per day.\n",
    "- Average and Standard Deviation: We compute the average and standard deviation of these daily demands.\n",
    "- Working Days Calculation: As before, we calculate the number of working days in each year.\n",
    "- Yearly Calculations: Multiply the average daily demand and standard deviation by the number of working days to get the yearly values.\n",
    "- Store Results: Store these values in a dictionary, convert it to a DataFrame, and save it to an Excel file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a489de07-bc9b-49c0-b8ba-6ead3258381a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a temp data frame (df) to count the average daily demand: \n",
    "df = lpa_data_sample_imputed\n",
    "\n",
    "# Convert receiptdate to datetime\n",
    "df['receiptdate'] = pd.to_datetime(df['receiptdate'])\n",
    "\n",
    "# Helper function to count working days\n",
    "def count_working_days(year):\n",
    "    start_date = f'{year}-01-01'\n",
    "    end_date = f'{year}-12-31'\n",
    "    #    start_date = f'{pd.to_datetime(snapshot_start).year}-{pd.to_datetime(snapshot_start).month}-{pd.to_datetime(snapshot_start).day}'\n",
    "    #    end_date = f'{pd.to_datetime(snapshot_end).year}-{pd.to_datetime(snapshot_end).month}-{pd.to_datetime(snapshot_end).day}'\n",
    "    date_range = pd.date_range(start_date, end_date, freq='B')\n",
    "    return len(date_range)\n",
    "\n",
    "# Initialize results dictionary\n",
    "results = {\n",
    "    'year': [],\n",
    "    'Average Daily Demand': [],\n",
    "    'Yearly Demand': [],\n",
    "    'Standard Deviation Daily Demand': [],\n",
    "    'Yearly Standard Deviation': []\n",
    "}\n",
    "\n",
    "# Process each year separately\n",
    "for year in df['year'].unique():\n",
    "    # Filter data for the specific year\n",
    "    df_year = df[df['year'] == year]\n",
    "        \n",
    "    # Group by date and count demands per day\n",
    "    daily_demand = df_year.groupby('receiptdate').size()\n",
    "    \n",
    "    # Calculate the average daily demand for the year\n",
    "    average_daily_demand = daily_demand.mean()\n",
    "    \n",
    "    # Calculate the total number of demand entries for the year\n",
    "    #total_demand = df_year['receiptdate'].count()\n",
    "    # Calculate the number of days in the year\n",
    "    #num_days = (df_year['receiptdate'].max() - df_year['receiptdate'].min()).days + 1\n",
    "    # Calculate the average daily demand for the year\n",
    "    #average_daily_demand = total_demand / num_days\n",
    "    \n",
    "    # Calculate the standard deviation of daily demands for the year\n",
    "    std_dev_daily_demand = daily_demand.std()\n",
    "    \n",
    "    # Calculate the number of working days in the year\n",
    "    working_days = count_working_days(year)\n",
    "    \n",
    "    # Calculate the yearly demand based on working days\n",
    "    yearly_demand = average_daily_demand * working_days\n",
    "    \n",
    "    # Calculate the yearly standard deviation based on working days\n",
    "    yearly_std_dev = std_dev_daily_demand * working_days\n",
    "    \n",
    "    # Store the results in the dictionary\n",
    "    results['year'].append(year)\n",
    "    results['Average Daily Demand'].append(average_daily_demand)\n",
    "    results['Yearly Demand'].append(yearly_demand)\n",
    "    results['Standard Deviation Daily Demand'].append(std_dev_daily_demand)\n",
    "    results['Yearly Standard Deviation'].append(yearly_std_dev)\n",
    "\n",
    "# Convert results dictionary to DataFrame\n",
    "annual_reciepts = pd.DataFrame(results)\n",
    "\n",
    "# Sort based on year\n",
    "annual_reciepts = annual_reciepts.sort_values(by=['year'])\n",
    "\n",
    "# Save to Excel file with specified sheet name\n",
    "annual_reciepts.to_excel('Annual_Demands.xlsx', sheet_name='Annual Demands', index=False)\n",
    "\n",
    "# Save to CSV file\n",
    "annual_reciepts.to_csv('Annual_Demands.csv', index=False)\n",
    "\n",
    "print(\"Results:\")\n",
    "print(annual_reciepts)\n",
    "\n",
    "# The results are saved in an Excel file named Annual_Demands.xlsx with a sheet named \"Annual Demands\". \n",
    "#The file will contain columns for each year, the average daily demand, \n",
    "#the yearly demand, the standard deviation of daily demands, and the yearly standard deviation."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bce6b69d-876f-463c-ad33-14482c777f1a",
   "metadata": {},
   "source": [
    "# Number of LPA reciepts\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90dd8f3a-f1d4-4753-b55a-15cb1620ffcf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Average daily receipts for 2023\n",
    "\n",
    "# create a drop down list with average daily receipts of LPA application in 2024 in the range say from 4000 – 7000 in increments of 100. \n",
    "# Then this should be used as an estimate to apply unceratinty and to be converted into an age-specific annual donor forecast.\n",
    "\n",
    "# Filter data to involve Registered status and Post-covid data from 2022 onwards\n",
    "unique_receipts_post_covid = lpa_data_sample_imputed\n",
    "\n",
    "###unique_receipts_post_covid[unique_receipts_post_covid['status'].str.contains(\"Registered\")]\n",
    "\n",
    "\n",
    "#unique_receipts_post_covid = unique_receipts_post_covid.query('year > 2021')\n",
    "\n",
    "\n",
    "#df[df['Overall_Percentage'].isin([value for value in df['Overall_Percentage'] if value > 60])]\n",
    "#df[df.apply(lambda row: row['Overall_Percentage'] > 55, axis=1)]\n",
    "# # The “loc” method is used to access a group of rows and columns by label(s) or a boolean array. \n",
    "# #We can utilise it to filter a DataFrame based on specific column values.\n",
    "#df.loc[df['Overall_Percentage'] > 40]\n",
    "# # The “iloc” method is similar to “loc” but uses integer-based indexing instead of labels. \n",
    "# #It allows us to filter a DataFrame by specifying the row and column indices.\n",
    "#df[df.iloc[:, -1] > 40]\n",
    "\n",
    "\n",
    "#count_unique_receipts_monthly = unique_receipts_post_covid.groupby(['month_year'])['receiptdate'].agg('count').reset_index()\n",
    "\n",
    "unique_receipts_post_covid = unique_receipts_post_covid.groupby(['receiptdate', 'age'])['uid'].agg('count').reset_index()\n",
    "\n",
    "#count_unique_receipts_monthly = unique_receipts_post_covid.groupby(['year', 'month_year'])['uid'].agg('count').reset_index()\n",
    "\n",
    "#count_unique_receipts_monthly = unique_receipts_post_covid.groupby(['month_year'])['receiptdate'].nunique().reset_index(name='count')\n",
    "\n",
    "# Calculating the overall percentage for each donor and adding a new column\n",
    "#count_unique_receipts_daily['Overall_Percentage'] = count_unique_receipts_daily.iloc[:, 1:].mean(axis=1)\n",
    "\n",
    "\n",
    "\n",
    "count_unique_receipts_daily = unique_receipts_post_covid.rename(columns={'uid': 'daily_demand'})\n",
    "\n",
    "count_unique_receipts_daily['avg_daily_demand'] = count_unique_receipts_daily['daily_demand'].mean()\n",
    "\n",
    "\n",
    "# Extract month letter and year \n",
    "###lpa_reciepts['month_year'] = lpa_reciepts['receiptdate'].dt.strftime('%b-%y')\n",
    "# Calculate the number of unique records by month and year\n",
    "#count_reciepts_month = lpa_reciepts.groupby(['year', 'month_year'])['receiptdate'].nunique().reset_index(name='count')\n",
    "###Count_daily_reciepts = lpa_reciepts.groupby(['receiptdate']).count()\n",
    "\n",
    "#unique_records = df.groupby('unique_key').agg('count').reset_index()  #.groupby(['year'])['unique_key'].nunique().reset_index(name='count')\n",
    "###Count_daily_reciepts = Count_daily_reciepts.rename(columns={\"count\": \"Count_of_daily_reciepts\"})\n",
    "\n",
    "\n",
    "# Extract month letter and year \n",
    "count_unique_receipts_daily['month_year'] = count_unique_receipts_daily['receiptdate'].dt.strftime('%b-%y')\n",
    "count_unique_receipts_daily['year'] = count_unique_receipts_daily['receiptdate'].dt.strftime('%Y')\n",
    "print(count_unique_receipts_daily)\n",
    "\n",
    "#count_unique_receipts_daily.groupby(['receiptdate', 'year', 'month_year', 'age'])['daily_demand'].agg('count').reset_index()\n",
    "#count_reciepts_month = count_unique_receipts_daily.groupby(['year', 'month_year'])['receiptdate'].agg('sum').reset_index()\n",
    "#annual_ = historical_data.groupby(['year'])['daily_demand'].agg('sum').reset_index()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "822cd2e9-b5d0-4e68-877b-e87bbc44863a",
   "metadata": {},
   "source": [
    "•\tI removed the records of receipts registered in the weekends / bank holidays and updated the LPA model a little bit more (Please find the new model in \"Y:\\Forecasting Model\\Long_Term_Model_2024\\Demand Forecasts July 2024 v1.xlsm\").\n",
    "\n",
    "•\tI have updated the python code in to pre-process the data to calculate the number of registered receipts in the short-term demand data for 2021 (during COVID).\n",
    "\n",
    "•\tThen Calculated Average, Standard Deviation of LPA daily demands during the pandemic as well as Lower / upper bounds of Confidence Interval for the receipts.\n",
    "\n",
    "•\tTo calculate the Covid impact Ratio of Short-Term to Long-Term Demand, I have computed the ratio of the average short-term demand to the average long-term demand. This ratio represents the relative impact of the short-term period on the long-term forecast: covid_impact_ratio = covid_mean_demand / post_covid_mean_demand.\n",
    "\n",
    "•\tI used ARIMA (Autoregressive Integrated Moving Average) and Naïve extrapolation for the short term and long term forecasting.\n",
    "\n",
    "•\tI would like to apply more advanced models in the future to improve this, e.g., use models like SARIMA , exponential smoothing, or machine learning models (LSTM and Bayesian methods) to forecast short-term demand. These models can incorporate recent trends and seasonality. Incorporate COVID-19 Variables: Include variables that capture the impact of COVID-19, such as lockdown periods, infection rates, and economic indicators.\n",
    "\n",
    "•\tFor Long-Term Forecasting of LPA demands, I am using a Baseline Forecast to create a baseline long-term forecast using historical data and trends.\n",
    "\n",
    "•\tThen To adjust the long term forecasts for COVID-19 Impact, I am going to adjust the baseline forecast by incorporating the uncertainty from the short-term forecasts. This can be done by applying a proportionate adjustment based on the observed impact during the pandemic.\n",
    "\n",
    "•\tI have used age groups = ['18-20', '21-25', '26-30', '31-35', '36-40','41-45','46-50', '51-55', '56-60', '61-65', '66-70', '71+'] in my python code to analysis the age-specific impacts (ongoing).\n",
    "\n",
    "•\tIn the future, I would like to exploit some age-specific analysis such as: Demographic Trends: Consider demographic trends and how they might influence future LPA registrations, and Segmented Forecasting: Perform separate forecasts for different age groups to account for varying behaviours and impacts.\n",
    "\n",
    "•\tI have added a drop-down list in the excel model under the ‘LPA Control Assumption’ Tab for average daily receipts of LPA applications in 2024.\n",
    "\n",
    "•\tFor UPDATED FORECAST: I am planning next week to apply the uncertainty: Adjusted Long-Term Forecast = Future Long-Term Forecast × covid_impact_ratio.\n",
    "\n",
    "•\tAlso I would work to automate the process of adjustment by changing the impact and different points during the time, we can continue to track this and update quarterly and also to vary the receipts inputs to reflect uncertainty around this estimate which will then also be reflected in the longer term forecast.\n",
    "\n",
    "•\tTo quantifying Uncertainty, I am trying to use the Scenario Analysis and develop different scenarios (e.g., optimistic, pessimistic, and most likely) to understand the range of possible outcomes. Also I would like to utilise this further by calculating confidence intervals for your forecasts to quantify the uncertainty.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e874dde9-cbc9-4edc-a983-89868d2d3eef",
   "metadata": {},
   "source": [
    "# To measure and calculate the impact of COVID-19 on short-term demand forecasting and apply this uncertainty to long-term forecasting, you can follow these steps:\n",
    "\n",
    "1. Data Collection and Analysis\n",
    "Historical Data: Gather historical data on the number of Living Power of Attorney (LPA) registrations across different age groups.\n",
    "COVID-19 Impact Data: Collect data on how COVID-19 affected LPA registrations. This could include changes in registration rates during the pandemic.\n",
    "2. Short-Term Forecasting\n",
    "Model Selection: Use models like ARIMA (AutoRegressive Integrated Moving Average), exponential smoothing, or machine learning models to forecast short-term demand. These models can incorporate recent trends and seasonality.\n",
    "Incorporate COVID-19 Variables: Include variables that capture the impact of COVID-19, such as lockdown periods, infection rates, and economic indicators1.\n",
    "3. Quantifying Uncertainty\n",
    "Scenario Analysis: Develop different scenarios (e.g., optimistic, pessimistic, and most likely) to understand the range of possible outcomes.\n",
    "Confidence Intervals: Calculate confidence intervals for your forecasts to quantify the uncertainty.\n",
    "4. Long-Term Forecasting\n",
    "Baseline Forecast: Create a baseline long-term forecast using historical data and trends.\n",
    "Adjust for COVID-19 Impact: Adjust the baseline forecast by incorporating the uncertainty from the short-term forecasts. This can be done by applying a proportionate adjustment based on the observed impact during the pandemic.\n",
    "5. Age Group Analysis\n",
    "Segmented Forecasting: Perform separate forecasts for different age groups to account for varying behaviors and impacts.\n",
    "Demographic Trends: Consider demographic trends and how they might influence future LPA registrations.\n",
    "6. Validation and Adjustment\n",
    "Model Validation: Validate your models using out-of-sample data to ensure accuracy.\n",
    "Continuous Monitoring: Continuously monitor actual registrations against forecasts and adjust models as needed.\n",
    "\n",
    "For instance, if you observed a 20% increase in LPA registrations during the pandemic for a specific age group, you might apply a similar adjustment to your long-term forecast for that age group, while considering other factors like economic recovery and changes in public awareness."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1cf70f5a-4464-412d-b990-aacf20c52df1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# long_term_forecast_date\n",
    "start_date_covid = date(2022, 1, 1) # \"2021-01-01\"\n",
    "start_date_covid = start_date_covid.strftime('%Y-%m-%d')\n",
    "start_date_covid\n",
    "#start_date_post_covid = date(2022, 1, 1) # \"2022-01-01\"\n",
    "#start_date_post_covid = start_date.strftime('%Y-%m-%d')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ef7fbea-0b90-44ff-86cc-aa17f6b83457",
   "metadata": {},
   "outputs": [],
   "source": [
    "# LPA historical data\n",
    "\n",
    "historical_data = \n",
    "\n",
    "historical_data = historical_data.rename(columns={'receiptdate': 'date', 'uid': 'demand'})\n",
    "\n",
    "historical_data['date'] = pd.to_datetime(historical_data['date'], errors = 'coerce') #.dt.date\n",
    "historical_data['date'] = historical_data['date'].dt.strftime('%Y-%m-%d')\n",
    "\n",
    "\n",
    "\n",
    "historical_data = historical_data.sort_values(by=['date', 'age'])\n",
    "# Covid data\n",
    "covid_data = historical_data #.query('date < {start_date_covid}')\n",
    "covid_data = covid_data[covid_data.apply(lambda row: row['date'] < start_date_covid, axis=1)]\n",
    "print(covid_data.head(3))\n",
    "print(covid_data.tail(3))\n",
    "\n",
    "# Post Covid data\n",
    "post_covid_data = historical_data #.query('date < {start_date_covid}')\n",
    "post_covid_data = post_covid_data[post_covid_data.apply(lambda row: row['date'] >= start_date_covid, axis=1)]\n",
    "print(post_covid_data.head(3))\n",
    "print(post_covid_data.tail(3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "417a1a77-07a5-4818-80fc-b645bdbdaa62",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "821c605a-088e-4549-80e0-1cca7b7407d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate the impact of Covid\n",
    "## short-term demand data for 2021 (during COVID)\n",
    "daily_covid_demand = covid_data.groupby(['date'])['demand'].agg('count').reset_index()\n",
    "covid_daily_demands = daily_covid_demand['demand']\n",
    "\n",
    "# Mean and standard deviation\n",
    "covid_mean_demand = covid_daily_demands.mean()\n",
    "covid_std_dev_demand = covid_daily_demands.std()\n",
    "\n",
    "print(f\"Average of LPA daily demands during the pandemic: {covid_mean_demand}\")\n",
    "print(f\"Standard Deviation of LPA daily demands during the pandemic: {covid_std_dev_demand}\")\n",
    "\n",
    "# 95% Confidence Interval\n",
    "covid_ci_lower = covid_mean_demand - 1.96 * covid_std_dev_demand\n",
    "covid_ci_upper = covid_mean_demand + 1.96 * covid_std_dev_demand\n",
    "\n",
    "print(f\"Covid Mean Demand: {covid_mean_demand}\")\n",
    "print(f\"95% Confidence Interval during the pandemic: [{covid_ci_lower}, {covid_ci_upper}]\")\n",
    "\n",
    "##Coefficient of Variation is the standardized standard deviation. We calculate the standard deviation, but then scale it with the mean of the series to guard against scale dependency.\n",
    "##This shows the variability of a time series. If COV^2 is high, that means that the variability in the series is also high.\n",
    "covid_cov = covid_std_dev_demand / covid_mean_demand\n",
    "covid_cov\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac25aea3-061f-414a-8768-120113317359",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8df3eb3-5ae8-4377-aa2c-01212ae45afa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate the impact of Post Covid\n",
    "\n",
    "daily_post_covid_demand = post_covid_data.groupby(['date'])['demand'].agg('count').reset_index()\n",
    "post_covid_daily_demands = daily_post_covid_demand['demand']\n",
    "\n",
    "# Mean and standard deviation\n",
    "post_covid_mean_demand = post_covid_daily_demands.mean()\n",
    "post_covid_std_dev_demand = post_covid_daily_demands.std()\n",
    "\n",
    "print(f\"Average of LPA daily demands after the pandemic: {post_covid_mean_demand}\")\n",
    "print(f\"Standard Deviation of LPA daily demands after the pandemic: {post_covid_std_dev_demand}\")\n",
    "\n",
    "# 95% Confidence Interval\n",
    "post_covid_ci_lower = post_covid_mean_demand - 1.96 * post_covid_std_dev_demand\n",
    "post_covid_ci_upper = post_covid_mean_demand + 1.96 * post_covid_std_dev_demand\n",
    "\n",
    "print(f\"Post Covid Mean Demand: {covid_mean_demand}\")\n",
    "print(f\"95% Confidence Interval after the pandemic: [{post_covid_ci_lower}, {post_covid_ci_upper}]\")\n",
    "\n",
    "##Coefficient of Variation is the standardized standard deviation. We calculate the standard deviation, but then scale it with the mean of the series to guard against scale dependency.\n",
    "##This shows the variability of a time series. If COV^2 is high, that means that the variability in the series is also high.\n",
    "post_covid_cov = post_covid_std_dev_demand / post_covid_mean_demand\n",
    "post_covid_cov\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16045c43-040f-4dd2-993c-3474f08581f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Calculate the Ratio of Short-Term to Long-Term Demand:\n",
    "#Compute the ratio of the average short-term demand to the average long-term demand. This ratio represents the relative impact of the short-term period on the long-term forecast.\n",
    "covid_impact_ratio = covid_mean_demand / post_covid_mean_demand\n",
    "\n",
    "# Apply the Uncertainty to Future Long-Term Forecasts:\n",
    "#Multiply the future long-term forecasts by the estimated uncertainty to incorporate the impact of the short-term period.\n",
    "\n",
    "\n",
    "# Estimate the Uncertainty:\n",
    "print(f\"The ratio of {covid_impact_ratio} indicates that the short-term demand during COVID was 20% higher than the long-term average.\")\n",
    "\n",
    "# Apply the Uncertainty:\n",
    "Adjusted Long-Term Forecast = Future Long-Term Forecast × covid_impact_ratio"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a474a94-817b-4541-94ba-e51a6d50620e",
   "metadata": {},
   "source": [
    "To calculate a ratio and proportion to assess the impact of short-term forecasting on long-term forecasting, you can follow these steps:\n",
    "\n",
    "Calculate the Average Short-Term Demand:\n",
    "Determine the average demand during the short-term period affected by COVID-19.\n",
    "\n",
    "Calculate the Average Long-Term Demand:\n",
    "Determine the average demand over a longer period, which could include pre-COVID, during COVID, and post-COVID periods.\n",
    "\n",
    "Calculate the Ratio of Short-Term to Long-Term Demand:\n",
    "Compute the ratio of the average short-term demand to the average long-term demand. This ratio represents the relative impact of the short-term period on the long-term forecast.\n",
    "\n",
    "Estimate the Uncertainty:\n",
    "Use the ratio calculated in step 3 to estimate the uncertainty in future long-term forecasts.\n",
    "\n",
    "Apply the Uncertainty to Future Long-Term Forecasts:\n",
    "Multiply the future long-term forecasts by the estimated uncertainty to incorporate the impact of the short-term period.\n",
    "\n",
    "Example Calculation\n",
    "Average Short-Term Demand:\n",
    "Let's assume we have short-term demand data for 2021 (during COVID) and the average daily demand during this period is 6000.\n",
    "\n",
    "Average Long-Term Demand:\n",
    "Assume the average daily demand over the long-term period (e.g., 2019-2023) is 5000.\n",
    "\n",
    "Calculate the Ratio:\n",
    "\n",
    "Ratio = Average Short-Term Demand / Average Long-Term Demand\n",
    "\n",
    "Estimate the Uncertainty:\n",
    "The ratio of 1.2 indicates that the short-term demand during COVID was 20% higher than the long-term average.\n",
    "\n",
    "Apply the Uncertainty:\n",
    "If your future long-term forecast is 5500, apply the ratio to adjust for the estimated uncertainty:\n",
    "\n",
    "Adjusted Long-Term Forecast = Future Long-Term Forecast × Ratio \n",
    "Adjusted Long-Term Forecast=Future Long-Term Forecast×Ratio=5500×1.2=6600\n",
    "Excel Implementation\n",
    "You can implement this in Excel with the following steps:\n",
    "\n",
    "Data Preparation:\n",
    "\n",
    "List your short-term and long-term demand data in two separate columns.\n",
    "Calculate the averages of these columns.\n",
    "Calculate the Ratio:\n",
    "\n",
    "Use the formula =AVERAGE(ShortTermData) / AVERAGE(LongTermData) to get the ratio.\n",
    "Adjust Future Long-Term Forecast:\n",
    "\n",
    "Multiply your future long-term forecast by the calculated ratio to incorporate the uncertainty.\n",
    "Excel Example\n",
    "Data:\n",
    "\n",
    "Column A: Short-term demand data\n",
    "Column B: Long-term demand data\n",
    "Cell D1: Future long-term forecast\n",
    "Formulas:\n",
    "\n",
    "Cell C1: =AVERAGE(A1:A30) (Average short-term demand)\n",
    "Cell C2: =AVERAGE(B1:B30) (Average long-term demand)\n",
    "Cell C3: =C1 / C2 (Ratio)\n",
    "Cell D2: =D1 * C3 (Adjusted future long-term forecast)\n",
    "Sample Excel Sheet\n",
    "kotlin\n",
    "Copy code\n",
    "| Short-Term Data | Long-Term Data | Calculation           | Forecast       |\n",
    "|-----------------|----------------|-----------------------|----------------|\n",
    "| 6000            | 5000           | Average Short-Term    | 5500           |\n",
    "| ...             | ...            | Average Long-Term     |                |\n",
    "| ...             | ...            | Ratio                 |                |\n",
    "| ...             | ...            | Adjusted Forecast     | =5500*Ratio    |\n",
    "In this example, replace the ellipses (...) with your actual data. This setup will allow you to calculate the impact of short-term forecasting on long-term forecasts and adjust for uncertainty in Excel."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e412b03-0200-445c-82f4-d360cbf5759a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from statsmodels.tsa.arima.model import ARIMA\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "class LPAForecast:\n",
    "    def __init__(self, post_covid_data, covid_data, historical_data): #, economic_data):\n",
    "        self.post_covid_data = post_covid_data\n",
    "        self.covid_data = covid_data\n",
    "        #self.economic_data = economic_data\n",
    "        self.historical_data = historical_data\n",
    "        \n",
    "    def preprocess_data(self):\n",
    "        # Merge historical data with COVID-19 impact data and economic data\n",
    "        self.data = historical_data #pd.merge(self.post_covid_data, self.covid_data, on='date', how='left')\n",
    "        #self.data = pd.merge(self.data, self.economic_data, on='date', how='left')\n",
    "        self.data.fillna(0, inplace=True)\n",
    "\n",
    "    def short_term_forecast(self, order=(5,1,0)):\n",
    "        # Split data into training and testing sets\n",
    "        train, test = train_test_split(self.data, test_size=0.2, shuffle=False)\n",
    "        \n",
    "        # Fit ARIMA model\n",
    "        model = ARIMA(train['demand'], order=order, exog=train[['covid_impact']]) #, 'economic_indicator']])\n",
    "        self.model_fit = model.fit()\n",
    "        \n",
    "        # Forecast\n",
    "        self.short_term_predictions = self.model_fit.forecast(steps=len(test), exog=test[['covid_impact']]) #, 'economic_indicator']])\n",
    "        #baseline_forecast = self.model_fit.forecast(steps=24)  # Assuming 2 years monthly forecast\n",
    "        self.test = test\n",
    "\n",
    "    def quantify_uncertainty(self):\n",
    "        # Calculate confidence intervals\n",
    "        self.conf_int = self.model_fit.get_forecast(steps=len(self.test), exog=self.test[['covid_impact']]).conf_int() #, 'economic_indicator']]).conf_int()\n",
    "        #self.conf_int = self.model_fit.get_forecast(steps=len(self.test)).conf_int()\n",
    "        \n",
    "    def long_term_forecast(self, adjustment_factor=1.2):\n",
    "        # Baseline forecast using historical trend\n",
    "        #future_economic_data = self.economic_data.tail(24)  # Assuming 2 years monthly forecast\n",
    "        baseline_forecast = self.model_fit.forecast(steps=24, exog=future_economic_data[['covid_impact']]) #, 'economic_indicator']])\n",
    "        #baseline_forecast = self.model_fit.forecast(steps=24)  # Assuming 2 years monthly forecast\n",
    "        \n",
    "        # Adjust for COVID-19 impact\n",
    "        self.long_term_predictions = baseline_forecast * adjustment_factor\n",
    "\n",
    "        \n",
    "# Write a function to apply exponential smoothing applied to the age-specific short-term forecasts to incorporate covid impact and do sentivitity tests \n",
    "# follow the excel formula in LPA model and control assumption:\n",
    "# =GB9*'LPA Control Assumptions'!$B$123+(1-'LPA Control Assumptions'!$B$123)*(GB9-GA9+GB9)\n",
    "    #def age_specific_uncertainty(self)\n",
    "    \n",
    "    \n",
    "    def forecast_by_age_group(self, age_groups):\n",
    "        self.age_group_forecasts = {}\n",
    "        for age_group in age_groups:\n",
    "            # Filter data for the age group\n",
    "            age_group_data = self.data[self.data['age_group'] == age_group]\n",
    "            \n",
    "            # Fit ARIMA model\n",
    "            model = ARIMA(age_group_data['demand'], order=(5,1,0), exog=age_group_data[['covid_impact']]) #, 'economic_indicator']])\n",
    "            model_fit = model.fit()\n",
    "            \n",
    "            # Forecast\n",
    "            forecast = model_fit.forecast(steps=24, exog=future_economic_data[['covid_impact']]) #, 'economic_indicator']])\n",
    "            self.age_group_forecasts[age_group] = forecast\n",
    "#         for age_group in age_groups:\n",
    "#             # Filter data for the age group\n",
    "#             age_group_data = self.data[self.data['age_group'] == age_group]\n",
    "#             # Fit ARIMA model\n",
    "#             model = ARIMA(age_group_data['registrations'], order=(5,1,0))\n",
    "#             model_fit = model.fit()\n",
    "#             # Forecast\n",
    "#             forecast = model_fit.forecast(steps=24)\n",
    "#             self.age_group_forecasts[age_group] = forecast\n",
    "    def validate_model(self):\n",
    "        # Compare predictions with actual values\n",
    "        self.validation_results = self.test['demand'] - self.short_term_predictions\n",
    "\n",
    "    def aggregate_by_age_group(self, bins, labels):\n",
    "        # Create a new column 'AgeGroup' with the age bins\n",
    "        self.data['AgeGroup'] = pd.cut(self.data['age'], bins=bins, labels=labels, right=False)\n",
    "        \n",
    "        # Aggregate data within each age group\n",
    "        aggregated_data = self.data.groupby('AgeGroup').agg({\n",
    "            'demand': 'sum',\n",
    "            'name': 'count'\n",
    "        }).rename(columns={'name': 'count'})\n",
    "        \n",
    "        return aggregated_data\n",
    "\n",
    "    def run_forecast(self, age_groups, bins, labels):\n",
    "        self.preprocess_data()\n",
    "        self.short_term_forecast()\n",
    "        self.quantify_uncertainty()\n",
    "        self.long_term_forecast()\n",
    "        self.forecast_by_age_group(age_groups)\n",
    "        self.validate_model()\n",
    "        return self.aggregate_by_age_group(bins, labels)\n",
    "\n",
    "# usage\n",
    "#historical_data = pd.read_csv('historical_data.csv')  # Replace with actual data source\n",
    "#covid_data = pd.read_csv('covid_data.csv')  # Replace with actual data source\n",
    "#economic_data = pd.read_csv('economic_data.csv')  # Replace with actual data source\n",
    "age_groups = ['18-20', '21-25', '26-30', '31-35', '36-40','41-45','46-50', '51-55', '56-60', '61-65', '66-70', '71+']\n",
    "bins = [18, 20, 30, 40, 50, 60, 70, 100]\n",
    "labels = ['18-20', '21-25', '26-30', '31-35', '36-40','41-45','46-50', '51-55', '56-60', '61-65', '66-70', '71+']\n",
    "\n",
    "forecast = LPAForecast(post_covid_data, covid_data, historical_data)# , economic_data)\n",
    "aggregated_data = forecast.run_forecast(age_groups, bins, labels)\n",
    "\n",
    "print(aggregated_data)\n",
    "\n",
    "#age_groups = ['18-30', '31-50', '51-70', '71+']\n",
    "\n",
    "#forecast = LPAForecast(post_covid_data, covid_data)\n",
    "#forecast.run_forecast(age_groups)\n",
    "#print(forecast.long_term_predictions)\n",
    "#print(forecast.age_group_forecasts)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "214f4ec0-d301-4bec-8d24-d5bd142c93ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "daily_demand = count_unique_receipts_daily['daily_demand']\n",
    "\n",
    "# Mean and standard deviation\n",
    "mean_demand = daily_demand.mean()\n",
    "std_dev_demand = daily_demand.std()\n",
    "\n",
    "print(f\"Average of LPA daily demand: {mean_demand}\")\n",
    "print(f\"Standard Deviation of LPA daily demand: {std_dev_demand}\")\n",
    "\n",
    "# 95% Confidence Interval\n",
    "ci_lower = mean_demand - 1.96 * std_dev_demand\n",
    "ci_upper = mean_demand + 1.96 * std_dev_demand\n",
    "\n",
    "print(f\"Mean Demand: {mean_demand}\")\n",
    "print(f\"95% Confidence Interval: [{ci_lower}, {ci_upper}]\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7d25525-363b-4da3-b46d-f5d44608dbf2",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ac93bdc-5bdb-4a67-9d5a-e6ed41d5bbfc",
   "metadata": {},
   "source": [
    "# Forecast Error Measures: Intermittent Demand\n",
    "\n",
    "*ref: https://deep-and-shallow.com/2020/10/07/forecast-error-measures-intermittent-demand/\n",
    "\n",
    "Average Demand Interval is the average interval in time periods between two non-zero demand. i.e. if the ADI for a time series is 1.9, it means that on an average we see a non-zero demand every 1.9 time periods.\n",
    "\n",
    "ADI = \\frac{\\text{Total number of time periods}}{\\text{Number of non zero demand}}\n",
    "\n",
    "ADI is a measure of intermittency; the higher it is, the ore intermittent the series is.\n",
    "\n",
    "Coefficient of Variation is the standardized standard deviation. We calculate the standard deviation, but then scale it with the mean of the series to guard against scale dependency.\n",
    "\n",
    "COV = \\frac{\\text{Std. Deviation}}{\\text{Mean}}\n",
    "\n",
    "This shows the variability of a time series. If COV^2 is high, that means that the variability in the series is also high.\n",
    "\n",
    "Based on these two demand characteristics, Syntetos and Boylan has theoretically derived cutoff values which defines a marked change in the type of behaviour. They have defined intermittency cutoff as 1.32 and COV^2 cutoff as 0.49. Using these cutoffs, they defined highs and lows and then putting both together a grid which classifies timeseries into Smooth, Erratic, Intermittent, and Lumpy."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd1f5016-96de-427b-9b9b-59fd8f67d48d",
   "metadata": {},
   "source": [
    "## use the average daily demand from historical data as the basis for your naive forecast.\n",
    "### So by having daily demand data for the past year, the forecast for tomorrow would be equal to today’s demand."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "704bd926-a8fa-4a0f-8117-5a21d8ebc79b",
   "metadata": {},
   "source": [
    "# **Naïve extrapolation**\n",
    "Also known as the “naïve forecast,” is a straightforward method for demand forecasting. In Excel, apply this technique by assuming that future demand will be the same as the most recent observed value.\n",
    "- A naïve extrapolation of the receipts trend immediately before the broadcast event on the 21 November gives us some idea of what receipt volumes might have been between December 2023 and March 2024 and therefore what effect the broadcast had on overall receipt volumes. \n",
    "    - create a drop down list with average daily receipts of LPA application in 2024 in the range say from 4000 – 7000 in increments of 100.\n",
    "\n",
    "# **UPDATED FORECAST: AVERAGE DAILY RECEIPTS**\n",
    "In terms of how to apply all of this to the long term LPA model:\n",
    "- Convert the receipts forecast to an annual total by multiplying by the number of working days. If we used the central estimate of 5600 then multiplying this by 256 which the number of working days in 2024 gives an annual total for receipts of 1,433,600. This can be converted into an estimate of the number of donors based on the ratio of donors to receipts (say) over the last couple of years. And then convert the donor estimate into age specific estimates based on the distribution by age , again over (say) the last couple of years."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7eed1172-2917-4817-a84f-0e630c0ddb2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Naïve forecast\n",
    "naïve_forecast = daily_demand.iloc[-1]  # Last observed demand\n",
    "naïve_forecast"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "529b86eb-cc0e-486f-bf3e-b455351107bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Adjust with uncertainty\n",
    "forecast_with_uncertainty = np.random.normal(loc=naïve_forecast, scale=std_dev_demand, size=1000)\n",
    "\n",
    "# Summary of the forecast\n",
    "forecast_mean = forecast_with_uncertainty.mean()\n",
    "forecast_ci_lower = np.percentile(forecast_with_uncertainty, 2.5)\n",
    "forecast_ci_upper = np.percentile(forecast_with_uncertainty, 97.5)\n",
    "\n",
    "print(f\"Forecast Mean: {forecast_mean}\")\n",
    "print(f\"Forecast 95% CI: [{forecast_ci_lower}, {forecast_ci_upper}]\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c826784-6955-465f-a120-daeec16c1446",
   "metadata": {},
   "outputs": [],
   "source": [
    "#df_count_unique_month = pd.DataFrame(count_unique_month)\n",
    "age_lower_limit = 50\n",
    "age_upper_limit = 60\n",
    "age_receipts_2024_50to60 = lpa_unique.loc[(lpa_unique[\"age\"] >= age_lower_limit) &\n",
    "                 (lpa_unique[\"age\"] <= age_upper_limit)]\n",
    "\n",
    "#age_range = [50:70]\n",
    "#count_unique_month.loc[count_unique_month[\"month_year\"].isin(age_range)]\n",
    "\n",
    "receipts_2024_age50to60 = age_receipts_2024_50to60[age_receipts_2024_50to60['month_year'].isin(['Jan-24', 'Feb-24', 'Mar-24'])]\n",
    "#count_unique_month2 = count_unique_month1[count_unique_month1['month_year'].isin([50:70])]\n",
    "#df_count_unique_month['month_year'] = pd.to_datetime(df_count_unique_month['month_year'], format='%b-%y')\n",
    "#df_count_unique_month = df_count_unique_month.sort_values(df_count_unique_month.columns[1])\n",
    "#df_count_unique_month = df_count_unique_month.set_index(['month_year'])\n",
    "receipts_2024_age50to60\n",
    "# Save the result into a csv file\n",
    "#.to_csv(r'count_unique_month_age.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8738aea-0371-46cf-b6eb-cf12e42a918f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Age specific proportion:\n",
    "\n",
    "total_receipts_2024_age50to60 = receipts_2024_age50to60.groupby(\n",
    "    ['status'])['unique_key'].agg('count').reset_index()\n",
    "#total_registered_receipts_2024_age50to60 = total_receipts_2024_age50to60.query(\n",
    "#'status = Registered')\n",
    "total_registered_receipts_2024_age50to60 = total_receipts_2024_age50to60.loc[(\n",
    "    total_receipts_2024_age50to60[\"status\"] == \"Registered\")]\n",
    "\n",
    "total_registered_receipts_2024_age50to60 = total_registered_receipts_2024_age50to60.rename(columns={'unique_key': 'total_reciepts'})\n",
    "\n",
    "total_registered_receipts_2024_age50to60"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff57d124-e347-4c35-9e4e-3c72622907de",
   "metadata": {},
   "outputs": [],
   "source": [
    "#df_count_unique_month = pd.DataFrame(count_unique_month)\n",
    "age_lower_limit = 61\n",
    "age_upper_limit = 71\n",
    "age_receipts_2024_61to71 = lpa_unique.loc[(lpa_unique[\"age\"] >= age_lower_limit) &\n",
    "                 (lpa_unique[\"age\"] <= age_upper_limit)]\n",
    "\n",
    "#age_range = [50:70]\n",
    "#count_unique_month.loc[count_unique_month[\"month_year\"].isin(age_range)]\n",
    "\n",
    "receipts_2024_age51to71 = age_receipts_2024_61to71[age_receipts_2024_61to71['month_year'].isin(['Jan-24', 'Feb-24', 'Mar-24'])]\n",
    "#count_unique_month2 = count_unique_month1[count_unique_month1['month_year'].isin([50:70])]\n",
    "#df_count_unique_month['month_year'] = pd.to_datetime(df_count_unique_month['month_year'], format='%b-%y')\n",
    "#df_count_unique_month = df_count_unique_month.sort_values(df_count_unique_month.columns[1])\n",
    "#df_count_unique_month = df_count_unique_month.set_index(['month_year'])\n",
    "receipts_2024_age51to71\n",
    "# Save the result into a csv file\n",
    "#.to_csv(r'count_unique_month_age.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d46a7bae-1af2-4d30-9fc0-ac34476f0abd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Age specific proportion:\n",
    "\n",
    "total_receipts_2024_age61to71 = receipts_2024_age51to71.groupby(\n",
    "    ['status'])['unique_key'].agg('count').reset_index()\n",
    "#total_registered_receipts_2024_age50to60 = total_receipts_2024_age50to60.query(\n",
    "#'status = Registered')\n",
    "total_registered_receipts_2024_age61to71 = total_receipts_2024_age61to71.loc[(\n",
    "    total_receipts_2024_age61to71[\"status\"] == \"Registered\")]\n",
    "\n",
    "total_registered_receipts_2024_age61to71 = total_registered_receipts_2024_age61to71.rename(columns={'unique_key': 'total_reciepts'})\n",
    "\n",
    "total_registered_receipts_2024_age61to71['total_reciepts']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8907e69-7d63-47a4-bfe8-5a3f53c76139",
   "metadata": {},
   "outputs": [],
   "source": [
    "Total_registrations = total_registered_receipts_2024_age50to60['total_reciepts'] +total_registered_receipts_2024_age61to71['total_reciepts']\n",
    "\n",
    "print(f\"Total registrations among all age groups for the first three month of 2024 is: {Total_registrations}\")\n",
    "\n",
    "proportion_50to60 = total_registered_receipts_2024_age50to60['total_reciepts'] / Total_registrations\n",
    "print(f\"Proportion for 50-60 age group is: {round(proportion_50to60,2)} (or {round(proportion_50to60,2)*100}%).\")\n",
    "\n",
    "proportion_61to71 = total_registered_receipts_2024_age61to71['total_reciepts'] / Total_registrations\n",
    "print(f\"Proportion for 61-71 age group is: {round(proportion_61to71,2)} (or {round(proportion_61to71,2)*100}%).\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb3744c9-cac8-4fc8-9a61-a6b88a57e91d",
   "metadata": {},
   "source": [
    "# Age specific proportion:\n",
    "\n",
    "## Convert the receipts forecast to an annual total by multiplying by the number of working days by the daily forecasts. If we used the central estimate of 5600 then multiplying this by 256, which is the number of working days in 2024, gives an annual total for receipts of 1,433,600. This can be converted into an estimate of the number of donors based on the ratio of donors to receipts over the last couple of years. And then convert the donor estimate into age specific estimates based on the distribution by age (50 to 70 years old) over the last couple of years. Convert to Age-Specific Annual Demand Forecast by:\n",
    "     - Determine the age groups to consider (the impacts on 50- to 70-year-olds).\n",
    "     - Estimate the proportion of LPA demands in each age group.\n",
    "     - Multiply the average daily receipts by the proportion for each age group to get the annual donor forecast.\n",
    "\n",
    "## Calculate Annual Receipts:\n",
    "    - With the central estimate of 5600 daily receipts.\n",
    "    - Multiply this by the number of working days in 2024 (256) to get the annual total for receipts:\n",
    "    - Annual Receipts = 5600 × 256 = 1,433,600\n",
    "\n",
    "## Estimate the Number of Donors:\n",
    "    - Use the historical ratio of donors to receipts over the last couple of years to estimate the number of donors.\n",
    "    - Let’s assume the ratio is consistent. If not, you may need to adjust based on recent trends.\n",
    "\n",
    "## Age-Specific Estimates:\n",
    "    - To convert the donor estimate into age-specific estimates, follow these steps:\n",
    "    - Determine the age groups you want to consider (e.g., 50 to 70 years old).\n",
    "    - Estimate the proportion of LPA demands within each age group.\n",
    "    - Multiply the average daily receipts by the proportion for each age group to get the annual donor forecast for that age group.\n",
    "\n",
    "## Calculate Proportions:\n",
    "- Divide the number of LPA registrations within each age group by the total number of LPA registrations.\n",
    "- This will give you the proportion of LPA demands for each age group.\n",
    "\n",
    "## Apply Proportions:\n",
    "- Multiply the annual donor forecast by the proportion for each age group to get the age-specific annual demand forecast."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3fda269-fa3b-4ad6-8da4-2b1ebc7f0a77",
   "metadata": {
    "tags": []
   },
   "source": [
    "#  Incorporating uncertainty and short-term impacts into your long-term forecasting model: \n",
    "- ensuring that both historical and forecasted data exclude weekends and UK holidays, providing a more accurate and realistic forecast.\n",
    "- generate and analyze age-specific annual demand forecasts based on the overall forecast and specified age group proportions.\n",
    "- convert the receipts forecast to an annual total, estimate the number of donors, and convert the donor estimate into age-specific estimates based on the distribution by age.\n",
    "in order to apply uncertainty based on short term forecasting drivers such as post covid and advertisement impacts of this short term forecasting on the long term forecasting for number of daily reciepts of Living Power of Attorney  (LPA) demands by using average daily demands based on different scenarios Naïve extrapolation for future pandemic demands based on COVID-19 data age-specific reflect the uncertainty around the receipts forecast for next year update quarterly and also to vary the receipts inputs to reflect uncertainty around this estimate which will then also be reflected in the longer term age-specific forecast model?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "563d7700-bf44-495d-92e8-03cb95d3da2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.stats import norm\n",
    "import holidays\n",
    "\n",
    "# ForecastingModel: Encapsulates all forecasting-related methods and attributes.\n",
    "class ForecastingModel:\n",
    "    def __init__(self, historical_data, post_covid_impact=1.1, advertisement_impact=1.2):\n",
    "        self.historical_data = historical_data\n",
    "        self.post_covid_impact = post_covid_impact\n",
    "        self.advertisement_impact = advertisement_impact\n",
    "        self.uk_holidays = holidays.UnitedKingdom()\n",
    "        self.age_group_proportions = {\n",
    "            '50-60': 0.29,\n",
    "            '61-71': 0.71\n",
    "        }\n",
    "        self.working_days_per_year = 256\n",
    "        self.receipts_to_donors_ratio = 0.75  # Example ratio, should be calculated based on historical data\n",
    "\n",
    "\n",
    "    # remove_weekends_and_holidays: Removes weekends and holidays from the data.\n",
    "    def remove_weekends_and_holidays(self, data):\n",
    "        data['weekday'] = data['date'].dt.weekday\n",
    "        data['is_holiday'] = data['date'].isin(self.uk_holidays)\n",
    "        return data[(data['weekday'] < 5) & (~data['is_holiday'])]\n",
    "\n",
    "    #calculate_daily_demand_stats: Computes mean and standard deviation of demand.\n",
    "    def calculate_daily_demand_stats(self, data):\n",
    "        mean_demand = data['demand'].mean()\n",
    "        std_dev_demand = data['demand'].std()\n",
    "        return mean_demand, std_dev_demand\n",
    "\n",
    "    # naive_extrapolation_with_uncertainty: Generates a Naïve forecast with uncertainty.\n",
    "    def naive_extrapolation_with_uncertainty(self, last_observed, std_dev, num_days=90):\n",
    "        forecast = np.random.normal(loc=last_observed, scale=std_dev, size=num_days)\n",
    "        return forecast\n",
    "\n",
    "    # apply_scenario_analysis: Applies short-term impact factors to the base forecast.\n",
    "    def apply_scenario_analysis(self, base_forecast, impact_factor):\n",
    "        return base_forecast * impact_factor\n",
    "\n",
    "    # update_quarterly_forecast: Updates the forecast quarterly and removes weekends/holidays.\n",
    "    def update_quarterly_forecast(self, data, num_quarters=8):\n",
    "        forecasts = []\n",
    "        for _ in range(num_quarters):\n",
    "            mean_demand, std_dev_demand = self.calculate_daily_demand_stats(data)\n",
    "            last_observed = data['demand'].iloc[-1]\n",
    "            quarterly_forecast = self.naive_extrapolation_with_uncertainty(last_observed, std_dev_demand)\n",
    "            \n",
    "            new_dates = pd.date_range(start=data['date'].iloc[-1] + pd.Timedelta(days=1), periods=len(quarterly_forecast))\n",
    "            new_data = pd.DataFrame({'date': new_dates, 'demand': quarterly_forecast})\n",
    "            new_data = self.remove_weekends_and_holidays(new_data)\n",
    "            \n",
    "            forecasts.extend(new_data['demand'])\n",
    "            data = pd.concat([data, new_data], ignore_index=True)\n",
    "        return data\n",
    "\n",
    "    # generate_forecast: Generates a forecast for a specified number of days, with optional short-term impacts.\n",
    "    def generate_forecast(self, num_days=365, short_term=True):\n",
    "        mean_demand, std_dev_demand = self.calculate_daily_demand_stats(\n",
    "            self.historical_data)\n",
    "        base_forecast = self.naive_extrapolation_with_uncertainty(\n",
    "            mean_demand, \n",
    "            std_dev_demand, \n",
    "            num_days)\n",
    "        \n",
    "        if short_term:\n",
    "            forecast_post_covid = self.apply_scenario_analysis(\n",
    "                base_forecast, \n",
    "                self.post_covid_impact)\n",
    "            combined_forecast = self.apply_scenario_analysis(\n",
    "                forecast_post_covid, \n",
    "                self.advertisement_impact)\n",
    "        else:\n",
    "            combined_forecast = base_forecast\n",
    "        \n",
    "        forecast_dates = pd.date_range(\n",
    "            start=self.historical_data['date'].iloc[-1] + \n",
    "            pd.Timedelta(days=1), \n",
    "            periods=num_days)\n",
    "        \n",
    "        forecast_df = pd.DataFrame(\n",
    "            {'date': forecast_dates, \n",
    "             'forecast': combined_forecast})\n",
    "        \n",
    "        forecast_df = self.remove_weekends_and_holidays(forecast_df)\n",
    "        \n",
    "        return forecast_df\n",
    "    \n",
    "    # calculate_annual_total_receipts: Calculates the annual total receipts by multiplying the average daily receipts by the number of working days in the year.\n",
    "    def calculate_annual_total_receipts(self, forecast_df):\n",
    "        average_daily_receipts = forecast_df['forecast'].mean()\n",
    "        annual_total_receipts = average_daily_receipts * self.working_days_per_year\n",
    "        return annual_total_receipts\n",
    "    \n",
    "    # estimate_number_of_donors: Estimates the number of donors based on the annual total receipts and the receipts-to-donors ratio.\n",
    "    def estimate_number_of_donors(self, annual_total_receipts):\n",
    "        number_of_donors = annual_total_receipts * self.receipts_to_donors_ratio\n",
    "        return number_of_donors\n",
    "    \n",
    "    # calculate_age_specific_forecast: Converts the donor estimate into age-specific estimates based on the distribution by age.\n",
    "    def calculate_age_specific_forecast(self, number_of_donors):\n",
    "        age_specific_forecast = {}\n",
    "        for age_group, proportion in self.age_group_proportions.items():\n",
    "            age_specific_forecast[age_group] = number_of_donors * proportion\n",
    "        return age_specific_forecast    \n",
    "    \n",
    "    # plot_forecast: Plots the historical and forecasted data.\n",
    "    def plot_forecast(self, forecast_df):\n",
    "        plt.figure(figsize=(14, 7))\n",
    "        plt.plot(self.historical_data['date'].values, \n",
    "                 self.historical_data['demand'].values, \n",
    "                 label='Historical Data', color='blue')\n",
    "        \n",
    "        plt.plot(forecast_df['date'].values, \n",
    "                 forecast_df['forecast'].values, \n",
    "                 label='Forecast with Impacts', \n",
    "                 color='orange')\n",
    "        \n",
    "        plt.xlabel('Date')\n",
    "        plt.ylabel('Daily Receipts')\n",
    "        plt.title('LPA Daily Receipts Forecast with Uncertainty and Short-term Drivers')\n",
    "        plt.legend()\n",
    "        plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6882afa-bd40-489e-aede-ae21fd1c7ed5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# LPA historical data\n",
    "historical_data = count_unique_receipts_daily[['receiptdate', 'daily_demand']].rename(columns={'receiptdate': 'date', 'daily_demand': 'demand'})\n",
    "\n",
    "# Save the Historical Data\n",
    "historical_data.to_csv('Historical_Data.csv', index=False)\n",
    "\n",
    "# Instantiate the ForecastingModel class\n",
    "forecast_model = ForecastingModel(historical_data)\n",
    "\n",
    "# Remove weekends and holidays from historical data\n",
    "historical_data = forecast_model.remove_weekends_and_holidays(historical_data)\n",
    "\n",
    "# Generate short-term forecast\n",
    "short_term_forecast_df = forecast_model.generate_forecast(num_days=255, \n",
    "                                                          short_term=True)\n",
    "\n",
    "# Generate long-term forecast (without short-term impacts)\n",
    "long_term_forecast_df = forecast_model.generate_forecast(num_days=510, \n",
    "                                                         short_term=False)\n",
    "\n",
    "# Calculate annual total receipts for short-term forecast\n",
    "annual_total_receipts_short_term = forecast_model.calculate_annual_total_receipts(short_term_forecast_df)\n",
    "\n",
    "# Estimate number of donors for short-term forecast\n",
    "number_of_donors_short_term = forecast_model.estimate_number_of_donors(\n",
    "    annual_total_receipts_short_term)\n",
    "\n",
    "# Calculate age-specific forecasts for short-term forecast\n",
    "age_specific_forecast_short_term = forecast_model.calculate_age_specific_forecast(\n",
    "    number_of_donors_short_term)\n",
    "\n",
    "# Calculate annual total receipts for long-term forecast\n",
    "annual_total_receipts_long_term = forecast_model.calculate_annual_total_receipts(\n",
    "    long_term_forecast_df)\n",
    "\n",
    "# Estimate number of donors for long-term forecast\n",
    "number_of_donors_long_term = forecast_model.estimate_number_of_donors(\n",
    "    annual_total_receipts_long_term)\n",
    "\n",
    "# Calculate age-specific forecasts for long-term forecast\n",
    "age_specific_forecast_long_term = forecast_model.calculate_age_specific_forecast(\n",
    "    number_of_donors_long_term)\n",
    "\n",
    "# Plotting the short-term forecast\n",
    "forecast_model.plot_forecast(short_term_forecast_df)\n",
    "\n",
    "# Plotting the long-term forecast\n",
    "forecast_model.plot_forecast(long_term_forecast_df)\n",
    "\n",
    "# Save the short-term forecast data\n",
    "short_term_forecast_df.to_csv('lpa_short_term_forecast.csv', \n",
    "                              index=False)\n",
    "\n",
    "# Save the long-term forecast data\n",
    "long_term_forecast_df.to_csv('lpa_long_term_forecast.csv', \n",
    "                             index=False)\n",
    "\n",
    "# Print annual total receipts, number of donors, and age-specific forecasts\n",
    "print(\"Annual total receipts (short-term):\", annual_total_receipts_short_term)\n",
    "print(\"Number of donors (short-term):\", number_of_donors_short_term)\n",
    "print(\"Age-specific forecast (short-term):\", age_specific_forecast_short_term)\n",
    "\n",
    "print(\"Annual total receipts (long-term):\", annual_total_receipts_long_term)\n",
    "print(\"Number of donors (long-term):\", number_of_donors_long_term)\n",
    "print(\"Age-specific forecast (long-term):\", age_specific_forecast_long_term)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19a383ca-cf7f-4636-bdb3-f2fc6d0c91bb",
   "metadata": {},
   "source": [
    "# implement the above Python code in Excel, \n",
    "creating a drop down list with the average daily receipts of LPA application demands in 2024 in the range of 4000–7000 in increments of 100. \n",
    "Then this should be used as an estimate to apply uncertainty and be converted into an age-specific annual donor forecast?\n",
    "\n",
    "## Calculate Proportions:\n",
    "Divide the number of LPA registrations within each age group by the total number of LPA registrations.\n",
    "This will give you the proportion of LPA demands for each age group.\n",
    "Example Calculation:\n",
    "Suppose you have data showing 100 LPA registrations in the 50-60 age group and 50 registrations in the 61-70 age group.\n",
    "Total registrations = 100 + 50 = 150.\n",
    "Proportion for 50-60 age group = 100 / 150 = 0.67 (or 67%).\n",
    "Proportion for 61-70 age group = 50 / 150 = 0.33 (or 33%).\n",
    "Apply Proportions:\n",
    "Multiply the annual donor forecast by the proportion for each age group to get the age-specific annual demand forecast."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83244177-08ad-4fd4-add8-474d086648de",
   "metadata": {},
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from openpyxl import Workbook\n",
    "from openpyxl.utils.dataframe import dataframe_to_rows\n",
    "from openpyxl.worksheet.datavalidation import DataValidation\n",
    "from datetime import timedelta\n",
    "import holidays\n",
    "\n",
    "# Initialize the workbook and worksheet\n",
    "wb = Workbook()\n",
    "ws = wb.active\n",
    "ws.title = \"LPA Forecast Model\"\n",
    "\n",
    "# Create a dropdown list for average daily receipts\n",
    "dv = DataValidation(type=\"list\", formula1='\"4000,4100,4200,4300,4400,4500,4600,4700,4800,4900,5000,5100,5200,5300,5400,5500,5600,5700,5800,5900,6000,6100,6200,6300,6400,6500,6600,6700,6800,6900,7000\"', showDropDown=True)\n",
    "ws.add_data_validation(dv)\n",
    "ws['A1'] = \"Average Daily Receipts\"\n",
    "ws['B1'] = 5600  # Default value\n",
    "dv.add(ws['B1'])\n",
    "\n",
    "# Setup headers for the historical data\n",
    "ws.append([\"Date\", \"Demand\"])\n",
    "# Generate sample historical data\n",
    "np.random.seed(42)\n",
    "dates = pd.date_range(start='2020-01-01', end='2023-12-31', freq='D')\n",
    "historical_data = pd.DataFrame({\n",
    "    'date': dates,\n",
    "    'demand': np.random.poisson(lam=100, size=len(dates))  # Simulating daily demands\n",
    "})\n",
    "\n",
    "# Add historical data to worksheet\n",
    "for r in dataframe_to_rows(historical_data, index=False, header=False):\n",
    "    ws.append(r)\n",
    "\n",
    "# Calculate and add working days for forecast\n",
    "forecast_dates = pd.date_range(start='2024-01-01', end='2024-12-31', freq='D')\n",
    "forecast_data = pd.DataFrame({'date': forecast_dates})\n",
    "forecast_data['weekday'] = forecast_data['date'].dt.weekday\n",
    "forecast_data['is_holiday'] = forecast_data['date'].astype(str).isin(holidays.UnitedKingdom(years=2024))\n",
    "forecast_data['working_day'] = (forecast_data['weekday'] < 5) & (~forecast_data['is_holiday'])\n",
    "working_days_2024 = forecast_data['working_day'].sum()\n",
    "\n",
    "# Insert forecast model structure and calculations in Excel\n",
    "ws['A10'] = \"Forecast Parameters\"\n",
    "ws['A11'] = \"Post-COVID Impact\"\n",
    "ws['B11'] = 1.1\n",
    "ws['A12'] = \"Advertisement Impact\"\n",
    "ws['B12'] = 1.2\n",
    "\n",
    "ws['A14'] = \"Working Days in 2024\"\n",
    "ws['B14'] = working_days_2024\n",
    "\n",
    "ws['A16'] = \"Receipts to Donors Ratio\"\n",
    "ws['B16'] = 0.75\n",
    "\n",
    "ws['A18'] = \"Age Group\"\n",
    "ws['B18'] = \"Proportion\"\n",
    "ws['A19'] = \"50-59\"\n",
    "ws['B19'] = 0.25\n",
    "ws['A20'] = \"60-69\"\n",
    "ws['B20'] = 0.30\n",
    "ws['A21'] = \"70+\"\n",
    "ws['B21'] = 0.45\n",
    "\n",
    "ws['A23'] = \"Estimated Average Daily Receipts\"\n",
    "ws['B23'] = \"=B1\"\n",
    "\n",
    "ws['A24'] = \"Annual Total Receipts\"\n",
    "ws['B24'] = \"=B23 * B14\"\n",
    "\n",
    "ws['A25'] = \"Estimated Number of Donors\"\n",
    "ws['B25'] = \"=B24 * B16\"\n",
    "\n",
    "ws['A27'] = \"Age-Specific Annual Donor Forecast\"\n",
    "ws['A28'] = \"50-59\"\n",
    "ws['B28'] = \"=B25 * B19\"\n",
    "ws['A29'] = \"60-69\"\n",
    "ws['B29'] = \"=B25 * B20\"\n",
    "ws['A30'] = \"70+\"\n",
    "ws['B30'] = \"=B25 * B21\"\n",
    "\n",
    "# Save the workbook to a file\n",
    "file_path = 'LPA_Forecast_Model.xlsx'\n",
    "wb.save(file_path)\n",
    "file_path"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45d7bdf9-7a75-4618-8285-9d2d7d1d5345",
   "metadata": {},
   "source": [
    "Step-by-Step Guide for Excel Model\n",
    "1. Historical Data\n",
    "Create a Data Sheet:\n",
    "Name a sheet Historical Data.\n",
    "In column A, input your historical dates.\n",
    "In column B, input your historical daily demand.\n",
    "2. Remove Weekends and Holidays\n",
    "Create a Holiday List:\n",
    "\n",
    "Name a sheet Holidays.\n",
    "List UK holidays in column A.\n",
    "Add Formulas to Check for Weekends and Holidays:\n",
    "\n",
    "In Historical Data, add columns for Weekday and IsHoliday.\n",
    "Weekday: Use =WEEKDAY(A2, 2) to get the weekday number (1 for Monday, 7 for Sunday).\n",
    "IsHoliday: Use =IF(COUNTIF(Holidays!$A$2:$A$100, A2) > 0, TRUE, FALSE) to check if the date is a holiday.\n",
    "Filter out weekends and holidays: Use =AND(B2<>6, B2<>7, NOT(C2)) to filter only working days.\n",
    "3. Calculate Average and Standard Deviation\n",
    "Calculate Statistics:\n",
    "Use =AVERAGEIFS(B2:B1000, C2:C1000, TRUE) to calculate the mean of working day demands.\n",
    "Use =STDEV.P(IF(D2:D1000, B2:B1000)) to calculate the standard deviation, using an array formula.\n",
    "4. Naïve Extrapolation with Uncertainty\n",
    "Create a Forecast Sheet:\n",
    "\n",
    "Name a sheet Forecast.\n",
    "In column A, list future dates for the forecast period.\n",
    "Use a formula like =A2 + 1 to generate sequential dates.\n",
    "Generate Naïve Forecast:\n",
    "\n",
    "In column B, use the average calculated earlier as the base forecast.\n",
    "Add random noise to the forecast to introduce uncertainty using =NORMINV(RAND(), $B$1, $C$1), where $B$1 is the mean and $C$1 is the standard deviation.\n",
    "5. Apply Short-Term Impacts\n",
    "Post-COVID and Advertisement Impact:\n",
    "Use two separate columns to apply impacts.\n",
    "Post-COVID Impact: Multiply the base forecast by a factor, e.g., =B2 * 1.1.\n",
    "Advertisement Impact: Multiply the post-COVID forecast by another factor, e.g., =C2 * 1.2.\n",
    "6. Combine Impacts and Filter Weekends and Holidays\n",
    "Combined Impact Forecast:\n",
    "\n",
    "Combine impacts in a new column: =D2 * E2.\n",
    "Filter out Weekends and Holidays:\n",
    "\n",
    "Repeat the steps used in the historical data to remove weekends and holidays from the forecast.\n",
    "7. Quarterly Updates\n",
    "Create Quarterly Forecasts:\n",
    "Use separate sections in the Forecast sheet for each quarter.\n",
    "Update the base forecast using the last observed demand.\n",
    "Excel Implementation Example\n",
    "Here is an example layout of the Excel sheet with formulas.\n",
    "\n",
    "Historical Data Sheet\n",
    "Date\tDemand\tWeekday\tIsHoliday\tWorkingDay\n",
    "01/01/2020\t100\t=WEEKDAY(A2, 2)\t=IF(COUNTIF(Holidays!$A$2:$A$100, A2) > 0, TRUE, FALSE)\t=AND(B2<>6, B2<>7, NOT(C2))\n",
    "02/01/2020\t120\t...\t...\t...\n",
    "...\t...\t...\t...\t...\n",
    "Forecast Sheet\n",
    "Date\tBase Forecast\tPost-COVID Impact\tAdvertisement Impact\tCombined Impact\tWeekday\tIsHoliday\tWorkingDay\n",
    "01/01/2024\t=AVERAGE($B$2:$B$1000)\t=B2 * 1.1\t=C2 * 1.2\t=D2 * E2\t=WEEKDAY(A2, 2)\t=IF(COUNTIF(Holidays!$A$2:$A$100, A2) > 0, TRUE, FALSE)\t=AND(F2<>6, F2<>7, NOT(G2))\n",
    "02/01/2024\t=NORMINV(RAND(), $B$1, $C$1)\t...\t...\t...\t...\t...\t...\n",
    "...\t...\t...\t...\t...\t...\t...\t...\n",
    "Tips\n",
    "Use Excel's Data Analysis Toolpak to assist with statistical functions if needed.\n",
    "Create dynamic named ranges for holidays to easily update the list.\n",
    "Use Excel's conditional formatting and filters to highlight weekends and holidays.\n",
    "By following these steps and formulas, you can build an Excel model that captures the essence of the Python script, including handling uncertainty and applying short-term impacts on your forecasting.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "857fd46c-1d23-47aa-8e33-99baadf2a198",
   "metadata": {},
   "source": [
    "Removing Weekends and Holidays:\n",
    "In Excel, you can use the WEEKDAY function to determine the day of the week (0 for Sunday, 1 for Monday, etc.).\n",
    "To exclude weekends and holidays, you’ll need a separate column to mark holidays (e.g., “Is Holiday”) and filter out weekends (where the weekday is 0 or 6).\n",
    "The filtered data will be your adjusted historical data.\n",
    "Calculating Daily Demand Statistics:\n",
    "Use the AVERAGE function to calculate the mean demand.\n",
    "Use the STDEV function to calculate the standard deviation of demand.\n",
    "Naive Extrapolation with Uncertainty:\n",
    "Assuming your last observed demand is in cell A1, you can use the following formula to generate a random forecast:\n",
    "=NORM.INV(RAND(), A1, std_dev_demand)\n",
    "This formula generates a random value from a normal distribution with the mean of the last observed demand and the specified standard deviation.\n",
    "Scenario Analysis:\n",
    "If you have post-COVID and advertisement impact factors (e.g., 1.1 and 1.2), multiply your forecast by these factors accordingly.\n",
    "Quarterly Forecast Update:\n",
    "Assuming your historical data starts from cell A2, use the following formula to calculate the quarterly forecast:\n",
    "=A2 + NORM.INV(RAND(), mean_demand, std_dev_demand)\n",
    "Drag this formula down for the desired number of quarters.\n",
    "Generating the Forecast:\n",
    "Create a date column (e.g., starting from cell B2) with the next day after your historical data.\n",
    "In the adjacent column, use the formula for the combined forecast (including post-COVID and advertisement impacts).\n",
    "Annual Total Receipts and Donor Estimates:\n",
    "Calculate the average daily receipts from your forecast.\n",
    "Multiply by the number of working days in a year (256) to get annual total receipts.\n",
    "Estimate the number of donors based on your receipts-to-donors ratio.\n",
    "Age-Specific Forecast:\n",
    "Multiply the total number of donors by the proportion for each age group."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a45a2ef-770b-442e-803d-9ec86f2aebb0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db530cfa-5968-4c6b-866e-4f15154ff771",
   "metadata": {},
   "outputs": [],
   "source": [
    "Explanation\n",
    "Class Definition:\n",
    "\n",
    "ForecastingModel: Encapsulates all forecasting-related methods and attributes.\n",
    "__init__: Initializes the class with historical data and impact factors for short-term scenarios.\n",
    "remove_weekends_and_holidays: Removes weekends and holidays from the data.\n",
    "calculate_daily_demand_stats: Computes mean and standard deviation of demand.\n",
    "naive_extrapolation_with_uncertainty: Generates a Naïve forecast with uncertainty.\n",
    "apply_scenario_analysis: Applies short-term impact factors to the base forecast.\n",
    "update_quarterly_forecast: Updates the forecast quarterly and removes weekends/holidays.\n",
    "generate_forecast: Generates a forecast for a specified number of days, with optional short-term impacts.\n",
    "plot_forecast: Plots the historical and forecasted data.\n",
    "Data Simulation and Forecasting:\n",
    "\n",
    "Generates sample historical data.\n",
    "Instantiates the ForecastingModel class.\n",
    "Removes weekends and holidays from the historical data.\n",
    "Generates both short-term and long-term forecasts.\n",
    "Plots the forecasts.\n",
    "Saves the forecast data to CSV files.\n",
    "This OOP-based approach allows you to easily extend and customize the forecasting model, and to generate and plot forecasts for different scenarios (short-term and long-term) in a structured manner."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55fb3368-4eb7-4c6c-91de-f9aa3060b8e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.stats import norm\n",
    "\n",
    "# Generate sample historical data\n",
    "np.random.seed(42)\n",
    "dates = pd.date_range(start='2020-01-01', end='2023-12-31', freq='D')\n",
    "historical_data = pd.DataFrame({\n",
    "    'date': dates,\n",
    "    'demand': np.random.poisson(lam=100, size=len(dates))  # Simulating daily demands\n",
    "})\n",
    "\n",
    "# Function to calculate average daily demand and uncertainty\n",
    "def calculate_daily_demand_stats(data):\n",
    "    mean_demand = data['demand'].mean()\n",
    "    std_dev_demand = data['demand'].std()\n",
    "    return mean_demand, std_dev_demand\n",
    "\n",
    "# Function to perform Naïve extrapolation with uncertainty\n",
    "def naive_extrapolation_with_uncertainty(last_observed, std_dev, num_days=90):\n",
    "    forecast = np.random.normal(loc=last_observed, scale=std_dev, size=num_days)\n",
    "    return forecast\n",
    "\n",
    "# Function to update quarterly forecasts\n",
    "def update_quarterly_forecast(data, num_quarters=4):\n",
    "    forecasts = []\n",
    "    for _ in range(num_quarters):\n",
    "        mean_demand, std_dev_demand = calculate_daily_demand_stats(data)\n",
    "        last_observed = data['demand'].iloc[-1]\n",
    "        quarterly_forecast = naive_extrapolation_with_uncertainty(last_observed, std_dev_demand)\n",
    "        forecasts.extend(quarterly_forecast)\n",
    "        new_dates = pd.date_range(start=data['date'].iloc[-1] + pd.Timedelta(days=1), periods=len(quarterly_forecast))\n",
    "        new_data = pd.DataFrame({'date': new_dates, 'demand': quarterly_forecast})\n",
    "        data = pd.concat([data, new_data], ignore_index=True)\n",
    "    return data\n",
    "\n",
    "# Function to apply scenario analysis\n",
    "def apply_scenario_analysis(base_forecast, impact_factor):\n",
    "    return base_forecast * impact_factor\n",
    "\n",
    "# Apply short-term drivers (post-COVID and advertisement impacts)\n",
    "post_covid_impact = 1.1  # 10% increase\n",
    "advertisement_impact = 1.2  # 20% increase\n",
    "\n",
    "# Generate base forecast\n",
    "mean_demand, std_dev_demand = calculate_daily_demand_stats(historical_data)\n",
    "base_forecast = naive_extrapolation_with_uncertainty(mean_demand, std_dev_demand, num_days=365)\n",
    "\n",
    "# Apply scenario impacts\n",
    "forecast_post_covid = apply_scenario_analysis(base_forecast, post_covid_impact)\n",
    "forecast_advertisement = apply_scenario_analysis(base_forecast, advertisement_impact)\n",
    "\n",
    "# Combine impacts\n",
    "combined_forecast = apply_scenario_analysis(forecast_post_covid, advertisement_impact)\n",
    "\n",
    "# Update quarterly forecasts\n",
    "updated_forecast_data = update_quarterly_forecast(historical_data)\n",
    "\n",
    "# Plotting the results\n",
    "plt.figure(figsize=(14, 7))\n",
    "plt.plot(historical_data['date'], historical_data['demand'], label='Historical Data', color='blue')\n",
    "plt.plot(pd.date_range(start=historical_data['date'].iloc[-1] + pd.Timedelta(days=1), periods=365), \n",
    "         combined_forecast, label='Forecast with Impacts', color='orange')\n",
    "plt.xlabel('Date')\n",
    "plt.ylabel('Daily Receipts')\n",
    "plt.title('LPA Daily Receipts Forecast with Uncertainty and Short-term Drivers')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "print(\"Forecast with combined impacts for the next year:\")\n",
    "print(combined_forecast)\n",
    "\n",
    "# Save the forecast data\n",
    "forecast_dates = pd.date_range(start=historical_data['date'].iloc[-1] + pd.Timedelta(days=1), periods=365)\n",
    "forecast_df = pd.DataFrame({'date': forecast_dates, 'forecast': combined_forecast})\n",
    "forecast_df.to_csv('lpa_forecast.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b7f3282-a186-42ec-a232-bdd401c30b6d",
   "metadata": {},
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.stats import norm\n",
    "import holidays\n",
    "\n",
    "# Generate sample historical data\n",
    "np.random.seed(42)\n",
    "dates = pd.date_range(start='2020-01-01', end='2023-12-31', freq='D')\n",
    "historical_data = pd.DataFrame({\n",
    "    'date': dates,\n",
    "    'demand': np.random.poisson(lam=100, size=len(dates))  # Simulating daily demands\n",
    "})\n",
    "\n",
    "# UK holidays\n",
    "uk_holidays = holidays.UnitedKingdom()\n",
    "\n",
    "# Function to remove weekends and holidays\n",
    "def remove_weekends_and_holidays(data):\n",
    "    data['weekday'] = data['date'].dt.weekday\n",
    "    data['is_holiday'] = data['date'].isin(uk_holidays)\n",
    "    return data[(data['weekday'] < 5) & (~data['is_holiday'])]\n",
    "\n",
    "# Remove weekends and holidays from historical data\n",
    "historical_data = remove_weekends_and_holidays(historical_data)\n",
    "\n",
    "# Function to calculate average daily demand and uncertainty\n",
    "def calculate_daily_demand_stats(data):\n",
    "    mean_demand = data['demand'].mean()\n",
    "    std_dev_demand = data['demand'].std()\n",
    "    return mean_demand, std_dev_demand\n",
    "\n",
    "# Function to perform Naïve extrapolation with uncertainty\n",
    "def naive_extrapolation_with_uncertainty(last_observed, std_dev, num_days=90):\n",
    "    forecast = np.random.normal(loc=last_observed, scale=std_dev, size=num_days)\n",
    "    return forecast\n",
    "\n",
    "# Function to update quarterly forecasts\n",
    "def update_quarterly_forecast(data, num_quarters=4):\n",
    "    forecasts = []\n",
    "    for _ in range(num_quarters):\n",
    "        mean_demand, std_dev_demand = calculate_daily_demand_stats(data)\n",
    "        last_observed = data['demand'].iloc[-1]\n",
    "        quarterly_forecast = naive_extrapolation_with_uncertainty(last_observed, std_dev_demand)\n",
    "        \n",
    "        # Generate new dates for the forecast period\n",
    "        new_dates = pd.date_range(start=data['date'].iloc[-1] + pd.Timedelta(days=1), periods=len(quarterly_forecast))\n",
    "        new_data = pd.DataFrame({'date': new_dates, 'demand': quarterly_forecast})\n",
    "        \n",
    "        # Remove weekends and holidays from new data\n",
    "        new_data = remove_weekends_and_holidays(new_data)\n",
    "        \n",
    "        forecasts.extend(new_data['demand'])\n",
    "        data = pd.concat([data, new_data], ignore_index=True)\n",
    "    return data\n",
    "\n",
    "# Function to apply scenario analysis\n",
    "def apply_scenario_analysis(base_forecast, impact_factor):\n",
    "    return base_forecast * impact_factor\n",
    "\n",
    "# Apply short-term drivers (post-COVID and advertisement impacts)\n",
    "post_covid_impact = 1.1  # 10% increase\n",
    "advertisement_impact = 1.2  # 20% increase\n",
    "\n",
    "# Generate base forecast\n",
    "mean_demand, std_dev_demand = calculate_daily_demand_stats(historical_data)\n",
    "base_forecast = naive_extrapolation_with_uncertainty(mean_demand, std_dev_demand, num_days=365)\n",
    "\n",
    "# Apply scenario impacts\n",
    "forecast_post_covid = apply_scenario_analysis(base_forecast, post_covid_impact)\n",
    "forecast_advertisement = apply_scenario_analysis(base_forecast, advertisement_impact)\n",
    "\n",
    "# Combine impacts\n",
    "combined_forecast = apply_scenario_analysis(forecast_post_covid, advertisement_impact)\n",
    "\n",
    "# Update quarterly forecasts\n",
    "updated_forecast_data = update_quarterly_forecast(historical_data)\n",
    "\n",
    "# Plotting the results\n",
    "plt.figure(figsize=(14, 7))\n",
    "plt.plot(historical_data['date'], historical_data['demand'], label='Historical Data', color='blue')\n",
    "plt.plot(pd.date_range(start=historical_data['date'].iloc[-1] + pd.Timedelta(days=1), periods=len(combined_forecast)), \n",
    "         combined_forecast, label='Forecast with Impacts', color='orange')\n",
    "plt.xlabel('Date')\n",
    "plt.ylabel('Daily Receipts')\n",
    "plt.title('LPA Daily Receipts Forecast with Uncertainty and Short-term Drivers')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "print(\"Forecast with combined impacts for the next year:\")\n",
    "print(combined_forecast)\n",
    "\n",
    "# Save the forecast data\n",
    "forecast_dates = pd.date_range(start=historical_data['date'].iloc[-1] + pd.Timedelta(days=1), periods=len(combined_forecast))\n",
    "forecast_df = pd.DataFrame({'date': forecast_dates, 'forecast': combined_forecast})\n",
    "forecast_df = remove_weekends_and_holidays(forecast_df)\n",
    "forecast_df.to_csv('lpa_forecast.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa831dac-eaf1-4707-a462-a00c51135a1f",
   "metadata": {},
   "source": [
    "# Generate Random Quarterly Forecast IN EXCEL:\n",
    "In cell B2 (or any other cell where you want to start your quarterly forecast), use the following formula:\n",
    "=A2 + NORM.INV(RAND(), mean_demand, std_dev_demand)\n",
    "\n",
    "A2 represents the last observed demand.\n",
    "mean_demand and std_dev_demand are the calculated mean and standard deviation of daily demand, respectively.\n",
    "This formula generates a random value from a normal distribution with the specified mean and standard deviation.\n",
    "Drag the Formula:\n",
    "Drag the formula down for the desired number of quarters (e.g., drag it down to B3, B4, etc.).\n",
    "Each cell will calculate a new quarterly forecast based on the previous quarter’s demand."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9487d5c-0430-4a8f-9db5-92eddf6ff043",
   "metadata": {},
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.stats import norm\n",
    "import holidays\n",
    "\n",
    "# Generate sample historical data\n",
    "np.random.seed(42)\n",
    "dates = pd.date_range(start='2020-01-01', end='2023-12-31', freq='D')\n",
    "historical_data = pd.DataFrame({\n",
    "    'date': dates,\n",
    "    'demand': np.random.poisson(lam=100, size=len(dates))  # Simulating daily demands\n",
    "})\n",
    "\n",
    "# UK holidays\n",
    "uk_holidays = holidays.UnitedKingdom()\n",
    "\n",
    "# Function to remove weekends and holidays\n",
    "def remove_weekends_and_holidays(data):\n",
    "    data['weekday'] = data['date'].dt.weekday\n",
    "    data['is_holiday'] = data['date'].isin(uk_holidays)\n",
    "    return data[(data['weekday'] < 5) & (~data['is_holiday'])]\n",
    "\n",
    "# Remove weekends and holidays from historical data\n",
    "historical_data = remove_weekends_and_holidays(historical_data)\n",
    "\n",
    "# Function to calculate average daily demand and uncertainty\n",
    "def calculate_daily_demand_stats(data):\n",
    "    mean_demand = data['demand'].mean()\n",
    "    std_dev_demand = data['demand'].std()\n",
    "    return mean_demand, std_dev_demand\n",
    "\n",
    "# Function to perform Naïve extrapolation with uncertainty\n",
    "def naive_extrapolation_with_uncertainty(last_observed, std_dev, num_days=90):\n",
    "    forecast = np.random.normal(loc=last_observed, scale=std_dev, size=num_days)\n",
    "    return forecast\n",
    "\n",
    "# Function to update quarterly forecasts\n",
    "def update_quarterly_forecast(data, num_quarters=4):\n",
    "    forecasts = []\n",
    "    for _ in range(num_quarters):\n",
    "        mean_demand, std_dev_demand = calculate_daily_demand_stats(data)\n",
    "        last_observed = data['demand'].iloc[-1]\n",
    "        quarterly_forecast = naive_extrapolation_with_uncertainty(last_observed, std_dev_demand)\n",
    "        \n",
    "        # Generate new dates for the forecast period\n",
    "        new_dates = pd.date_range(start=data['date'].iloc[-1] + pd.Timedelta(days=1), periods=len(quarterly_forecast))\n",
    "        new_data = pd.DataFrame({'date': new_dates, 'demand': quarterly_forecast})\n",
    "        \n",
    "        # Remove weekends and holidays from new data\n",
    "        new_data = remove_weekends_and_holidays(new_data)\n",
    "        \n",
    "        forecasts.extend(new_data['demand'])\n",
    "        data = pd.concat([data, new_data], ignore_index=True)\n",
    "    return data\n",
    "\n",
    "# Function to apply scenario analysis\n",
    "def apply_scenario_analysis(base_forecast, impact_factor):\n",
    "    return base_forecast * impact_factor\n",
    "\n",
    "# Apply short-term drivers (post-COVID and advertisement impacts)\n",
    "post_covid_impact = 1.1  # 10% increase\n",
    "advertisement_impact = 1.2  # 20% increase\n",
    "\n",
    "# Generate base forecast\n",
    "mean_demand, std_dev_demand = calculate_daily_demand_stats(historical_data)\n",
    "base_forecast = naive_extrapolation_with_uncertainty(mean_demand, std_dev_demand, num_days=365)\n",
    "\n",
    "# Apply scenario impacts\n",
    "forecast_post_covid = apply_scenario_analysis(base_forecast, post_covid_impact)\n",
    "forecast_advertisement = apply_scenario_analysis(base_forecast, advertisement_impact)\n",
    "\n",
    "# Combine impacts\n",
    "combined_forecast = apply_scenario_analysis(forecast_post_covid, advertisement_impact)\n",
    "\n",
    "# Update quarterly forecasts\n",
    "updated_forecast_data = update_quarterly_forecast(historical_data)\n",
    "\n",
    "# Plotting the results\n",
    "plt.figure(figsize=(14, 7))\n",
    "plt.plot(historical_data['date'], historical_data['demand'], label='Historical Data', color='blue')\n",
    "plt.plot(pd.date_range(start=historical_data['date'].iloc[-1] + pd.Timedelta(days=1), periods=len(combined_forecast)), \n",
    "         combined_forecast, label='Forecast with Impacts', color='orange')\n",
    "plt.xlabel('Date')\n",
    "plt.ylabel('Daily Receipts')\n",
    "plt.title('LPA Daily Receipts Forecast with Uncertainty and Short-term Drivers')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "print(\"Forecast with combined impacts for the next year:\")\n",
    "print(combined_forecast)\n",
    "\n",
    "# Save the forecast data\n",
    "forecast_dates = pd.date_range(start=historical_data['date'].iloc[-1] + pd.Timedelta(days=1), periods=len(combined_forecast))\n",
    "forecast_df = pd.DataFrame({'date': forecast_dates, 'forecast': combined_forecast})\n",
    "forecast_df = remove_weekends_and_holidays(forecast_df)\n",
    "forecast_df.to_csv('lpa_forecast.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0a54238-8ee2-4843-a64a-016bd487c8a0",
   "metadata": {},
   "source": [
    "import statsmodels.api as sm\n",
    "from statsmodels.tsa.arima.model import ARIMA\n",
    "\n",
    "# Load your LPA demand data (replace with your actual data)\n",
    "# Assume 'daily_demand' is a pandas Series with daily demand values\n",
    "\n",
    "# Short-term forecasting (Naïve Extrapolation)\n",
    "naive_forecast = daily_demand.shift(1)\n",
    "\n",
    "# Calculate prediction intervals (adjust alpha as needed)\n",
    "model = ARIMA(daily_demand, order=(1, 1, 1))\n",
    "model_fit = model.fit()\n",
    "forecast, stderr, conf_int = model_fit.forecast(steps=1, alpha=0.05)\n",
    "\n",
    "# Incorporate advertising impact (adjust as needed)\n",
    "# Example: Multiply forecast by advertising factor (low, medium, high)\n",
    "\n",
    "# Reflect post-COVID-19 impact (adjust as needed)\n",
    "# Example: Adjust forecast based on historical COVID-19 data\n",
    "\n",
    "# Long-term age-specific model (create your own)\n",
    "# Apply age-specific factors to overall forecast\n",
    "\n",
    "# Quarterly updates (revisit and adjust)\n",
    "\n",
    "# Print results\n",
    "print(\"Naïve Forecast for t+1:\", naive_forecast.iloc[-1])\n",
    "print(\"Forecast for t+1:\", forecast[0])\n",
    "print(\"Prediction Interval (95%):\", conf_int[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e246d0e6-0ba9-479e-916f-91e9da2d3eb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "count_unique_receipts_monthly = count_unique_receipts_daily.groupby(['year', 'month_year'], as_index=False).mean()\n",
    "count_unique_receipts_monthly = count_unique_receipts_monthly.rename(columns={'daily_demand': 'avg_monthly_demand'})\n",
    "\n",
    "# Display the result\n",
    "print(count_unique_receipts_monthly)\n",
    "\n",
    "count_unique_receipts_annual = count_unique_receipts_monthly.groupby(['year'], as_index=False).mean()\n",
    "count_unique_receipts_annual = count_unique_receipts_annual.rename(columns={'avg_monthly_demand': 'avg_annual_demand'})\n",
    "\n",
    "# Sort\n",
    "count_unique_receipts_annual = count_unique_receipts_annual.sort_values(by=['year'])\n",
    "\n",
    "# Display the result\n",
    "print(count_unique_receipts_annual)\n",
    "\n",
    "#count_unique_receipts_daily['Overall_Percentage'] = count_unique_receipts_daily.iloc[:, 1].mean(axis=1)\n",
    "\n",
    "# Save the result into a csv file\n",
    "#lpa_reciepts.to_csv(r'lpa_reciepts.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b790d55-dc45-4085-8b7a-197e8ba2c4d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "lpa_actuals = lpa_unique.query('year > 2021')\n",
    "lpa_actuals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef0070df-bdd4-4b38-b6f0-6604d1742370",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Set index\n",
    "lpa_actuals['receiptdate'] = pd.to_datetime(lpa_actuals['receiptdate'])\n",
    "\n",
    "lpa_actuals = lpa_actuals.set_index('receiptdate')#.asfreq('D')\n",
    "\n",
    "print(lpa_actuals.head())\n",
    "print(lpa_actuals.tail())\n",
    "\n",
    "## Select the appropriate variable to be forecasted\n",
    "lpa_actuals_data = lpa_actuals['unique_key']\n",
    "\n",
    "## infer the frequency of the data:\n",
    "#lpa_actuals_data = lpa_actuals_data.asfreq(pd.infer_freq(lpa_actuals_data.index))\n",
    "\n",
    "# lim_divorce_data = divorce_data[start_date:end_date]\n",
    "\n",
    "# start_date_years = datetime.strptime(start_date, \n",
    "#                                      '%Y-%m-%d') + relativedelta(years = 0)\n",
    "# print(start_date_years)\n",
    "\n",
    "# start_date_formatted = start_date_years.date()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "886d04ee-4369-4264-8d47-c56accb21dea",
   "metadata": {},
   "outputs": [],
   "source": [
    "lpa_unique.info()\n",
    "lpa_unique.shape\n",
    "lpa_unique.describe()\n",
    "lpa_unique.corr()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5bcd6356-df32-4a8f-8197-d82756375d7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Visualisation of the time series\n",
    "#Plot the Actual Divorces\n",
    "\n",
    "plt.figure(figsize=(20, 8))\n",
    "plt.plot(lpa_actuals_data)\n",
    "plt.title('UK Actual LPA Data', fontsize=20)\n",
    "plt.ylabel('Demands', fontsize=16)\n",
    "\n",
    "for year in range(2023,2024): #datetime.strptime(end_date, '%Y-%m-%d').year):\n",
    "    plt.axvline(pd.to_datetime(str(year) + '-01-01'), color = 'k', \n",
    "                linestyle='--', alpha = 0.2)\n",
    "    \n",
    "plt.savefig('ActualLPAData.png', dpi=300, bbox_inches='tight')\n",
    "plt.show()  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce045b7c-19b9-4733-98d5-e62d35b95d61",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Average daily receipts for 2024\n",
    "\n",
    "# create a drop down list with average daily receipts of LPA application in 2024 in the range say from 4000 – 7000 in increments of 100. \n",
    "# Then this should be used as an estimate to apply unceratinty and to be converted into an age-specific annual donor forecast.\n",
    "\n",
    "# Filter data to involve Registered status and Post-covid data from 2022 onwards\n",
    "unique_receipts_post_covid = lpa_unique\n",
    "unique_receipts_post_covid[unique_receipts_post_covid['status'].str.contains(\"Registered\")]\n",
    "unique_receipts_post_covid = unique_receipts_post_covid.query('year > 2023')\n",
    "#df[df['Overall_Percentage'].isin([value for value in df['Overall_Percentage'] if value > 60])]\n",
    "#df[df.apply(lambda row: row['Overall_Percentage'] > 55, axis=1)]\n",
    "# # The “loc” method is used to access a group of rows and columns by label(s) or a boolean array. \n",
    "# #We can utilise it to filter a DataFrame based on specific column values.\n",
    "#df.loc[df['Overall_Percentage'] > 40]\n",
    "# # The “iloc” method is similar to “loc” but uses integer-based indexing instead of labels. \n",
    "# #It allows us to filter a DataFrame by specifying the row and column indices.\n",
    "#df[df.iloc[:, -1] > 40]\n",
    "\n",
    "\n",
    "#count_unique_receipts_monthly = unique_receipts_post_covid.groupby(['month_year'])['receiptdate'].agg('count').reset_index()\n",
    "\n",
    "unique_receipts_post_covid = unique_receipts_post_covid.groupby(['receiptdate'])['unique_key'].agg('count').reset_index()\n",
    "\n",
    "#count_unique_receipts_monthly = unique_receipts_post_covid.groupby(['year', 'month_year'])['uid'].agg('count').reset_index()\n",
    "\n",
    "#count_unique_receipts_monthly = unique_receipts_post_covid.groupby(['month_year'])['receiptdate'].nunique().reset_index(name='count')\n",
    "\n",
    "# Calculating the overall percentage for each donor and adding a new column\n",
    "#count_unique_receipts_daily['Overall_Percentage'] = count_unique_receipts_daily.iloc[:, 1:].mean(axis=1)\n",
    "\n",
    "\n",
    "\n",
    "count_unique_receipts_daily = unique_receipts_post_covid.rename(columns={'unique_key': 'daily_count'})\n",
    "\n",
    "count_unique_receipts_daily['avg_daily_count'] = count_unique_receipts_daily['daily_count'].mean()\n",
    "\n",
    "\n",
    "# Extract month letter and year \n",
    "###lpa_reciepts['month_year'] = lpa_reciepts['receiptdate'].dt.strftime('%b-%y')\n",
    "# Calculate the number of unique records by month and year\n",
    "#count_reciepts_month = lpa_reciepts.groupby(['year', 'month_year'])['receiptdate'].nunique().reset_index(name='count')\n",
    "###Count_daily_reciepts = lpa_reciepts.groupby(['receiptdate']).count()\n",
    "\n",
    "#unique_records = df.groupby('unique_key').agg('count').reset_index()  #.groupby(['year'])['unique_key'].nunique().reset_index(name='count')\n",
    "###Count_daily_reciepts = Count_daily_reciepts.rename(columns={\"count\": \"Count_of_daily_reciepts\"})\n",
    "\n",
    "#\n",
    "\n",
    "\n",
    "# Extract month letter and year \n",
    "count_unique_receipts_daily['month_year'] = count_unique_receipts_daily['receiptdate'].dt.strftime('%b-%y')\n",
    "count_unique_receipts_daily['year'] = count_unique_receipts_daily['receiptdate'].dt.strftime('%Y')\n",
    "print(count_unique_receipts_daily)\n",
    "\n",
    "count_unique_receipts_monthly = count_unique_receipts_daily.groupby(['year', 'month_year'], as_index=False).mean()\n",
    "count_unique_receipts_monthly = count_unique_receipts_monthly.rename(columns={'daily_count': 'avg_monthly_count'})\n",
    "\n",
    "# Display the result\n",
    "print(count_unique_receipts_monthly)\n",
    "\n",
    "count_unique_receipts_annual = count_unique_receipts_monthly.groupby(['year'], as_index=False).mean()\n",
    "count_unique_receipts_annual = count_unique_receipts_annual.rename(columns={'avg_monthly_count': 'avg_annual_count'})\n",
    "\n",
    "# Display the result\n",
    "print(count_unique_receipts_annual)\n",
    "\n",
    "#count_unique_receipts_daily['Overall_Percentage'] = count_unique_receipts_daily.iloc[:, 1].mean(axis=1)\n",
    "\n",
    "# Save the result into a csv file\n",
    "#lpa_reciepts.to_csv(r'lpa_reciepts.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b17122b-d651-4845-84c6-fc191e153db2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "f4441107-af2e-41fc-b26e-d97bebbefda1",
   "metadata": {},
   "source": [
    "# Work out how many people applied for lpa and recieved the power of atthorney and how many applications in a year/month/week by age group since 2007?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d91646b3-755c-405d-9cf0-ffbe25a26fee",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1ce0cc6-03b8-46d6-b28c-ce8a13503efc",
   "metadata": {},
   "outputs": [],
   "source": [
    "d = lpa_unique.sort_values(by='unique_key')\n",
    "count_unique_grouped_year = d.groupby(['year'])['unique_key'].nunique().reset_index(name='count')\n",
    "count_unique_grouped_year"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b12b129-dea6-44e6-9dc3-8052e77a37f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "d = lpa_unique.sort_values(by='unique_key')\n",
    "count_unique_grouped_age = d.groupby(['age'])['unique_key'].nunique().reset_index(name='count')\n",
    "count_unique_grouped_age"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33b3f949-855d-4ed5-aa5b-c5aa170cd6f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "g = lpa_unique\n",
    "# Group by 'item' and 'color', then count the occurrences\n",
    "count_unique_grouped_all = g.groupby(['receiptdate', 'uid', 'casesubtype', 'status', 'donor_dob', 'donor_postcode', 'donor_gender', 'age', 'year'])['unique_key'].count().reset_index(name='count')\n",
    "count_unique_grouped_all"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c18be8a5-3357-4f91-9d26-af397dabcc02",
   "metadata": {},
   "outputs": [],
   "source": [
    "g = lpa_unique\n",
    "# Group by 'item' and 'color', then count the occurrences\n",
    "count_unique_grouped_age_year = g.groupby(['age', 'year'])['unique_key'].count().reset_index(name='count')\n",
    "count_unique_grouped_age_year = count_unique_grouped_age_year.rename(columns={\"count\": \"Count_of_CASEID\"})\n",
    "count_unique_grouped_age_year\n",
    "\n",
    "# Save the result into a csv file\n",
    "count_unique_grouped_age_year.to_csv(r'count_unique_grouped_age_year.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d3301ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "d = lpa_unique.sort_values(by='unique_key')\n",
    "count_unique_grouped_year = d.groupby(['year'])['unique_key'].nunique().reset_index(name='count')\n",
    "count_unique_grouped_yearcount_unique_grouped_age_year.to_csv(r'count_unique_grouped_age_year.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8fda9dcf",
   "metadata": {},
   "outputs": [],
   "source": [
    "d = lpa_unique.sort_values(by='unique_key')\n",
    "count_unique_grouped_age = d.groupby(['age'])['unique_key'].nunique().reset_index(name='count')\n",
    "count_unique_grouped_age"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ecbaf322",
   "metadata": {},
   "outputs": [],
   "source": [
    "g = lpa_unique\n",
    "# Group by 'item' and 'color', then count the occurrences\n",
    "count_unique_grouped_all = g.groupby(['receiptdate', 'uid', 'casesubtype', 'status', 'donor_dob', 'donor_postcode', 'donor_gender', 'age', 'year'])['unique_key'].count().reset_index(name='count')\n",
    "count_unique_grouped_all"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09307402",
   "metadata": {},
   "outputs": [],
   "source": [
    "g = lpa_unique\n",
    "# Group by 'item' and 'color', then count the occurrences\n",
    "count_unique_grouped_age_year = g.groupby(['age', 'year'])['unique_key'].count().reset_index(name='count')\n",
    "count_unique_grouped_age_year = count_unique_grouped_age_year.rename(columns={\"count\": \"Count_of_CASEID\"})\n",
    "count_unique_grouped_age_year\n",
    "\n",
    "# Save the result into a csv file\n",
    "count_unique_grouped_age_year.to_csv(r'count_unique_grouped_age_year.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5baac2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract the year of reciept from the receiptdate\n",
    "#receipt_year = int(record[\"receiptdate\"].split(\"-\")[0])\n",
    "\n",
    "# Calculate the number of unique records by age and year\n",
    "count_unique_records = lpa_data_no_index.groupby(\n",
    "    ['year', 'donor_gender', 'age'])['unique_key'].nunique().reset_index(\n",
    "    name='count')\n",
    "#####count_unique_records = lpa_data.reset_index(name='count')\n",
    "#####count_unique_records = lpa_data.reset_index(name='count')\n",
    "\n",
    "\n",
    "#unique_records = df.groupby('unique_key').agg('count').reset_index()  #.groupby(['year'])['unique_key'].nunique().reset_index(name='count')\n",
    "####count_unique_records = count_unique_records.rename(columns={\"count\": \"Count_of_CASEID\"})\n",
    "\n",
    "# Display the result\n",
    "####print(count_unique_records)\n",
    "\n",
    "# Save the result into a csv file\n",
    "#count_unique_records.to_csv(r'count_unique_records.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a6408eb-1305-4ed9-8701-40a6249f2883",
   "metadata": {},
   "source": [
    "# Dermine Whether the application type [casesubtype] is hw=health and welfare or pfa=property and finance"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "066db357-3848-4fdc-86fe-12294383e61a",
   "metadata": {},
   "source": [
    "# How many certificate provider (cp) for each lpa application?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06ebb3ab-4ae6-405f-95de-6c7b259c344a",
   "metadata": {},
   "source": [
    "# Location based data and geographical data for the donor can be used to identify the financial situation and wherether they are located in England or Wales"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6889b3cc-d296-4eed-8270-17ca0c3f9d4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract the year from the receiptdate\n",
    "#receipt_year = birth_year        \n",
    "\n",
    "# Calculate the age of the person\n",
    "#age = receiptdate - birth_year\n",
    "#lpa_df['a'] = \n",
    "############(pd.to_datetime(lpa_df['receiptdate'], errors = 'coerce').dt.day - pd.to_datetime(lpa_df['donor_dob'], errors = 'coerce').dt.day) # / 365.25\n",
    "#lpa_df\n",
    "\n",
    "# Create an Excel writer\n",
    "writer = pd.ExcelWriter('LPA_Data_actuals_Years.xlsx', engine='xlsxwriter')\n",
    "\n",
    "# Iterate through unique years and save data to separate sheets\n",
    "for year in count_unique_grouped_age_year['year'].unique():\n",
    "    year_data = count_unique_grouped_age_year[count_unique_grouped_age_year['year'] == year]\n",
    "    year_data.to_excel(writer, sheet_name=str(year), index=False)\n",
    "\n",
    "\n",
    "# Save the Excel file\n",
    "writer.save()\n",
    "writer.close()  # Close the ExcelWriter\n",
    "\n",
    "year_data\n",
    "# # Iterate through unique years and save data to separate sheets\n",
    "# for year in lpa_df['year'].unique():\n",
    "#     year_data = lpa_df[lpa_df['year'] == year]\n",
    "#     chunk_size = 100000  # Adjust as needed\n",
    "#     num_chunks = len(year_data) // chunk_size + 1\n",
    "#     for i in range(num_chunks):\n",
    "#         start_idx = i * chunk_size\n",
    "#         end_idx = (i + 1) * chunk_size\n",
    "#         chunk_data = lpa_df.iloc[start_idx:end_idx]\n",
    "#         chunk_data.to_excel(writer, sheet_name=f'Sheet{i}', index=False)\n",
    "\n",
    "# # Save the Excel file\n",
    "# writer.save()\n",
    "# writer.close()  # Close the ExcelWriter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1881181d-7089-429b-8237-005d6314f8bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pivot table with count aggregation\n",
    "pivot_table = pd.pivot_table(lpa_unique,\n",
    "                              values='unique_key',\n",
    "                              index='age',\n",
    "                              columns='year',\n",
    "                              aggfunc='count')\n",
    "\n",
    "# Replace NaN values with zeros\n",
    "pivot_table_filled = pivot_table.fillna(0)\n",
    "\n",
    "print(pivot_table_filled)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce3a3741-b69e-48c3-b9b6-94fc816c8de1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the result into a csv file\n",
    "pivot_table_filled.to_csv(r'count_unique_records.csv')\n",
    "\n",
    "# Define the source path of the CSV file (assuming it's in the current directory)\n",
    "source_csv_path = \"count_unique_records.csv\"\n",
    "\n",
    "# Define the target directory where the CSV file should be placed\n",
    "target_directory = \"csv_files\"\n",
    "\n",
    "# Create the target directory if it doesn't exist\n",
    "if not os.path.exists(target_directory):\n",
    "    os.makedirs(target_directory)\n",
    "\n",
    "# Move the CSV file to the target directory\n",
    "shutil.move(source_csv_path, os.path.join(target_directory, \"count_unique_records.csv\"))\n",
    "\n",
    "# Print a success message\n",
    "print(f\"The CSV file {source_csv_path} was successfully moved to {target_directory}.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2b3cbec-4340-4428-a247-59caa72e9f4c",
   "metadata": {},
   "source": [
    "# Using Bayesian criteria in Monte Carlo simulation to incorporate uncertainty in time series forecasting for long term demands considering the post-COVID impacts?\n",
    "- Incorporating Bayesian criteria into Monte Carlo simulation for time series forecasting allows you to account for uncertainty and make more robust predictions, especially in scenarios with significant disruptions like post-COVID impacts.\n",
    "\n",
    "## Steps to Implement Bayesian Monte Carlo Simulation for Time Series Forecasting\n",
    "1. Model Selection and Prior Specification:\n",
    "    - Select a Time Series Model: Common models include ARIMA, SARIMA, Exponential Smoothing (ETS), and more complex models like Bayesian Structural Time Series (BSTS).\n",
    "    - Specify Priors: Define prior distributions for your model parameters. Priors should reflect your beliefs about the parameters before observing the data. For post-COVID impacts, priors might be wider to reflect increased uncertainty.\n",
    "\n",
    "2. Fit the Model Using Bayesian Inference:\n",
    "    - Use Bayesian inference methods (e.g., Markov Chain Monte Carlo (MCMC)) to fit the model to historical data. This step updates your prior beliefs with observed data to obtain posterior distributions of the parameters.\n",
    "    - Tools like PyMC3, Stan, or TensorFlow Probability can be used for this purpose.\n",
    "\n",
    "3. Simulate Future Demand with Monte Carlo Simulation:\n",
    "    - Draw samples from the posterior distributions obtained in the previous step.\n",
    "    - For each sample, simulate future demand paths using the time series model. This involves generating many possible future scenarios based on the parameter samples.\n",
    "\n",
    "4. Incorporate Post-COVID Impacts:\n",
    "    - Adjust the model or priors to account for structural breaks or regime changes caused by COVID-19. This can involve incorporating covariates that capture post-COVID effects or using models that allow for sudden changes in trends and seasonality.\n",
    "\n",
    "5. Aggregate and Analyse Results:\n",
    "    - Collect the simulated future paths and analyze the distribution of forecasts. This gives you a probabilistic forecast that includes uncertainty.\n",
    "    - Key statistics such as mean, median, and credible intervals (e.g., 95% CI) can be derived from the simulated distributions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bfe531ba-bab8-4fc6-afc6-d76d15fd06ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pymc3 as pm #  specify priors and likelihood for the ARIMA model.\n",
    "import arviz as az\n",
    "\n",
    "# Generate some sample time series data\n",
    "np.random.seed(42)\n",
    "n = 100\n",
    "data = np.cumsum(np.random.randn(n))  # Random walk data\n",
    "\n",
    "# Specify the ARIMA model with Bayesian inference using PyMC3\n",
    "with pm.Model() as model:\n",
    "    # Priors for ARIMA(1,1,1) parameters\n",
    "    phi = pm.Normal('phi', mu=0, sigma=1)\n",
    "    theta = pm.Normal('theta', mu=0, sigma=1)\n",
    "    sigma = pm.HalfNormal('sigma', sigma=1)\n",
    "    \n",
    "    # Likelihood\n",
    "    y = pm.ARIMA('y', phi=phi, theta=theta, sigma=sigma, observed=data, order=(1, 1, 1))\n",
    "\n",
    "    # Posterior sampling\n",
    "    trace = pm.sample(1000, tune=1000, return_inferencedata=True)\n",
    "\n",
    "# Inspect the trace\n",
    "az.plot_trace(trace)\n",
    "plt.show()\n",
    "\n",
    "# Posterior predictive checks\n",
    "with model:\n",
    "    ppc = pm.sample_posterior_predictive(trace, var_names=['y'], samples=500)\n",
    "\n",
    "# Simulate future demands\n",
    "n_future = 20\n",
    "future_simulations = []\n",
    "\n",
    "## Simulate multiple future paths using the posterior samples.\n",
    "for sample in ppc['y']:\n",
    "    future_path = np.concatenate([data, np.zeros(n_future)])\n",
    "    for t in range(n, n + n_future):\n",
    "        future_path[t] = sample[t - n] + future_path[t - 1]\n",
    "    future_simulations.append(future_path[-n_future:])\n",
    "\n",
    "future_simulations = np.array(future_simulations)\n",
    "\n",
    "# Analyse future simulations\n",
    "mean_forecast = np.mean(future_simulations, axis=0)\n",
    "cred_interval = np.percentile(future_simulations, [2.5, 97.5], axis=0)\n",
    "\n",
    "# Plot the results\n",
    "## Calculate mean forecasts and credible intervals.\n",
    "## Plot historical data, mean forecast, and credible intervals to visualize the uncertainty.\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(data, label='Historical Data')\n",
    "plt.plot(range(n, n + n_future), mean_forecast, label='Mean Forecast')\n",
    "plt.fill_between(range(n, n + n_future), cred_interval[0], cred_interval[1], color='gray', alpha=0.5, label='95% Credible Interval')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f329ab83-fa45-4e47-a128-6999e6660afe",
   "metadata": {},
   "source": [
    "# Incorporating Post-COVID Impacts:\n",
    "\n",
    "## Structural Breaks: \n",
    "- Include indicators for pre- and post-COVID periods.\n",
    "- Data Generation: Simulate time series data with a clear structural break.\n",
    "- Model Specification: Define a Bayesian model with different means and variances before and after the break point.\n",
    "- Posterior Sampling: Use PyMC3 to sample from the posterior distribution of the parameters.\n",
    "- Posterior Predictive Checks: Generate future demand paths and analyze them.\n",
    "\n",
    "## Covariates: \n",
    "- Add covariates that capture post-COVID effects (e.g., economic indicators, policy changes).\n",
    "- This approach allows you to incorporate uncertainty due to post-COVID impacts and provides a more realistic forecast of long-term demands.\n",
    "\n",
    "## Regime Switching Models: \n",
    "- Use models that allow for regime changes, such as Bayesian Structural Time Series (BSTS) or state-space models.\n",
    "- Data Generation: Simulate time series data that switches regimes at a specific point.\n",
    "- Model Specification: Define a Bayesian model with different parameters for each regime.\n",
    "- Posterior Sampling: Use PyMC3 to sample from the posterior distribution of the parameters.\n",
    "- Posterior Predictive Checks: Generate future demand paths and analyze them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59b83c50-9536-4c6a-a547-a0a94bf5c01a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Covariate for post-COVID impact\n",
    "## to incorporate uncertainty due to post-COVID impacts and provides a more realistic forecast of long-term demands.\n",
    "with pm.Model() as model:\n",
    "    # Priors\n",
    "    phi = pm.Normal('phi', mu=0, sigma=1)\n",
    "    theta = pm.Normal('theta', mu=0, sigma=1)\n",
    "    sigma = pm.HalfNormal('sigma', sigma=1)\n",
    "    \n",
    "    # Covariate for post-COVID period\n",
    "    post_covid = np.arange(n) >= 60  # Assuming the break point is at time index 60\n",
    "    beta = pm.Normal('beta', mu=0, sigma=1)\n",
    "    \n",
    "    # Likelihood\n",
    "    y = pm.ARIMA('y', phi=phi, theta=theta, sigma=sigma, observed=data + beta * post_covid, order=(1, 1, 1))\n",
    "\n",
    "    # Posterior sampling\n",
    "    trace = pm.sample(1000, tune=1000, return_inferencedata=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa93a9c6-7188-427c-9e7d-5e66ce454af4",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(42)\n",
    "n = 100\n",
    "data_pre_covid = np.cumsum(np.random.randn(n // 2))  # Random walk pre-COVID\n",
    "data_post_covid = np.cumsum(np.random.randn(n // 2) * 2) + data_pre_covid[-1]  # Higher variance post-COVID\n",
    "data = np.concatenate([data_pre_covid, data_post_covid])\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8c54907-be1f-4f04-b47a-691708170133",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Structural Break Model\n",
    "## to account for sudden changes in the behavior of a time series. Here, we assume a break point where the post-COVID impact starts.\n",
    "\n",
    "# Generate some sample time series data with a structural break\n",
    "np.random.seed(42)\n",
    "n = 100\n",
    "data_pre_covid = np.cumsum(np.random.randn(n // 2))  # Random walk pre-COVID\n",
    "data_post_covid = np.cumsum(np.random.randn(n // 2) * 2) + data_pre_covid[-1]  # Higher variance post-COVID\n",
    "data = np.concatenate([data_pre_covid, data_post_covid])\n",
    "\n",
    "# Specify a structural break model using PyMC3\n",
    "with pm.Model() as model:\n",
    "    # Priors for parameters before and after the structural break\n",
    "    mu_pre = pm.Normal('mu_pre', mu=0, sigma=10)\n",
    "    mu_post = pm.Normal('mu_post', mu=0, sigma=10)\n",
    "    sigma_pre = pm.HalfNormal('sigma_pre', sigma=10)\n",
    "    sigma_post = pm.HalfNormal('sigma_post', sigma=10)\n",
    "    \n",
    "    # Indicator for structural break\n",
    "    switchpoint = n // 2\n",
    "    idx = np.arange(n)\n",
    "    \n",
    "    # Likelihood\n",
    "    mu = pm.math.switch(idx < switchpoint, mu_pre, mu_post)\n",
    "    sigma = pm.math.switch(idx < switchpoint, sigma_pre, sigma_post)\n",
    "    \n",
    "    y = pm.Normal('y', mu=mu, sigma=sigma, observed=data)\n",
    "\n",
    "    # Posterior sampling\n",
    "    trace = pm.sample(1000, tune=1000, return_inferencedata=True)\n",
    "\n",
    "# Inspect the trace\n",
    "az.plot_trace(trace)\n",
    "plt.show()\n",
    "\n",
    "# Posterior predictive checks\n",
    "with model:\n",
    "    ppc = pm.sample_posterior_predictive(trace, var_names=['y'], samples=500)\n",
    "\n",
    "# Simulate future demands\n",
    "n_future = 20\n",
    "future_simulations = []\n",
    "\n",
    "for sample in ppc['y']:\n",
    "    future_path = np.concatenate([data, np.zeros(n_future)])\n",
    "    for t in range(n, n + n_future):\n",
    "        future_path[t] = sample[t - n] + future_path[t - 1]\n",
    "    future_simulations.append(future_path[-n_future:])\n",
    "\n",
    "future_simulations = np.array(future_simulations)\n",
    "\n",
    "# Analyze future simulations\n",
    "mean_forecast = np.mean(future_simulations, axis=0)\n",
    "cred_interval = np.percentile(future_simulations, [2.5, 97.5], axis=0)\n",
    "\n",
    "# Plot the results\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(data, label='Historical Data')\n",
    "plt.plot(range(n, n + n_future), mean_forecast, label='Mean Forecast')\n",
    "plt.fill_between(range(n, n + n_future), cred_interval[0], cred_interval[1], color='gray', alpha=0.5, label='95% Credible Interval')\n",
    "plt.axvline(switchpoint, color='r', linestyle='--', label='Structural Break')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10c17653-4b19-4eaf-b355-c100344839b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Regime Switching Model\n",
    "## allow the time series to switch between different states or regimes, which can capture changes in behavior such as those caused by COVID-19.\n",
    "import pymc3 as pm\n",
    "import theano.tensor as tt\n",
    "import arviz as az\n",
    "\n",
    "# Generate some sample time series data with regime switching\n",
    "np.random.seed(42)\n",
    "n = 100\n",
    "data = np.zeros(n)\n",
    "regime = np.zeros(n, dtype=int)\n",
    "\n",
    "# Simulate data with regime switching\n",
    "for t in range(1, n):\n",
    "    if t > n // 2:\n",
    "        regime[t] = 1  # Switch regime at midpoint\n",
    "        data[t] = data[t - 1] + np.random.randn() * 2  # Higher variance in second regime\n",
    "    else:\n",
    "        data[t] = data[t - 1] + np.random.randn()\n",
    "\n",
    "# Specify a regime switching model using PyMC3\n",
    "with pm.Model() as model:\n",
    "    # Priors for regime 1 and regime 2 parameters\n",
    "    mu_1 = pm.Normal('mu_1', mu=0, sigma=10)\n",
    "    mu_2 = pm.Normal('mu_2', mu=0, sigma=10)\n",
    "    sigma_1 = pm.HalfNormal('sigma_1', sigma=10)\n",
    "    sigma_2 = pm.HalfNormal('sigma_2', sigma=10)\n",
    "    \n",
    "    # Regime indicator\n",
    "    state = pm.math.switch(tt.arange(n) < n // 2, 0, 1)\n",
    "    \n",
    "    # Likelihood\n",
    "    mu = pm.math.switch(state, mu_2, mu_1)\n",
    "    sigma = pm.math.switch(state, sigma_2, sigma_1)\n",
    "    \n",
    "    y = pm.Normal('y', mu=mu, sigma=sigma, observed=data)\n",
    "\n",
    "    # Posterior sampling\n",
    "    trace = pm.sample(1000, tune=1000, return_inferencedata=True)\n",
    "\n",
    "# Inspect the trace\n",
    "az.plot_trace(trace)\n",
    "plt.show()\n",
    "\n",
    "# Posterior predictive checks\n",
    "with model:\n",
    "    ppc = pm.sample_posterior_predictive(trace, var_names=['y'], samples=500)\n",
    "\n",
    "# Simulate future demands\n",
    "n_future = 20\n",
    "future_simulations = []\n",
    "\n",
    "for sample in ppc['y']:\n",
    "    future_path = np.concatenate([data, np.zeros(n_future)])\n",
    "    for t in range(n, n + n_future):\n",
    "        future_path[t] = sample[t - n] + future_path[t - 1]\n",
    "    future_simulations.append(future_path[-n_future:])\n",
    "\n",
    "future_simulations = np.array(future_simulations)\n",
    "\n",
    "# Analyze future simulations\n",
    "mean_forecast = np.mean(future_simulations, axis=0)\n",
    "cred_interval = np.percentile(future_simulations, [2.5, 97.5], axis=0)\n",
    "\n",
    "# Plot the results\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(data, label='Historical Data')\n",
    "plt.plot(range(n, n + n_future), mean_forecast, label='Mean Forecast')\n",
    "plt.fill_between(range(n, n + n_future), cred_interval[0], cred_interval[1], color='gray', alpha=0.5, label='95% Credible Interval')\n",
    "plt.axvline(n // 2, color='r', linestyle='--', label='Regime Switch')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77c7b3db-0582-4c65-8ca5-f682932c572d",
   "metadata": {},
   "source": [
    "# Mortality Statistics\n",
    "## Source Data For Mortality Statistics and Modelled Age Specific Survival Rates (Model Input Set By Control Assumptions)\n",
    "\n",
    "# LPA Control Assumptions\n",
    "## Specific Key Assumptions that control expected demand , LPA market size and saturation.\n",
    "\n",
    "\n",
    "# Meta data and Variable selection and Data Cleaning for the Mortality statastics data based on population projections:\n",
    "\n",
    "## Goal: \n",
    "### What proportion of the UK population are likely to buy LPA and still alive?\n",
    "*How many people are still alive (Living Donors bought LPA)*\n",
    "*Based on ONS Data of Population of Engalnd and Wales, how many people are still alive and how many of them are dead?*\n",
    "*e.g., if there are 1000 people and 100 of them are still alive and bought LPA,\n",
    "so there are 900 of them still didn't buy LPA.\n",
    "\n",
    "\n",
    "\n",
    "**1. These rates are standardised to the 2013 European Standard Population, expressed per million population; \n",
    "they allow comparisons between populations with different age structures, including between males and females and over time. \n",
    "**2.  Deaths per 1,000 live births. \n",
    "**3.  Death figures are based on deaths registered rather than deaths occurring in a calendar year.\n",
    "\n",
    "### For information on registration delays for a range of causes, see: \n",
    "    https://webarchive.nationalarchives.gov.uk/ukgwa/20160106020016/http://www.ons.gov.uk/ons/guide-method/user-guidance/health-and-life-events/impact-of-registration-delays-on-mortality-statistics/index.html\n",
    "\n",
    "A limiting factor in modelling numbers of surving LPA holders aged 90+ has been the absence of single age specific mortality rates \n",
    "for this group. Estimates* suggested that previously applied mortality rates were too low increasing the apparent numbers of \n",
    "surviving LPA holder saged 90+ and therefore over-estimating the \"sauration of this market.\n",
    "\n",
    "For the 2018 LPA forecast , Age specific mortality rates for those aged 90+ have therefore been extrapolated based on \n",
    "a standard log power law that best fits existing mortality rates to age. \n",
    "\n",
    "*numbers of surviving LPA holders were estimated to exceed the total projected  population in each age group which was \n",
    "clearly not possible.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b287ad4-7294-4a75-a9e7-71d4f397b8da",
   "metadata": {},
   "source": [
    "# LPA SURVIVAL TABLES:\n",
    " LPA MODEL/LPA SURVIVAL TABLES\n",
    "percentage of people are died in one year\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8aa6b618-1161-4001-94c2-df536c5a7953",
   "metadata": {},
   "source": [
    "# if a 1000 40 years old male bought an LPA in 2008, what proportion of are still alove today?\n",
    "\n",
    "# The model taking each age categories (categorical variable) and assumed that they are \n",
    "# singe age-specifics in the age category 18 to 90 and provide figure what percentage of people for male died within one year?\n",
    "\n",
    "## e.g., in the 15-19 age category, 0.3 percent of males died within one year in the UK and 0.03 per 1000\n",
    "## e.g., in the 25-29 age category, 0.6 percent of males died within one year in the UK and 0.06 per 1000\n",
    "## e.g., in the 70-74 age category, 23.7 percent of males died within one year in the UK or 2.37 per 1000\n",
    "\n",
    "## if you started at age 18, 7 years and become 25 years old ahead, \n",
    "## as the ages goes up you will fall into a higher mortality category (from 0.3 to 0.6)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96ae6aa8-4340-45f6-8ba4-d231f3a29a5e",
   "metadata": {},
   "source": [
    "# calculate naïve extrapolation for demand forecasting and calculate low planning estimate, centeral planning estimate, and high planning estimate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4fd8723-2c72-44a2-8278-a3d1ddfe5907",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract the month of reciept from the receiptdate\n",
    "#lpa_data_no_index['month'] = lpa_data_no_index['receiptdate'].dt.month\n",
    "#lpa_data_no_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a60479e1-8b7e-4565-af0d-4f59f6b33678",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# Calculate the number of unique records by month and year\n",
    "count_unique_month = lpa_data.groupby(['year', 'month_year', 'age'])['uid'].nunique().reset_index(name='count')\n",
    "\n",
    "\n",
    "#unique_records = df.groupby('unique_key').agg('count').reset_index()  #.groupby(['year'])['unique_key'].nunique().reset_index(name='count')\n",
    "count_unique_month = count_unique_month.rename(columns={\"count\": \"Count_of_CASEID_month\"})\n",
    "\n",
    "# Display the result\n",
    "print(count_unique_month)\n",
    "\n",
    "# Save the result into a csv file\n",
    "#count_unique_month.to_csv(r'count_unique_month.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa18b9fd-73d7-46d1-9616-3cb0b6229292",
   "metadata": {},
   "outputs": [],
   "source": [
    "#df_count_unique_month = pd.DataFrame(count_unique_month)\n",
    "age_lower_limit = 50\n",
    "age_upper_limit = 70\n",
    "count_unique_month1 = count_unique_month.loc[(count_unique_month[\"age\"] >= age_lower_limit) &\n",
    "                 (count_unique_month[\"age\"] <= age_upper_limit)]\n",
    "\n",
    "#age_range = [50:70]\n",
    "#count_unique_month.loc[count_unique_month[\"month_year\"].isin(age_range)]\n",
    "\n",
    "count_unique_month2 = count_unique_month1[count_unique_month1['month_year'].isin(['Jan-24', 'Feb-24', 'Mar-24'])]\n",
    "#count_unique_month2 = count_unique_month1[count_unique_month1['month_year'].isin([50:70])]\n",
    "#df_count_unique_month['month_year'] = pd.to_datetime(df_count_unique_month['month_year'], format='%b-%y')\n",
    "#df_count_unique_month = df_count_unique_month.sort_values(df_count_unique_month.columns[1])\n",
    "#df_count_unique_month = df_count_unique_month.set_index(['month_year'])\n",
    "\n",
    "count_unique_month_age = count_unique_month2\n",
    "# Save the result into a csv file\n",
    "count_unique_month_age.to_csv(r'count_unique_month_age.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9323622a-f1c3-4cbf-a881-c56f20279905",
   "metadata": {},
   "outputs": [],
   "source": [
    "#df_count_unique_month = pd.DataFrame(count_unique_month)\n",
    "age_lower_limit = 50\n",
    "age_upper_limit = 70\n",
    "count_unique_month1 = count_unique_month.loc[(count_unique_month[\"age\"] >= age_lower_limit) &\n",
    "                 (count_unique_month[\"age\"] <= age_upper_limit)]\n",
    "\n",
    "#age_range = [50:70]\n",
    "#count_unique_month.loc[count_unique_month[\"month_year\"].isin(age_range)]\n",
    "\n",
    "count_unique_month2 = count_unique_month1[count_unique_month1['month_year'].isin(['Jan-24', 'Feb-24', 'Mar-24'])]\n",
    "#count_unique_month2 = count_unique_month1[count_unique_month1['month_year'].isin([50:70])]\n",
    "#df_count_unique_month['month_year'] = pd.to_datetime(df_count_unique_month['month_year'], format='%b-%y')\n",
    "#df_count_unique_month = df_count_unique_month.sort_values(df_count_unique_month.columns[1])\n",
    "#df_count_unique_month = df_count_unique_month.set_index(['month_year'])\n",
    "\n",
    "count_unique_month_age = count_unique_month2\n",
    "# Save the result into a csv file\n",
    "count_unique_month_age.to_csv(r'count_unique_month_age.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7546fbde-74e9-40f6-b03b-fea16e93e1af",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# Calculate the naïve forecast (previous month's sales)\n",
    "count_unique_month_age['Naive_Forecast'] = \n",
    "count_unique_month_age['Count_of_CASEID_month'].shift(1)\n",
    "\n",
    "# Define planning estimate factors\n",
    "low_factor = 0.9\n",
    "high_factor = 1.1\n",
    "\n",
    "# Calculate planning estimates\n",
    "count_unique_month_age['Low_Planning_Estimate'] = \n",
    "count_unique_month_age['Naive_Forecast'] * low_factor\n",
    "\n",
    "count_unique_month_age['Central_Planning_Estimate'] = \n",
    "count_unique_month_age['Naive_Forecast']\n",
    "\n",
    "count_unique_month_age['High_Planning_Estimate'] = \n",
    "count_unique_month_age['Naive_Forecast'] * high_factor\n",
    "\n",
    "# Calculate MAPE (Mean Absolute Percentage Error)\n",
    "count_unique_month_age['Absolute_Percentage_Error'] = \n",
    "abs(count_unique_month_age['Count_of_CASEID_month'] -\n",
    "    count_unique_month_age['Naive_Forecast']) / \n",
    "count_unique_month_age['Count_of_CASEID_month']\n",
    "\n",
    "mape = count_unique_month_age['Absolute_Percentage_Error'].mean() * 100\n",
    "\n",
    "# Calculate MAD (Mean Absolute Deviation)\n",
    "count_unique_month_age['Absolute_Deviation'] = abs(count_unique_month_age['Count_of_CASEID_month'] - count_unique_month_age['Naive_Forecast'])\n",
    "mad = count_unique_month_age['Absolute_Deviation'].mean()\n",
    "\n",
    "# Display results\n",
    "print(count_unique_month_age)\n",
    "print(f\"MAPE: {mape:.2f}%\")\n",
    "print(f\"MAD: {mad:.2f}\")\n",
    "print(\"\\nPlanning Estimates:\")\n",
    "print(f\"Low Planning Estimate: {count_unique_month_age['Low_Planning_Estimate'].iloc[-1]:.2f}\")\n",
    "print(f\"Central Planning Estimate: {count_unique_month_age['Central_Planning_Estimate'].iloc[-1]:.2f}\")\n",
    "print(f\"High Planning Estimate: {count_unique_month_age['High_Planning_Estimate'].iloc[-1]:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99ce5ae4-fa0c-499e-aaee-9c38e47bd090",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a line plot\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.plot(df_count_unique_month['Count_of_CASEID_month'], df_count_unique_month['Low_Planning_Estimate'], label='Low Estimate', marker='o')\n",
    "plt.plot(df_count_unique_month['Count_of_CASEID_month'], df_count_unique_month['Central_Planning_Estimate'], label='Central Estimate', marker='s')\n",
    "plt.plot(df_count_unique_month['Count_of_CASEID_month'], df_count_unique_month['High_Planning_Estimate'], label='High Estimate', marker='^')\n",
    "\n",
    "# Add labels and title\n",
    "plt.xlabel('Month')\n",
    "plt.ylabel('Sales Estimate')\n",
    "plt.title('Demand Forecasting Estimates')\n",
    "plt.grid(True)\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe089b8b-61ca-424d-b7dd-adb9092ca2b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Number of LPA reciepts\n",
    "lpa_reciepts = lpa_unique\n",
    "# Extract month letter and year \n",
    "lpa_reciepts['month_year'] = lpa_reciepts['receiptdate'].dt.strftime('%b-%y')\n",
    "# Calculate the number of unique records by month and year\n",
    "count_reciepts_month = lpa_reciepts.groupby(['year', 'month_year'])['receiptdate'].nunique().reset_index(name='count')\n",
    "\n",
    "\n",
    "#unique_records = df.groupby('unique_key').agg('count').reset_index()  #.groupby(['year'])['unique_key'].nunique().reset_index(name='count')\n",
    "count_reciepts_month = count_reciepts_month.rename(columns={\"count\": \"Count_of_reciepts_month\"})\n",
    "\n",
    "# Display the result\n",
    "print(count_reciepts_month)\n",
    "\n",
    "# Save the result into a csv file\n",
    "#count_reciepts_month.to_csv(r'count_reciepts_month.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8257dabd-234c-49eb-bd04-7e3157e87e35",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Number of LPA reciepts\n",
    "lpa_reciepts = lpa_data\n",
    "\n",
    "# Extract month letter and year \n",
    "lpa_reciepts['month_year'] = lpa_reciepts['receiptdate'].dt.strftime('%b-%y')\n",
    "\n",
    "# Calculate the number of unique records by month and year\n",
    "Count_of_reciepts_annual = lpa_reciepts.groupby(['year'])['receiptdate'].nunique().reset_index(name='count')\n",
    "\n",
    "\n",
    "#unique_records = df.groupby('unique_key').agg('count').reset_index()  #.groupby(['year'])['unique_key'].nunique().reset_index(name='count')\n",
    "Count_of_reciepts_annual = Count_of_reciepts_annual.rename(columns={\"count\": \"Count_of_reciepts_annual\"})\n",
    "\n",
    "\n",
    "# Display the result\n",
    "print(Count_of_reciepts_annual)\n",
    "\n",
    "# Save the result into a csv file\n",
    "####Count_of_reciepts_annual.to_csv(r'Count_of_reciepts_annual.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c1c8fe0-7a2d-4cd8-bccc-4b302daaaa1c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
