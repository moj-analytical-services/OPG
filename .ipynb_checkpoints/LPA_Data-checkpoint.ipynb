{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f9dce19a-0b8e-4890-a9d2-d22dc9a1aba4",
   "metadata": {},
   "source": [
    "# OPG: LPA Data Pre-processing, Cleaning, Manipulation, Analysis, and insight tool\n",
    "\n",
    "#==============================================================================\n",
    "# @author: Dr. Leila Yousefi \n",
    "# MoJ Modelling Hub\n",
    "#==============================================================================# OPG : LPA Data Pre-processing and Cleaning tool\n",
    "\n",
    "==============================================================================\n",
    "\n",
    " OPG Demand Forecast modelling for LPA\n",
    " \n",
    " @author: Leila Yousefi\n",
    " \n",
    " MoJ Modelling Hub\n",
    " \n",
    "============================================================================== "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7517bdf9-08ba-4cc1-8c2f-b01ffdf7971d",
   "metadata": {},
   "source": [
    "## Setup\n",
    "\n",
    "Before you can run this project, you need to install some Python packages using the terminal:\n",
    "\n",
    "\n",
    "### create and activate  a virtual environment\n",
    "1. cd OPG\n",
    "2. python3 -m venv venv\n",
    "3. source venv/bin/activate\n",
    "\n",
    "### install the python packages required\n",
    "4. pip install --upgrade pip\n",
    "5. pip install -r requirements.txt\n",
    "\n",
    "### Updating your branch with main\n",
    "When working on your models it is likely that your branch will get out of date with the main branch. To update you branch with the latest changes from main open a terminal and run the following:\n",
    "\n",
    "Check your working tree, commit/push any changes if required\n",
    "\n",
    "git status\n",
    "Switch to the main branch and collect the latest changes, if any\n",
    "\n",
    "git switch main\n",
    "git fetch\n",
    "git pull\n",
    "Switch back to your branch and merge in the changes from main\n",
    "\n",
    "git switch <your initial>/model-a-development\n",
    "git merge main -m \"update branch with main\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf48e8d4-54cd-4d6e-aa17-92621c1bef97",
   "metadata": {},
   "source": [
    "# Installing the required packages:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e765f151-ee2c-44b2-8fcd-0a438f11633c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Un-comment and Run the below code if there is an error with packages installation:\n",
    "\n",
    "!pip install pip\n",
    "!pip install arrow_pd_parser\n",
    "!pip install pydbtools\n",
    "!pip install xlsxwriter\n",
    "!pip install holidays\n",
    "#!pip install statsforecast\n",
    "##You can add lines to install the required packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de52a12d-a2de-47f7-bba8-0da5097863cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install panda update"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62438500-01c6-4434-89b1-37ebddfa540b",
   "metadata": {},
   "source": [
    "# Importing the required packages:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83bed50b-9b72-4d69-9326-fd1f690522fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import sys\n",
    "print(sys.path)\n",
    "\n",
    "import os.path\n",
    "from os import path\n",
    "os.getcwd()\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "#import awswrangler as wr\n",
    "#import statsmodels.api as sm\n",
    "#import tensorflow as tf\n",
    "import boto3\n",
    "import getpass\n",
    "import pytz\n",
    "#import openpyxl\n",
    "#import matplotlib\n",
    "import csv\n",
    "from arrow_pd_parser import reader, writer\n",
    "import shutil\n",
    "import pydbtools as pydb\n",
    "import datetime as dt\n",
    "from datetime import timedelta\n",
    "from datetime import datetime\n",
    "from datetime import date\n",
    "#import statsforecast\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "%matplotlib inline\n",
    "from dateutil.relativedelta import relativedelta\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from io import StringIO\n",
    "\n",
    "#from tensorflow import keras\n",
    "import matplotlib as mpl\n",
    "mpl.rcParams['agg.path.chunksize'] = 10000\n",
    "from matplotlib import rc\n",
    "import statsmodels.api as sm\n",
    "\n",
    "# consistent plot size wherever not specifiied\n",
    "from pylab import rcParams\n",
    "mpl.rcParams['figure.figsize'] = (15,8)\n",
    "mpl.rcParams['axes.grid'] = False\n",
    "rcParams['xtick.labelsize'] = 14\n",
    "rcParams['ytick.labelsize'] = 14\n",
    "rcParams['axes.labelsize'] = 14\n",
    "\n",
    "%matplotlib inline\n",
    "%config InlineBackend.figure_format='retina'\n",
    "\n",
    "import xlsxwriter\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib as mpl\n",
    "from matplotlib import rc\n",
    "import statsmodels.api as sm\n",
    "\n",
    "# consistent plot size wherever not specifiied\n",
    "from pylab import rcParams\n",
    "mpl.rcParams['figure.figsize'] = (15,8)\n",
    "mpl.rcParams['axes.grid'] = False\n",
    "rcParams['xtick.labelsize'] = 14\n",
    "rcParams['ytick.labelsize'] = 14\n",
    "rcParams['axes.labelsize'] = 14\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd602cf8-4885-43a5-82a9-429f8ce730b0",
   "metadata": {},
   "source": [
    "# LPA Data Import from Dom1\n",
    "\n",
    "This was used previously before transfering data to AWS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07194ab2-9571-406a-85f9-af8d699de142",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ## Enter the corresponding bucket name\n",
    "# bucketName = \"alpha-opg-analytical\"\n",
    "\n",
    "# ##For Automation import getpass\n",
    "# #bucketName = getpass.getpass()\n",
    "\n",
    "\n",
    "# ## Select the folder\n",
    "# folderPath = \"sirius_data_cuts_3\"\n",
    "\n",
    "\n",
    "# ## Set the folder in which the final output will be uploade to in S3\n",
    "# #output_path = f\"s3://alpha-opg-analytical/\" + folderPath + \"/\"\n",
    "\n",
    "# ## Then create a new excel file and copy the previous record from the S3 buckets and add the newly copied raws\n",
    "# ## Finaly convert the excel file to csv and upload it in the following path:\n",
    "# ## s3://alpha-opg-analytical/sirius_data_cuts_3/\n",
    "\n",
    "\n",
    "# ## Explore the s3 bucket path\n",
    "# path_s3 = f\"s3://{bucketName}/{folderPath}/*.csv\"\n",
    "# print ([path_s3])\n",
    "\n",
    "\n",
    "# ## Listing CSV Files in an S3 Bucket Folder: \n",
    "# ### To list all CSV files in a specific folder within an S3 bucket, we can use the AWS CLI or the boto3 Python library. \n",
    "# ###list all files in a specific folder within an S3 bucket Using AWS CLI:\n",
    "# #aws s3 ls s3://your-bucket-name/your-folder-name/ --recursive\n",
    "\n",
    "# ### lists all CSV files in a specific folder within an S3 bucket using boto3:\n",
    "# def list_csv_files(bucketName, folderPath):\n",
    "#     s3 = boto3.resource('s3')\n",
    "#     bucket = s3.Bucket(bucketName)\n",
    "#     for obj in bucket.objects.filter(Prefix=folderPath):\n",
    "#         if obj.key.endswith('.csv'):\n",
    "#             print(obj.key)\n",
    "\n",
    "\n",
    "# ## list all csv in the current folder in s3 bucket:\n",
    "# ### Getting the List of CSV Files\n",
    "# csv_files = list_csv_files(bucketName, folderPath)\n",
    "# print([csv_files])\n",
    "\n",
    "\n",
    "# ## Check if the path exists:\n",
    "# s3 = boto3.resource('s3')\n",
    "# bucket = s3.Bucket(bucketName)\n",
    "\n",
    "# def IsObjectExists(path):\n",
    "#     for object_summary in bucket.objects.filter(Prefix=path):\n",
    "#         return True\n",
    "#     return False\n",
    "\n",
    "# for fileName in [csv_files]:\n",
    "#     if(IsObjectExists(f\"{folderPath}/{fileName}\")):\n",
    "#         print(\"Path for the actual LPA data exists\")\n",
    "#     else:\n",
    "#         print(\"Path for the actual LPA data doesn't exists\")\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19756c99-6632-4f3f-b123-5f9394198786",
   "metadata": {},
   "source": [
    "# S3 Bucket Data Extraction for LPA Data (actuals)\n",
    "\n",
    "These will be used when extracting the raw data from the S3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abb24022-7229-4377-828f-44437c18cf9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download and upload the LPA actual data into the S3 bucket\n",
    "\n",
    "## Enter the corresponding S3 bucket name\n",
    "bucketName = \"alpha-opg-analytical\"\n",
    "\n",
    "##For Automation import getpass\n",
    "#bucketName = getpass.getpass()\n",
    "\n",
    "\n",
    "## Select the corresponding folder includes new LPA data in S3 bucket:\n",
    "folderPath = \"sirius_data_cuts_3\"\n",
    "\n",
    "\n",
    "## Set the folder in which the final output will be uploade to in S3\n",
    "#output_path = f\"s3://alpha-opg-analytical/\" + folderPath + \"/\"\n",
    "\n",
    "## Then create a new excel file and copy the previous record from the S3 buckets and add the newly copied raws\n",
    "## Finaly convert the excel file to csv and upload it in the following path:\n",
    "## s3://alpha-opg-analytical/sirius_data_cuts_3/\n",
    "\n",
    "# Initialize the S3 client\n",
    "s3_client = boto3.client('s3')\n",
    "\n",
    "# Specify your bucket name and folder (prefix)\n",
    "bucket_name = bucketName\n",
    "folder_prefix = 'sirius_data_cuts_3/'\n",
    "\n",
    "# List objects in the specified folder\n",
    "response = s3_client.list_objects_v2(Bucket=bucket_name, Prefix=folder_prefix)\n",
    "\n",
    "# Extract the keys (file names) from the response\n",
    "file_keys = [obj['Key'] for obj in response.get('Contents', [])]\n",
    "\n",
    "# Filter out None values (if any)\n",
    "non_none_file_keys = [key for key in file_keys if key is not None]\n",
    "#print(non_none_file_keys)\n",
    "\n",
    "# Remove folder prefix from file keys\n",
    "file_names = [os.path.basename(key) for key in non_none_file_keys]\n",
    "#print(file_names)\n",
    "\n",
    "csv_extension = '.csv'\n",
    "filtered_file_names = [fn for fn in file_names if fn.lower().endswith(csv_extension)]\n",
    "\n",
    "print(filtered_file_names)\n",
    "\n",
    "# ## Explore the s3 bucket path\n",
    "# path_s3 = f\"s3://{bucketName}/{folderPath}/\"\n",
    "# print ([path_s3])\n",
    "\n",
    "# ## Check if the path exists:\n",
    "# s3 = boto3.resource('s3')\n",
    "# bucket = s3.Bucket(bucketName)\n",
    "\n",
    "# def IsObjectExists(path):\n",
    "#     for object_summary in bucket.objects.filter(Prefix=path):\n",
    "#         return True\n",
    "#     return False\n",
    "\n",
    "# if(IsObjectExists(path_s3)):\n",
    "#     print(\"Path for the actual data exists\")\n",
    "# else:\n",
    "#     print(\"Path for the actual data doesn't exists\")\n",
    "\n",
    "\n",
    "## Listing CSV Files in an S3 Bucket Folder: \n",
    "### To list all CSV files in a specific folder within an S3 bucket, we can use the AWS CLI or the boto3 Python library. \n",
    "###list all files in a specific folder within an S3 bucket Using AWS CLI:\n",
    "#aws s3 ls s3://your-bucket-name/your-folder-name/ --recursive\n",
    "\n",
    "# ### lists all CSV files in a specific folder within an S3 bucket using boto3:\n",
    "# def list_csv_files(bucketName, folderPath):\n",
    "#     s3 = boto3.resource('s3')\n",
    "#     bucket = s3.Bucket(bucketName)\n",
    "#     for obj in bucket.objects.filter(Prefix=folderPath):\n",
    "#         if obj.key.endswith('.csv'):\n",
    "#             print(obj.key)\n",
    "\n",
    "\n",
    "# ## list all csv in the current folder in s3 bucket:\n",
    "# ### Getting the List of CSV Files\n",
    "# csv_files = list_csv_files(bucketName, folderPath)\n",
    "# print([csv_files])\n",
    "\n",
    "\n",
    "# ## Opening CSV Files Based on Selected Column and Condition: \n",
    "# def process_csv_file(file_path, selected_column, condition):\n",
    "#     df = pd.read_csv(file_path)\n",
    "#     filtered_df = df[df[selected_column] == condition]\n",
    "#     # Do further processing with the filtered data\n",
    "\n",
    "# ## pre-process the csv file to only show the required columns (important variable for the BAU):\n",
    "# process_csv_file('path/to/your-csv-file.csv', 'column_name', 'desired_value')\n",
    "\n",
    "\n",
    "# # ## Enter your file name\n",
    "# # fileName = \"d24_2.csv\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3556dfa4-cdcc-49b2-be16-811d5528b39c",
   "metadata": {},
   "source": [
    "# Query the warehouse tables directly from Python/R"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6db194d-65ed-489c-b1d5-1e4f13a3a7aa",
   "metadata": {},
   "source": [
    "\n",
    "    \"\"\"\n",
    "    with events as (\n",
    "        select *\n",
    "        from \"dim_guardianship_dev_dbt\".\"fct_case_receipts\"\n",
    "        where extract_type = 'latest_extract'\n",
    "            and receipt_date >= date_parse('01-01-2008', '%d-%m-%Y')\n",
    "    ),\n",
    "    dates as (\n",
    "        select *\n",
    "        from \"common_lookup_dev_dbt\".\"dim_date\"\n",
    "    ),\n",
    "    donors as (\n",
    "        select *\n",
    "        from \"dim_guardianship_dev_dbt\".\"dim_donors\"\n",
    "    ),\n",
    "    cases as (\n",
    "        select *\n",
    "        from \"dim_guardianship_dev_dbt\".\"dim_cases\"\n",
    "    ),\n",
    "    attributes as (\n",
    "        select dates.calendar_year as receipt_year,\n",
    "            events.receipt_date,\n",
    "            cases.case_id,\n",
    "            cases.case_type,\n",
    "            cases.case_subtype,\n",
    "            cases.case_status,\n",
    "            cases.donor_age_at_receipt,\n",
    "            donors.gender,\n",
    "            donors.region_name,\n",
    "            events.extract_date\n",
    "        from events\n",
    "            left join dates on events.receipt_date = dates.date_name\n",
    "            left join cases on events.extract_case_id = ces.extract_case_id\n",
    "            left join donors on events.extract_donor_id = donors.extract_donor_id\n",
    "    )\n",
    "    select *\n",
    "    from attributes\n",
    "    \"\"\"\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e82badb8-20ff-494f-8f18-ea433d17f6a0",
   "metadata": {},
   "source": [
    "# Reading in Data\n",
    "\n",
    "This extracts a list of Power of Attorney receipts with the following columns: ['receiptdate', 'uid', 'type', 'casesubtype', 'status', 'donor_postcode', 'donor_gender', 'age']."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92662597-d37b-417e-b335-5bfb590c8b39",
   "metadata": {},
   "outputs": [],
   "source": [
    "# read_csv_files function for Reading in CSV Files in an S3 Bucket Folder\n",
    "\n",
    "def read_csv_files(bucket_name, file_names, selected_columns):\n",
    "    \"\"\"\n",
    "        This function is written to read in the data from all of CSV files in the corresponding directory in the S3 bucket\n",
    "        by using input variables:\n",
    "        the S3 bucket name,\n",
    "        file_names \n",
    "        and the selected_columns \n",
    "        The output are the CSV files in the list of dataframes: dfs \n",
    "    \"\"\"\n",
    "    dfs = {}  # Dictionary to store DataFrames\n",
    "\n",
    "    # Initialize the S3 client\n",
    "    s3_client = boto3.client('s3')\n",
    "\n",
    "    for file_name in file_names:\n",
    "        s3_path = f's3://{bucket_name}/{file_name}'\n",
    "        try:\n",
    "            # Read the CSV data into a Pandas DataFrame\n",
    "            csv_obj = s3_client.get_object(Bucket=bucket_name, Key=f'{folderPath}/{file_name}')\n",
    "            csv_string = csv_obj['Body'].read().decode('utf-8')\n",
    "            df = pd.read_csv(StringIO(csv_string))\n",
    "\n",
    "            # Select specific columns\n",
    "            df_selected = df[selected_columns]\n",
    "            dfs[file_name] = df_selected\n",
    "        except Exception as e:\n",
    "            print(f\"Error reading {file_name}: {e}\")\n",
    "\n",
    "    return dfs\n",
    "\n",
    "\n",
    "#bucket_name = bucketName\n",
    "#file_names = ['file1.csv', 'file2.csv']  # Replace with your actual file names\n",
    "\n",
    "## Filter the required variables from the datafarame:\n",
    "selected_columns = [\"receiptdate\",\"cases_glueexporteddate\",\"uid\",\"type\",\"casesubtype\",\"status\",\"donor_dob\",\"donor_postcode\",\"donor_gender\"]  # Replace with desired column names\n",
    "\n",
    "## The read_csv_files function for Reading in CSV Files in an S3 Bucket Folder:\n",
    "dataframes = read_csv_files(bucket_name, filtered_file_names, selected_columns)\n",
    "\n",
    "## Access individual DataFrames by file name\n",
    "for file_name, df_selected in dataframes.items():\n",
    "    print(f\"DataFrame for {file_name}:\")\n",
    "    print(df_selected.head())\n",
    "    \n",
    "## Concatenating DataFrames: \n",
    "### After reading all CSV files, you can concatenate the DataFrames using pd.concat:\n",
    "combined_df = pd.concat(dataframes, ignore_index=True)\n",
    "print(combined_df)\n",
    "\n",
    "## Writing Back to S3: Finally, write the combined DataFrame back to S3:\n",
    "#combined_data_encoded = combined_df.to_csv(None, index=False).encode('utf-8')\n",
    "#combined_file_name = 'combined_data.csv'  # Choose a suitable file name\n",
    "#s3_client.put_object(Body=combined_data_encoded, Bucket=bucket_name, Key=combined_file_name\n",
    "\n",
    "## Identify the type of data set and pre-processing: \n",
    "## Import, manipulate, and clean the data and impute missing values\n",
    "\n",
    "## Column renaming:\n",
    "#df1.rename(columns={'old_col1': 'common_col1', 'old_col2': 'common_col2'}, inplace=True)\n",
    "\n",
    "## Handling Data Mismatch:\n",
    "###Be cautious when combining data with different structures. If a column has incompatible data types (e.g., mixing strings and numbers), you may need to convert or handle them appropriately.\n",
    "#combined_df['numeric_col'] = pd.to_numeric(combined_df['numeric_col'], errors='coerce')\n",
    "\n",
    "## Aggregating Data:\n",
    "###If the DataFrames have different structures, consider aggregating them based on a common identifier (e.g., date or unique ID).\n",
    "#combined_df = df1.groupby('product_id').sum()  # Aggregate by product ID\n",
    "\n",
    "## merge DataFrames based on a common identifier:\n",
    "#merged_df = pd.merge(df1, df2, on='ID', how='inner')\n",
    "#print(merged_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f991a0c6-dc43-4c09-83e6-773c016d3d7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#     ## Select Date\n",
    "#     start_date = '2018-06-01' # start date for the train set\n",
    "#     start_prediction ='2023-02-01' # The end date for the train set\n",
    "#     end_prediction ='2024-02-01' # test / Validation set\n",
    "\n",
    "\n",
    "\n",
    "## Import the dataset and read in the actual data\n",
    "#df = wr.s3.read_csv([path1_s3], sep = ',', parse_dates=True) #import divorce data\n",
    "#read data\n",
    "#def parser(s):\n",
    "#    return datetime.strptime(s, '%Y-%m-%d')\n",
    "#df = wr.s3.read_csv([path1_s3], parse_dates=[0], index_col=0, squeeze=True, date_parser=parser)\n",
    "## iterating the columns\n",
    "#for col in df.columns:\n",
    "#    print(col)\n",
    "\n",
    "\n",
    "#lpa=LPA_data[[\"receiptdate\",\"cases_glueexporteddate\",\"uid\",\"type\",\"casesubtype\",\"status\",\"donor_dob\",\"donor_postcode\",\"donor_gender\"]]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5268ef2b-4e72-4080-a918-e7bfddc40822",
   "metadata": {},
   "source": [
    "# Automating the input dates to forecast LPAs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93434d88-6814-4c5b-8d96-e33a7eaa70d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the date you want to extract data based on the latest date extrated LPA data\n",
    "\n",
    "## Grab part of filename\n",
    "fist_CSV_fileName = filtered_file_names[1]\n",
    "snapshot_end = fist_CSV_fileName.split('opg-analytical_cases_P')[1].lstrip().split('_S')[0]\n",
    "#snapshot_end\n",
    "\n",
    "#snapshot_end = final_df.values[7].astype(str)[7]\n",
    "\n",
    "## Automating the input dates to forecast\n",
    "p = getpass.getpass(prompt='Do you want to change the starting date for forecasting? (Choose Yes=Y OR No=N)')\n",
    " \n",
    "if (p.lower() == 'n') | (p.lower() == ''): #defult start date\n",
    "    snapshot_start = '2006-12-31'\n",
    "    print('You have not choosen to change the date, the default date is: ' + snapshot_start)\n",
    "    ## The first date to be considered:\n",
    "else:    \n",
    "    ## Select Date\n",
    "    print('You have choosen to change the date.')\n",
    "    snapshot_start = input('Enter the period_start date (for training): e.g., \"2006-12-31\"')\n",
    "    print('snapshot_start: ' + snapshot_start)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "197c09c2-54bc-42df-b55b-ea188f2babb5",
   "metadata": {},
   "source": [
    "# Data pre-processing and cleaning - data engineering"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2fc0e1c-b824-4822-a8e5-20041f465a65",
   "metadata": {},
   "source": [
    "## Meta data and Variable selection and Data Cleaning for the LPA data in Data Warehouse:\n",
    "\n",
    "Goal: to work out how many people applied for lpa and recieved the power of atthorney and how many applications in a year/month/week by age group since 2007? \n",
    "\n",
    "### ages over 19 years old\n",
    "\n",
    "#### Unique case reference for each donor = [donor_dob + donor_postcode + donor_gender]\n",
    "\n",
    "##### Sort by the unique id and count how many application\n",
    "\n",
    "###### and then dermine Whether the application type [casesubtype] is hw=health and welfare or pfa=property and finance\n",
    "\n",
    "###### how many certificate provider (cp) for each lpa application?\n",
    "\n",
    "###### Location based data and geographical data for the donor can be used to identify the financial situation and wherether they are located in England or Wales\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50385b9d-958f-4a48-958b-bee05777d4cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Filter the records:\n",
    "df_filtered = combined_df\n",
    "\n",
    "## Convert the receipt date to date format \n",
    "df_filtered['receiptdate'] = pd.to_datetime(df_filtered['receiptdate'], errors = 'coerce') #.dt.date\n",
    "\n",
    "## Filter records between the selected dates\n",
    "df_filtered = df_filtered.loc[(df_filtered['receiptdate'] > pd.to_datetime(snapshot_start))]\n",
    "df_filtered = df_filtered.loc[(df_filtered['receiptdate'] < pd.to_datetime(snapshot_end))]\n",
    "\n",
    "## Filter the dataframe to select only lpa type records\n",
    "df_filtered = df_filtered.loc[(df_filtered['type'] == 'lpa')]\n",
    "\n",
    "# Create a dataframe of the selected columns\n",
    "## Select the appropriate variable to be forecasted\n",
    "df = df_filtered[[\"receiptdate\",\"uid\",\"casesubtype\",\"status\",\"donor_dob\",\"donor_postcode\",\"donor_gender\"]]\n",
    "\n",
    "## Remove Null values and records\n",
    "lpa_df = df.dropna()\n",
    "\n",
    "# Extract age by subtracting 'receiptdate' and 'donor_dob'\n",
    "lpa_df['age'] = pd.to_datetime(lpa_df['receiptdate'], errors = 'coerce').dt.year - pd.to_datetime(lpa_df['donor_dob'], errors = 'coerce').dt.year\n",
    "#lpa_df['age'] = relativedelta(date, dob).years\n",
    "\n",
    "# Convert the donor_dob column to a datatime format\n",
    "lpa_df['donor_dob'] = pd.to_datetime(lpa_df['donor_dob'], errors = 'coerce').dt.date\n",
    "\n",
    "# Convert the ‘receiptdate’ column to datetime format for proper plotting.\n",
    "# Convert 'receiptdate' to datetime format \n",
    "lpa_df['receiptdate'] = pd.to_datetime(lpa_df['receiptdate'], errors='coerce')\n",
    "\n",
    "# Extract year from 'receiptdate'\n",
    "lpa_df['year'] = lpa_df['receiptdate'].dt.year\n",
    "\n",
    "## Set index\n",
    "#df['receiptdate'] = pd.to_datetime(df['receiptdate'])\n",
    "\n",
    "#df = df.set_index('receiptdate').asfreq('D')\n",
    "\n",
    "####df['receiptdate'] = df.set_index('receiptdate',inplace=True)\n",
    "\n",
    "#df.index = df.index.to_period('D')\n",
    "                            \n",
    "print(lpa_df.head())\n",
    "print(lpa_df.tail())\n",
    "\n",
    "\n",
    "#lpa_df['age'] = pd.to_datetime(df['receiptdate'], errors = 'coerce').dt.date - pd.to_datetime(df['donor_dob'], errors = 'coerce').dt.date\n",
    "#lpa_df['receiptdate'] = pd.to_datetime(lpa_df['receiptdate']).dt.date#.apply(lambda x: x.strftime('%Y-%m-%d'))\n",
    "#print(lpa_df)#['receiptdate']\n",
    "#lpa_df\n",
    "\n",
    "#print(lpa_df['age'])\n",
    "\n",
    "## infer the frequency of the data:\n",
    "###lpa_df = df\n",
    "\n",
    "#lpa_df = df.asfreq(pd.infer_freq(df.index))\n",
    "\n",
    "#lpa_df = lpa_df[start_date:end_date]\n",
    "\n",
    "#start_date_years = datetime.strptime(start_date, \n",
    "#                                     '%Y-%m-%d') + relativedelta(years = 0)\n",
    "#print(start_date_years)\n",
    "\n",
    "#start_date_formatted = start_date_years.date()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "015e48bf-b1c7-4d9c-8521-e4c8619c8327",
   "metadata": {},
   "source": [
    "# Visualisation of the time series\n",
    "## Virtualisation of the LPA Data:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "232c0736-d6df-4b90-9ad4-5f33b5f5bd1e",
   "metadata": {},
   "source": [
    "# Plot 'age' against 'receiptdate'\n",
    "\n",
    "## --------------------------------------------------------------------------------  ##\n",
    "# Create a scatter plot with ‘receiptdate’ as the x-axis and ‘age’ as the y-axis.\n",
    "# Display the plot with appropriate labels and a grid.\n",
    "\n",
    "plt.figure(figsize=(20, 10))\n",
    "plt.scatter(lpa_df['receiptdate'], lpa_df['age'], alpha=0.5)\n",
    "plt.title('Age vs Receipt Date')\n",
    "plt.xlabel('Receipt Date')\n",
    "plt.ylabel('Age')\n",
    "plt.grid(True)\n",
    "plt.show()\n",
    "\n",
    "## --------------------------------------------------------------------------------  ##\n",
    "# Create a histogram of the 'age' column\n",
    "\n",
    "# This code will produce a histogram that displays the frequency distribution of ages in your dataset. \n",
    "# The bins parameter determines the number of bins used in the histogram, and you can adjust this number\n",
    "# to change the granularity of your histogram.\n",
    "\n",
    "plt.figure(figsize=(20, 10))\n",
    "plt.hist(lpa_df['age'], bins=20, alpha=0.7, color='blue')\n",
    "plt.title('Age Distribution')\n",
    "plt.xlabel('Age')\n",
    "plt.ylabel('Frequency')\n",
    "plt.grid(True)\n",
    "plt.show()\n",
    "\n",
    "## --------------------------------------------------------------------------------  ##\n",
    "# Create a line chart of age against receipt date\n",
    "# Sort the DataFrame by 'receiptdate' to ensure the line chart is ordered\n",
    "lpa_df.sort_values('receiptdate', inplace=True)\n",
    "\n",
    "# Plot 'age' against 'receiptdate' using a line chart\n",
    "plt.figure(figsize=(20, 10))\n",
    "plt.plot(lpa_df['receiptdate'], lpa_df['age'], marker='o', linestyle='-', color='blue')\n",
    "plt.title('Age vs Receipt Date')\n",
    "plt.xlabel('Receipt Date')\n",
    "plt.ylabel('Age')\n",
    "plt.grid(True)\n",
    "\n",
    "## --------------------------------------------------------------------------------  ##\n",
    "# Produce a line chart that displays the average age of individuals for each year based on the receipt dates in your dataset.\n",
    "# The data points are connected with a line, which helps in identifying any trends or patterns over the years.\n",
    "\n",
    "# Group the data by year and calculate the average age for each year\n",
    "age_by_year = lpa_df.groupby('year')['age'].mean().reset_index()\n",
    "\n",
    "# Plot 'age' against 'year' using a line chart\n",
    "plt.figure(figsize=(20, 10))\n",
    "plt.plot(age_by_year['year'], age_by_year['age'], marker='o', linestyle='-', color='blue')\n",
    "plt.title('Average Age vs Year')\n",
    "plt.xlabel('Year')\n",
    "plt.ylabel('Average Age')\n",
    "plt.grid(True)\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0817f1a4-4782-4762-bd6e-715e96728f49",
   "metadata": {},
   "source": [
    "## Observations:\n",
    "The trend in the line chart indicates the changes in the average age of individuals over the years, \n",
    "based on the receipt dates from your dataset.\n",
    "Such a visualization can help identify patterns, \n",
    "such as whether the average age is increasing, decreasing, or remaining relatively stable over time.\n",
    "\n",
    "For example:\n",
    "An upward trend would suggest that the average age is increasing each year.\n",
    "A downward trend would indicate that the average age is decreasing.\n",
    "A flat line would imply that there is little to no change in the average age over the years.\n",
    "These trends can be influenced by various factors, such as the demographics of the population being studied, \n",
    "changes in policies, or other external factors that might affect the age distribution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9819b606-ffd9-4e8c-bcc6-ac54e81cf4cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Plot the Actuals\n",
    "\n",
    "# lpa_series = lpa_df['age']\n",
    "# #lpa_series = df.squeeze()\n",
    "# plt.figure(figsize=(28, 14))\n",
    "# plt.plot(lpa_series)\n",
    "# plt.title('UK Actual LPA Data', fontsize=20)\n",
    "# plt.ylabel('Age', fontsize=16)\n",
    "# plt.axvline(pd.to_datetime(df['receiptdate'], errors = 'coerce').dt.year, color = 'k', linestyle='--', alpha = 0.2)\n",
    "# # for year in range(min(pd.to_datetime(df['receiptdate'], errors = 'coerce').dt.year), \n",
    "# #     datetime.strptime(snapshot_end, '%Y-%m-%d').year):\n",
    "# #     #datetime.strptime(\"2024-03-18\", '%Y-%m-%d').year):\n",
    "# #     plt.axvline(pd.to_datetime(df['receiptdate'], errors = 'coerce'), color = 'k', linestyle='--', alpha = 0.2)\n",
    "# #     #plt.axvline(pd.to_datetime(str(year) + '-01-01'), color = 'k', \n",
    "# #     #print(year)\n",
    "# plt.legend()    \n",
    "# #plt.savefig('UK_Actual_LPA_Data.png', dpi=300, bbox_inches='tight')\n",
    "# plt.show()  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d9843c8-2905-43ae-9974-1c184aeba587",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Perform the GROUP BY operation and calculate the count\n",
    "#Cases_by_year_age = lpa_df.groupby(\n",
    "#    ['receiptdate', 'uid', 'type', 'casesubtype', 'status', 'donor_postcode', 'donor_gender', 'age']) \\\n",
    "#    .agg({'No_of_Cases': 'count'}) \\ #['donor_postcode', 'donor_gender', 'age']\n",
    "#    .reset_index()\n",
    "\n",
    "#agg_funcs = dict(No_of_Cases = 'count')\n",
    "#Cases_by_year_age = lpa_df.set_index(['receiptdate', 'uid', 'type', 'casesubtype', 'status', 'donor_postcode', 'donor_gender', 'age']) \\\n",
    "#    .stack() \\\n",
    "#    .groupby(level=0) \\\n",
    "#    .agg(agg_funcs)\n",
    "\n",
    "\n",
    "#Cases_by_year_age\n",
    "#lpa_by_year_age = lpa_df[['receiptdate', 'uid', 'type', 'casesubtype', 'status', 'donor_postcode', 'donor_gender', 'age']] \\\n",
    "#                    .groupby(['donor_postcode', 'donor_gender', 'age'])  \\\n",
    "#                    .agg('count')#.sum()\n",
    "#lpa_by_year_age.to_csv(r'lpa_by_year_age.csv')\n",
    "\n",
    "\n",
    "\n",
    "#lpa_df.to_csv(r'lpa_df.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49ed0a3d-b058-41e4-8cbc-61cd51c67b72",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Missing Data Imputation:\n",
    "\n",
    "There are be some people in the LPA data with missing age (they are represented with negetive numbers in column age). \n",
    "So for missing data (age) imputation, his code is written to use age distribution of cases that they have age and\n",
    "apply this to the total number of doners in that year. \n",
    "Actually, we allocate proportionaly distributed age across each year of these missing ages. \n",
    "E.g., if we get 90% of age distribution for a particular year,\n",
    "we used this age distribution to be applied to the 100% of donors to get the total distribution. \n",
    "\n",
    "The code below: \n",
    "first, loads the data from the CSV file and replaces negative ages \n",
    "with NaN to represent missing data. \n",
    "\n",
    "It then calculates the age distribution for each year. \n",
    "\n",
    "For each year, it finds the indices of the missing ages and imputes \n",
    "them by randomly choosing from the age distribution of that year. \n",
    "\n",
    "The imputed ages are proportional to the age distribution \n",
    "of the donors that year. \n",
    "\n",
    "Finally, it saves the DataFrame with the imputed ages to a new CSV file.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94e0a4fd-f395-40b0-b785-c361f1b11029",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Function to calculate the number of unique records by age, year, gender, and postcode\n",
    "# #def calculate_unique_records_by_age_year_gender_postcode(records):\n",
    "# # Get the current year\n",
    "# #current_year = datetime.now().year  \n",
    "# #Create a dictionary to store counts for each age, year, gender, and postcode combination\n",
    "# age_year_gender_postcode_counts = {}\n",
    "\n",
    "# records = lpa_df\n",
    "    \n",
    "# # Iterate over each record\n",
    "# for record in records:         \n",
    "#     # Extract gender and postcode\n",
    "#     gender = record[\"donor_gender\"]\n",
    "#     postcode = record[\"donor_postcode\"]\n",
    "#     dob = record[\"donor_dob\"]\n",
    "    \n",
    "#     # Create a unique key combining age, gender, and postcode\n",
    "#     key = (dob, gender, postcode)\n",
    "        \n",
    "#     # Increment the count for the key\n",
    "#     age_year_gender_postcode_counts[key] = age_year_gender_postcode_counts.get(key, 0) + 1\n",
    "        \n",
    "# return age_year_gender_postcode_counts\n",
    "\n",
    "# # Call the function and print the results\n",
    "# unique_records_by_age_year_gender_postcode = calculate_unique_records_by_age_year_gender_postcode(records)\n",
    "\n",
    "# print(\"Number of unique records by age, year, gender, and postcode:\")\n",
    "\n",
    "# for key, count in unique_records_by_age_year_gender_postcode.items():\n",
    "#     dob, gender, postcode = key\n",
    "#     print(f\"Date of Birth (D.o.B): {dob}, Gender: {gender}, Postcode: {postcode}, Count: {count}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1ce1de0-088c-4447-a432-f0dce3facfee",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This code will output the number of unique records for each age in each year for each donor gender in each donor postcode.\n",
    "# It calculates the age based on the current year and the birth year of each person in the records.\n",
    "# Then, it creates a unique key combining age, year, gender, and postcode, and increments the count for each key.\n",
    "# Finally, it prints the results showing the count of unique records for each combination.\n",
    "\n",
    "#from datetime import datetime\n",
    "\n",
    "# Sample data representing records with donor gender, donor postcode, and date of birth\n",
    "#records = [\n",
    "#    {\"donor_gender\": \"Male\", \"donor_postcode\": \"AB12 3CD\", \"date_of_birth\": \"1999-05-15\"},\n",
    "#    {\"donor_gender\": \"Female\", \"donor_postcode\": \"XY34 5YZ\", \"date_of_birth\": \"1994-08-20\"},\n",
    "#    {\"donor_gender\": \"Male\", \"donor_postcode\": \"CD56 7EF\", \"date_of_birth\": \"1996-02-10\"},\n",
    "#    {\"donor_gender\": \"Male\", \"donor_postcode\": \"FG78 9HI\", \"date_of_birth\": \"2000-11-30\"},\n",
    "#    {\"donor_gender\": \"Female\", \"donor_postcode\": \"JK90 1LM\", \"date_of_birth\": \"1987-03-25\"},\n",
    "#    {\"donor_gender\": \"Male\", \"donor_postcode\": \"OP23 4QR\", \"date_of_birth\": \"1993-09-05\"}\n",
    "#]\n",
    "\n",
    "# Function to calculate the number of unique records by age, year, gender, and postcode\n",
    "#def calculate_unique_records_by_age_year_gender_postcode(records):\n",
    "    # Get the current year\n",
    "#    current_year = datetime.now().year\n",
    "    \n",
    "    # Create a dictionary to store counts for each age, year, gender, and postcode combination\n",
    "#    age_year_gender_postcode_counts = {}\n",
    "    \n",
    "    # Iterate over each record\n",
    "#    for record in records:\n",
    "        # Extract the year of birth from the date_of_birth\n",
    "#        birth_year = int(record[\"date_of_birth\"].split(\"-\")[0])\n",
    "        \n",
    "        # Calculate the age of the person\n",
    "#        age = current_year - birth_year\n",
    "        \n",
    "        # Extract the year from the date_of_birth\n",
    "#        year = birth_year\n",
    "        \n",
    "        # Extract gender and postcode\n",
    "#        gender = record[\"donor_gender\"]\n",
    "#        postcode = record[\"donor_postcode\"]\n",
    "        \n",
    "        # Create a unique key combining age, year, gender, and postcode\n",
    "#        key = (age, year, gender, postcode)\n",
    "        \n",
    "        # Increment the count for the key\n",
    "#        age_year_gender_postcode_counts[key] = age_year_gender_postcode_counts.get(key, 0) + 1\n",
    "        \n",
    "#    return age_year_gender_postcode_counts\n",
    "\n",
    "# Call the function and print the results\n",
    "#unique_records_by_age_year_gender_postcode = calculate_unique_records_by_age_year_gender_postcode(records)\n",
    "#print(\"Number of unique records by age, year, gender, and postcode:\")\n",
    "#for key, count in unique_records_by_age_year_gender_postcode.items():\n",
    "#    age, year, gender, postcode = key\n",
    "#    print(f\"Age: {age}, Year: {year}, Gender: {gender}, Postcode: {postcode}, Count: {count}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07154148-04e8-4096-918b-625a4fed04b8",
   "metadata": {},
   "source": [
    "# Missing age imutation\n",
    "\n",
    "There are two issues with the age:\n",
    "\n",
    "1. The donor_gender might be missing or entered incorrectly\n",
    "\n",
    "2. The derieved age might be higher than 126 years old\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68ed109d-7ce6-4e76-a6f2-0712c10f1cc2",
   "metadata": {},
   "outputs": [],
   "source": [
    "lpa_data_sample_imputed = lpa_df\n",
    "\n",
    "# Filter rows with negative or greater than 126 age values\n",
    "criteria = lpa_data_sample_imputed[(lpa_data_sample_imputed['age'] < 0) | (lpa_data_sample_imputed['age'] > 126)]\n",
    "\n",
    "# Display the filtered rows\n",
    "print(criteria)\n",
    "\n",
    "# Replace age values with NULL (NaN) in the filtered rows\n",
    "lpa_data_sample_imputed.loc[criteria.index, 'age'] = np.nan #None\n",
    "\n",
    "# Display the updated DataFrame\n",
    "print(lpa_data_sample_imputed)\n",
    "\n",
    "# Group by year and count age groups\n",
    "age_distribution = lpa_data_sample_imputed.groupby('year')['age'].value_counts()\n",
    "\n",
    "# Fill missing ages with the most common age for each year\n",
    "most_common_age = lpa_data_sample_imputed.groupby('year')['age'].apply(lambda x: x.mode().iloc[0])\n",
    "lpa_data_sample_imputed['age'] = lpa_data_sample_imputed.apply(lambda row: most_common_age[row['year']] if pd.isna(row['age']) else row['age'], axis=1)\n",
    "\n",
    "# Display the age distribution after filling missing ages\n",
    "print(\"\\nAge distribution by year (including filled missing ages):\")\n",
    "print(age_distribution)\n",
    "\n",
    "# Display the final DataFrame\n",
    "print(\"\\nFinal DataFrame:\")\n",
    "print(lpa_data_sample_imputed)\n",
    "\n",
    "# Save the dataframe with imputed ages\n",
    "lpa_data_sample_imputed.to_csv('lpa_data_sample_imputed.csv', index=False)\n",
    "\n",
    "# Print a success message\n",
    "print(\"The missing age data has been successfully imputed and saved to lpa_data_sample_imputed.csv file.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5dedfc27-b664-4e22-b06a-ef0bf64c9765",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# lpa_data_sample_imputed = lpa_df\n",
    "\n",
    "\n",
    "# # Identify the rows with missing age (represented as negative numbers)\n",
    "# ## 1. The donor_gender might be missing or entered incorrectly:  < 0\n",
    "# ## 2. The derieved age might be higher than 126 years old > 126\n",
    "# lpa_data_sample_imputed['missing_age'] = (lpa_data_sample_imputed['age'] < 0) | (lpa_data_sample_imputed['age'] > 126)\n",
    "\n",
    "# # Replace negative ages with NaN\n",
    "# lpa_data_sample_imputed.loc[missing_age, 'age'] = np.nan\n",
    "\n",
    "# # Calculate the age distribution for each year excluding missing ages\n",
    "# age_distribution = lpa_data_sample_imputed.loc[~missing_age].groupby('year')['age'].value_counts(normalize=True)\n",
    "\n",
    "# # Calculate the age distribution for each year\n",
    "# age_distribution_per_year = lpa_data_sample_imputed.groupby('year')['age'].value_counts(normalize=True)\n",
    "\n",
    "# # Apply the age distribution to the total number of donors in each year\n",
    "# for year in df['year'].unique():\n",
    "#     # Calculate the number of missing ages in the current year\n",
    "#     num_missing = missing_age & (df['year'] == year)\n",
    "    \n",
    "#     # If there are missing ages in the current year\n",
    "#     if num_missing.sum() > 0:\n",
    "#         # Generate ages according to the age distribution of the current year\n",
    "#         imputed_ages = np.random.choice(age_distribution[year].index, \n",
    "#                                         p=age_distribution[year].values, \n",
    "#                                         size=num_missing.sum())\n",
    "        \n",
    "#         # Assign the generated ages to the missing ages\n",
    "#         df.loc[num_missing, 'age'] = imputed_ages\n",
    "\n",
    "\n",
    "# # Apply the age distribution to the missing ages\n",
    "# for year in lpa_data_sample_imputed['year'].unique():\n",
    "#     missing_age_indices = lpa_data_sample_imputed[(lpa_data_sample_imputed['year'] == year) & (lpa_data_sample_imputed['age'].isna())].index\n",
    "#     if not missing_age_indices.empty:\n",
    "#         imputed_ages = np.random.choice(age_distribution_per_year[year].index, \n",
    "#                                         p=age_distribution_per_year[year].values, \n",
    "#                                         size=len(missing_age_indices))\n",
    "#         lpa_data_sample_imputed.loc[missing_age_indices, 'age'] = imputed_ages\n",
    "\n",
    "      \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0916770-95b6-4bff-8b14-b1d103bc585a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a unique identifier based on multiple columns:\n",
    "# lpa_unique_key = lpa_df\n",
    "\n",
    "\n",
    "# #df1.set_index(['donor_postcode', 'donor_gender', 'age']).index.factorize()[0]+1\n",
    "# lpa_unique_key.insert(loc = 0, column='ukey', value = lpa_unique_key.set_index(['donor_postcode', 'donor_gender', 'age']).index.factorize()[0]+1)\n",
    "# #lpa_unique_key\n",
    "\n",
    "# #(lpa_unique_key.fillna({'donor_postcode':'', 'donor_gender':'', 'age':''})\n",
    "# #   .groupby(['donor_postcode', 'donor_gender', 'age'],sort=False).ngroup()+1)\n",
    "\n",
    "# #lpa_unique_key.loc[lpa_unique_key['type']=='lpa','ukey'].agg(['nunique','count','size'])\n",
    "# #lpa_unique_key.query('type == \"lpa\"')['ukey'].agg(['nunique','count','size'])\n",
    "# #lpa_unique_key.query('casesubtype == \"hw\"')['ukey'].agg(['nunique','count','size'])\n",
    "# #lpa_unique_key.query('casesubtype == \"pfa\"')['ukey'].agg(['nunique','count','size'])\n",
    "# #lpa_unique_key.groupby(['ukey']).count()\n",
    "# #lpa_unique_key['count_ukey'] = lpa_unique_key['ukey'].value_counts()\n",
    "# #lpa_unique_key\n",
    "\n",
    "\n",
    "\n",
    "# lpa_unique_key['CountbyUkey'] = lpa_unique_key.groupby(['donor_postcode', 'donor_gender']).age.transform('count')\n",
    "# lpa_unique_key['CountbyAge'] = lpa_unique_key.groupby('year').age.transform('count').sum()\n",
    "\n",
    "# # Perform the GROUP BY operation and calculate the sum\n",
    "# lpa_age = lpa_unique_key.groupby(['donor_postcode', 'donor_gender', 'age']) \\\n",
    "#     .agg({'CountbyAge': 'sum'}) \\\n",
    "#     .reset_index()\n",
    "\n",
    "# print(lpa_age)\n",
    "# #lpa_unique_key['month'] = lpa_unique_key['ArrivalDate'].dt.month\n",
    "\n",
    "\n",
    "# # Cases_by_year_age\n",
    "\n",
    "# #lpa_by_year_age = lpa_unique_key[['receiptdate', 'uid', 'type', 'casesubtype', 'status', 'donor_postcode', 'donor_gender', 'age']] \\\n",
    "# #                    .groupby(['donor_postcode', 'donor_gender', 'age'])  \\\n",
    "# #                    .agg('count')#.sum()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "607f2311-c45a-4016-ac0e-f2ca2a88c588",
   "metadata": {},
   "source": [
    "# Generate a Unique key by combining age, donor_gender, and donor_postcode\n",
    "\n",
    "For ages over 19 years old:\n",
    "Unique case reference for each donor = [donor_dob + donor_postcode + donor_gender]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d42c4dde-0614-45ab-91a1-254117ca8f41",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# DataFrame with the count of unique records for each combination of age and year. \n",
    "# Generate a unique key by combining age, donor_gender, and donor_postcode, \n",
    "# and then calculate the number of unique records by age and year.\n",
    "\n",
    "lpa_unique = lpa_data_sample_imputed\n",
    "\n",
    "# Remove spaces from the donor postcodes\n",
    "lpa_unique['donor_postcode'] = lpa_unique['donor_postcode'].str.strip()\n",
    "lpa_unique['donor_postcode'] = lpa_unique['donor_postcode'].str.replace(' ', '')\n",
    "\n",
    "# Generate a unique key by combining age, donor_gender, and donor_postcode\n",
    "lpa_unique['unique_key'] = lpa_unique['donor_dob'].astype(str) \\\n",
    "+ lpa_unique['donor_gender'] + lpa_unique['donor_postcode']\n",
    "\n",
    "# lpa_by_year_age = lpa_unique_key\n",
    "\n",
    "# lpa_by_year_age.to_csv(r'lpa_by_year_age.csv')\n",
    "\n",
    "# remove duplicate rows based on Id values(unique_key) and \n",
    "# keep only the row that don't have 0 value in all the fields.\n",
    "\n",
    "\n",
    "duplicateMask = lpa_unique.duplicated('unique_key', keep=False)\n",
    "\n",
    "lpa_unique = pd.concat([lpa_unique.loc[duplicateMask & lpa_unique[['age', 'donor_gender', 'donor_postcode']].ne(0).any(axis=1)], \\\n",
    "               lpa_unique[~duplicateMask]])\n",
    "\n",
    "#lpa_df['zero']=lpa_df.select_dtypes(['int','float']).eq(0).sum(axis=1)\n",
    "#df=df.sort_values(['zero','Id']).drop_duplicates(subset=['Id']).drop(columns='zero')df['zero']=df.select_dtypes(['int','float']).eq(0).sum(axis=1)\n",
    "#df=df.sort_values(['zero','Id']).drop_duplicates(subset=['Id']).drop(columns='zero')\n",
    "\n",
    "#lpa_unique = lpa_unique.drop_duplicates(subset=\"unique_key\")\n",
    "lpa_unique"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c2235a2-7a56-4d54-b10d-e0b17c6aa8ff",
   "metadata": {},
   "source": [
    "# Save the LPA data with new unique keys (as a unique ID)\n",
    "\n",
    "Sort by the unique id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51f4fb88-82cb-4844-bcae-72657302a02e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sort the rows of dataframe by  'unique_key'  \n",
    "## column inplace\n",
    "\n",
    "#lpa_df_index = lpa_unique.sort_values(lpa_unique.columns[9])\n",
    "\n",
    "# Extract month letter and year \n",
    "lpa_unique['month_year'] = lpa_unique['receiptdate'].dt.strftime('%b-%y')\n",
    "\n",
    "## Sort by 'unique_key' column in ascending order\n",
    "lpa_df_index = lpa_unique.sort_values(by=['unique_key','receiptdate'])\n",
    "\n",
    "\n",
    "#lpa_df_index = lpa_unique.sort_values['unique_key']\n",
    "#lpa_df_index = lpa_unique.sort_values(by = 'unique_key', axis = 1, inplace = True, ascending = True)\n",
    "#lpa_df_index = lpa_unique.reindex(sorted(lpa_unique.columns), axis=1)\n",
    "\n",
    "## Set index\n",
    "#df['receiptdate'] = pd.to_datetime(df['receiptdate'])\n",
    "\n",
    "#df = df.set_index('receiptdate').asfreq('D')\n",
    "#lpa_df_index['unique_key'] = \n",
    "\n",
    "## Set the unique key as an ID (index)\n",
    "lpa_df_index.set_index('unique_key', inplace = True)\n",
    "\n",
    "#df.index = df.index.to_period('D')\n",
    "                            \n",
    "###print(df.head())\n",
    "###print(df.tail())\n",
    "\n",
    "#Missing_data = lpa_df_index[(lpa_data_sample_imputed['age'] < 0 | lpa_data_sample_imputed['age'] > 126)]\n",
    "#print(Missing_data)\n",
    "\n",
    "\n",
    "\n",
    "# Extract and save data into a csv file\n",
    "lpa_data = lpa_df_index\n",
    "\n",
    "\n",
    "#lpa_data.to_csv(r'lpa_data.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bce6b69d-876f-463c-ad33-14482c777f1a",
   "metadata": {},
   "source": [
    "# Number of LPA reciepts\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90dd8f3a-f1d4-4753-b55a-15cb1620ffcf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Average daily receipts for 2023\n",
    "\n",
    "# create a drop down list with average daily receipts of LPA application in 2024 in the range say from 4000 – 7000 in increments of 100. \n",
    "# Then this should be used as an estimate to apply unceratinty and to be converted into an age-specific annual donor forecast.\n",
    "\n",
    "# Filter data to involve Registered status and Post-covid data from 2022 onwards\n",
    "unique_receipts_post_covid = lpa_unique\n",
    "\n",
    "###unique_receipts_post_covid[unique_receipts_post_covid['status'].str.contains(\"Registered\")]\n",
    "\n",
    "\n",
    "unique_receipts_post_covid = unique_receipts_post_covid.query('year > 2018')\n",
    "\n",
    "\n",
    "#df[df['Overall_Percentage'].isin([value for value in df['Overall_Percentage'] if value > 60])]\n",
    "#df[df.apply(lambda row: row['Overall_Percentage'] > 55, axis=1)]\n",
    "# # The “loc” method is used to access a group of rows and columns by label(s) or a boolean array. \n",
    "# #We can utilise it to filter a DataFrame based on specific column values.\n",
    "#df.loc[df['Overall_Percentage'] > 40]\n",
    "# # The “iloc” method is similar to “loc” but uses integer-based indexing instead of labels. \n",
    "# #It allows us to filter a DataFrame by specifying the row and column indices.\n",
    "#df[df.iloc[:, -1] > 40]\n",
    "\n",
    "\n",
    "#count_unique_receipts_monthly = unique_receipts_post_covid.groupby(['month_year'])['receiptdate'].agg('count').reset_index()\n",
    "\n",
    "unique_receipts_post_covid = unique_receipts_post_covid.groupby(['receiptdate'])['unique_key'].agg('count').reset_index()\n",
    "\n",
    "#count_unique_receipts_monthly = unique_receipts_post_covid.groupby(['year', 'month_year'])['uid'].agg('count').reset_index()\n",
    "\n",
    "#count_unique_receipts_monthly = unique_receipts_post_covid.groupby(['month_year'])['receiptdate'].nunique().reset_index(name='count')\n",
    "\n",
    "# Calculating the overall percentage for each donor and adding a new column\n",
    "#count_unique_receipts_daily['Overall_Percentage'] = count_unique_receipts_daily.iloc[:, 1:].mean(axis=1)\n",
    "\n",
    "\n",
    "\n",
    "count_unique_receipts_daily = unique_receipts_post_covid.rename(columns={'unique_key': 'daily_demand'})\n",
    "\n",
    "count_unique_receipts_daily['avg_daily_demand'] = count_unique_receipts_daily['daily_demand'].mean()\n",
    "\n",
    "\n",
    "# Extract month letter and year \n",
    "###lpa_reciepts['month_year'] = lpa_reciepts['receiptdate'].dt.strftime('%b-%y')\n",
    "# Calculate the number of unique records by month and year\n",
    "#count_reciepts_month = lpa_reciepts.groupby(['year', 'month_year'])['receiptdate'].nunique().reset_index(name='count')\n",
    "###Count_daily_reciepts = lpa_reciepts.groupby(['receiptdate']).count()\n",
    "\n",
    "#unique_records = df.groupby('unique_key').agg('count').reset_index()  #.groupby(['year'])['unique_key'].nunique().reset_index(name='count')\n",
    "###Count_daily_reciepts = Count_daily_reciepts.rename(columns={\"count\": \"Count_of_daily_reciepts\"})\n",
    "\n",
    "#\n",
    "\n",
    "\n",
    "# Extract month letter and year \n",
    "count_unique_receipts_daily['month_year'] = count_unique_receipts_daily['receiptdate'].dt.strftime('%b-%y')\n",
    "count_unique_receipts_daily['year'] = count_unique_receipts_daily['receiptdate'].dt.strftime('%Y')\n",
    "print(count_unique_receipts_daily)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "214f4ec0-d301-4bec-8d24-d5bd142c93ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "daily_demand = count_unique_receipts_daily['daily_demand']\n",
    "\n",
    "# Mean and standard deviation\n",
    "mean_demand = daily_demand.mean()\n",
    "std_dev_demand = daily_demand.std()\n",
    "\n",
    "print(f\"Average of LPA daily demand: {mean_demand}\")\n",
    "print(f\"Standard Deviation of LPA daily demand: {std_dev_demand}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7d25525-363b-4da3-b46d-f5d44608dbf2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 95% Confidence Interval\n",
    "ci_lower = mean_demand - 1.96 * std_dev_demand\n",
    "ci_upper = mean_demand + 1.96 * std_dev_demand\n",
    "\n",
    "print(f\"Mean Demand: {mean_demand}\")\n",
    "print(f\"95% Confidence Interval: [{ci_lower}, {ci_upper}]\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd1f5016-96de-427b-9b9b-59fd8f67d48d",
   "metadata": {},
   "source": [
    "## use the average daily demand from historical data as the basis for your naive forecast.\n",
    "### So by having daily demand data for the past year, the forecast for tomorrow would be equal to today’s demand."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "704bd926-a8fa-4a0f-8117-5a21d8ebc79b",
   "metadata": {},
   "source": [
    "# **Naïve extrapolation**\n",
    "Also known as the “naïve forecast,” is a straightforward method for demand forecasting. In Excel, apply this technique by assuming that future demand will be the same as the most recent observed value.\n",
    "- A naïve extrapolation of the receipts trend immediately before the broadcast event on the 21 November gives us some idea of what receipt volumes might have been between December 2023 and March 2024 and therefore what effect the broadcast had on overall receipt volumes. \n",
    "    - create a drop down list with average daily receipts of LPA application in 2024 in the range say from 4000 – 7000 in increments of 100.\n",
    "\n",
    "# **UPDATED FORECAST: AVERAGE DAILY RECEIPTS**\n",
    "In terms of how to apply all of this to the long term LPA model:\n",
    "- Convert the receipts forecast to an annual total by multiplying by the number of working days. If we used the central estimate of 5600 then multiplying this by 256 which the number of working days in 2024 gives an annual total for receipts of 1,433,600. This can be converted into an estimate of the number of donors based on the ratio of donors to receipts (say) over the last couple of years. And then convert the donor estimate into age specific estimates based on the distribution by age , again over (say) the last couple of years."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7eed1172-2917-4817-a84f-0e630c0ddb2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Naïve forecast\n",
    "naïve_forecast = daily_demand.iloc[-1]  # Last observed demand\n",
    "naïve_forecast"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "529b86eb-cc0e-486f-bf3e-b455351107bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# Adjust with uncertainty\n",
    "forecast_with_uncertainty = np.random.normal(loc=naïve_forecast, scale=std_dev_demand, size=1000)\n",
    "\n",
    "# Summary of the forecast\n",
    "forecast_mean = forecast_with_uncertainty.mean()\n",
    "forecast_ci_lower = np.percentile(forecast_with_uncertainty, 2.5)\n",
    "forecast_ci_upper = np.percentile(forecast_with_uncertainty, 97.5)\n",
    "\n",
    "print(f\"Forecast Mean: {forecast_mean}\")\n",
    "print(f\"Forecast 95% CI: [{forecast_ci_lower}, {forecast_ci_upper}]\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3fda269-fa3b-4ad6-8da4-2b1ebc7f0a77",
   "metadata": {},
   "source": [
    "#  Incorporating uncertainty and short-term impacts into your long-term forecasting model: \n",
    "- ensuring that both historical and forecasted data exclude weekends and UK holidays, providing a more accurate and realistic forecast.\n",
    "- generate and analyze age-specific annual demand forecasts based on the overall forecast and specified age group proportions.\n",
    "- convert the receipts forecast to an annual total, estimate the number of donors, and convert the donor estimate into age-specific estimates based on the distribution by age.\n",
    "in order to apply uncertainty based on short term forecasting drivers such as post covid and advertisement impacts of this short term forecasting on the long term forecasting for number of daily reciepts of Living Power of Attorney  (LPA) demands by using average daily demands based on different scenarios Naïve extrapolation for future pandemic demands based on COVID-19 data age-specific reflect the uncertainty around the receipts forecast for next year update quarterly and also to vary the receipts inputs to reflect uncertainty around this estimate which will then also be reflected in the longer term age-specific forecast model?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2bb50095-8fb2-4d0a-818b-25fbab3d620a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.stats import norm\n",
    "import holidays\n",
    "\n",
    "# Generate sample historical data\n",
    "np.random.seed(42)\n",
    "dates = pd.date_range(start='2020-01-01', end='2023-12-31', freq='D')\n",
    "historical_data = pd.DataFrame({\n",
    "    'date': dates,\n",
    "    'demand': np.random.poisson(lam=100, size=len(dates))  # Simulating daily demands\n",
    "})\n",
    "\n",
    "# UK holidays\n",
    "uk_holidays = holidays.UnitedKingdom()\n",
    "\n",
    "# Function to remove weekends and holidays\n",
    "def remove_weekends_and_holidays(data):\n",
    "    data['weekday'] = data['date'].dt.weekday\n",
    "    data['is_holiday'] = data['date'].isin(uk_holidays)\n",
    "    return data[(data['weekday'] < 5) & (~data['is_holiday'])]\n",
    "\n",
    "# Remove weekends and holidays from historical data\n",
    "historical_data = remove_weekends_and_holidays(historical_data)\n",
    "\n",
    "# Function to calculate average daily demand and uncertainty\n",
    "def calculate_daily_demand_stats(data):\n",
    "    mean_demand = data['demand'].mean()\n",
    "    std_dev_demand = data['demand'].std()\n",
    "    return mean_demand, std_dev_demand\n",
    "\n",
    "# Function to perform Naïve extrapolation with uncertainty\n",
    "def naive_extrapolation_with_uncertainty(last_observed, std_dev, num_days=90):\n",
    "    forecast = np.random.normal(loc=last_observed, scale=std_dev, size=num_days)\n",
    "    return forecast\n",
    "\n",
    "# Function to update quarterly forecasts\n",
    "def update_quarterly_forecast(data, num_quarters=4):\n",
    "    forecasts = []\n",
    "    for _ in range(num_quarters):\n",
    "        mean_demand, std_dev_demand = calculate_daily_demand_stats(data)\n",
    "        last_observed = data['demand'].iloc[-1]\n",
    "        quarterly_forecast = naive_extrapolation_with_uncertainty(last_observed, std_dev_demand)\n",
    "        \n",
    "        # Generate new dates for the forecast period\n",
    "        new_dates = pd.date_range(start=data['date'].iloc[-1] + pd.Timedelta(days=1), periods=len(quarterly_forecast))\n",
    "        new_data = pd.DataFrame({'date': new_dates, 'demand': quarterly_forecast})\n",
    "        \n",
    "        # Remove weekends and holidays from new data\n",
    "        new_data = remove_weekends_and_holidays(new_data)\n",
    "        \n",
    "        forecasts.extend(new_data['demand'])\n",
    "        data = pd.concat([data, new_data], ignore_index=True)\n",
    "    return data\n",
    "\n",
    "# Function to apply scenario analysis\n",
    "def apply_scenario_analysis(base_forecast, impact_factor):\n",
    "    return base_forecast * impact_factor\n",
    "\n",
    "# Apply short-term drivers (post-COVID and advertisement impacts)\n",
    "post_covid_impact = 1.1  # 10% increase\n",
    "advertisement_impact = 1.2  # 20% increase\n",
    "\n",
    "# Generate base forecast\n",
    "mean_demand, std_dev_demand = calculate_daily_demand_stats(historical_data)\n",
    "base_forecast = naive_extrapolation_with_uncertainty(mean_demand, std_dev_demand, num_days=365)\n",
    "\n",
    "# Apply scenario impacts\n",
    "forecast_post_covid = apply_scenario_analysis(base_forecast, post_covid_impact)\n",
    "forecast_advertisement = apply_scenario_analysis(base_forecast, advertisement_impact)\n",
    "\n",
    "# Combine impacts\n",
    "combined_forecast = apply_scenario_analysis(forecast_post_covid, advertisement_impact)\n",
    "\n",
    "# Update quarterly forecasts\n",
    "updated_forecast_data = update_quarterly_forecast(historical_data)\n",
    "\n",
    "# Plotting the results\n",
    "plt.figure(figsize=(14, 7))\n",
    "plt.plot(historical_data['date'], historical_data['demand'], label='Historical Data', color='blue')\n",
    "plt.plot(pd.date_range(start=historical_data['date'].iloc[-1] + pd.Timedelta(days=1), periods=len(combined_forecast)), \n",
    "         combined_forecast, label='Forecast with Impacts', color='orange')\n",
    "plt.xlabel('Date')\n",
    "plt.ylabel('Daily Receipts')\n",
    "plt.title('LPA Daily Receipts Forecast with Uncertainty and Short-term Drivers')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "print(\"Forecast with combined impacts for the next year:\")\n",
    "print(combined_forecast)\n",
    "\n",
    "# Save the forecast data\n",
    "forecast_dates = pd.date_range(start=historical_data['date'].iloc[-1] + pd.Timedelta(days=1), periods=len(combined_forecast))\n",
    "forecast_df = pd.DataFrame({'date': forecast_dates, 'forecast': combined_forecast})\n",
    "forecast_df = remove_weekends_and_holidays(forecast_df)\n",
    "forecast_df.to_csv('lpa_forecast.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "563d7700-bf44-495d-92e8-03cb95d3da2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.stats import norm\n",
    "import holidays\n",
    "\n",
    "# ForecastingModel: Encapsulates all forecasting-related methods and attributes.\n",
    "class ForecastingModel:\n",
    "    def __init__(self, historical_data, post_covid_impact=1.1, advertisement_impact=1.2):\n",
    "        self.historical_data = historical_data\n",
    "        self.post_covid_impact = post_covid_impact\n",
    "        self.advertisement_impact = advertisement_impact\n",
    "        self.uk_holidays = holidays.UnitedKingdom()\n",
    "        self.age_group_proportions = {\n",
    "            '50-59': 0.25,\n",
    "            '60-69': 0.30,\n",
    "            '70+': 0.45\n",
    "        }\n",
    "        self.working_days_per_year = 256\n",
    "        self.receipts_to_donors_ratio = 0.75  # Example ratio, should be calculated based on historical data\n",
    "\n",
    "\n",
    "    # remove_weekends_and_holidays: Removes weekends and holidays from the data.\n",
    "    def remove_weekends_and_holidays(self, data):\n",
    "        data['weekday'] = data['date'].dt.weekday\n",
    "        data['is_holiday'] = data['date'].isin(self.uk_holidays)\n",
    "        return data[(data['weekday'] < 5) & (~data['is_holiday'])]\n",
    "\n",
    "    #calculate_daily_demand_stats: Computes mean and standard deviation of demand.\n",
    "    def calculate_daily_demand_stats(self, data):\n",
    "        mean_demand = data['demand'].mean()\n",
    "        std_dev_demand = data['demand'].std()\n",
    "        return mean_demand, std_dev_demand\n",
    "\n",
    "    # naive_extrapolation_with_uncertainty: Generates a Naïve forecast with uncertainty.\n",
    "    def naive_extrapolation_with_uncertainty(self, last_observed, std_dev, num_days=90):\n",
    "        forecast = np.random.normal(loc=last_observed, scale=std_dev, size=num_days)\n",
    "        return forecast\n",
    "\n",
    "    # apply_scenario_analysis: Applies short-term impact factors to the base forecast.\n",
    "    def apply_scenario_analysis(self, base_forecast, impact_factor):\n",
    "        return base_forecast * impact_factor\n",
    "\n",
    "    # update_quarterly_forecast: Updates the forecast quarterly and removes weekends/holidays.\n",
    "    def update_quarterly_forecast(self, data, num_quarters=4):\n",
    "        forecasts = []\n",
    "        for _ in range(num_quarters):\n",
    "            mean_demand, std_dev_demand = self.calculate_daily_demand_stats(data)\n",
    "            last_observed = data['demand'].iloc[-1]\n",
    "            quarterly_forecast = self.naive_extrapolation_with_uncertainty(last_observed, std_dev_demand)\n",
    "            \n",
    "            new_dates = pd.date_range(start=data['date'].iloc[-1] + pd.Timedelta(days=1), periods=len(quarterly_forecast))\n",
    "            new_data = pd.DataFrame({'date': new_dates, 'demand': quarterly_forecast})\n",
    "            new_data = self.remove_weekends_and_holidays(new_data)\n",
    "            \n",
    "            forecasts.extend(new_data['demand'])\n",
    "            data = pd.concat([data, new_data], ignore_index=True)\n",
    "        return data\n",
    "\n",
    "    # generate_forecast: Generates a forecast for a specified number of days, with optional short-term impacts.\n",
    "    def generate_forecast(self, num_days=365, short_term=True):\n",
    "        mean_demand, std_dev_demand = self.calculate_daily_demand_stats(self.historical_data)\n",
    "        base_forecast = self.naive_extrapolation_with_uncertainty(mean_demand, std_dev_demand, num_days)\n",
    "        \n",
    "        if short_term:\n",
    "            forecast_post_covid = self.apply_scenario_analysis(base_forecast, self.post_covid_impact)\n",
    "            combined_forecast = self.apply_scenario_analysis(forecast_post_covid, self.advertisement_impact)\n",
    "        else:\n",
    "            combined_forecast = base_forecast\n",
    "        \n",
    "        forecast_dates = pd.date_range(start=self.historical_data['date'].iloc[-1] + pd.Timedelta(days=1), periods=num_days)\n",
    "        forecast_df = pd.DataFrame({'date': forecast_dates, 'forecast': combined_forecast})\n",
    "        forecast_df = self.remove_weekends_and_holidays(forecast_df)\n",
    "        return forecast_df\n",
    "    \n",
    "    # calculate_annual_total_receipts: Calculates the annual total receipts by multiplying the average daily receipts by the number of working days in the year.\n",
    "    def calculate_annual_total_receipts(self, forecast_df):\n",
    "        average_daily_receipts = forecast_df['forecast'].mean()\n",
    "        annual_total_receipts = average_daily_receipts * self.working_days_per_year\n",
    "        return annual_total_receipts\n",
    "    \n",
    "    \n",
    "    # estimate_number_of_donors: Estimates the number of donors based on the annual total receipts and the receipts-to-donors ratio.\n",
    "    def estimate_number_of_donors(self, annual_total_receipts):\n",
    "        number_of_donors = annual_total_receipts * self.receipts_to_donors_ratio\n",
    "        return number_of_donors\n",
    "    \n",
    "    # calculate_age_specific_forecast: Converts the donor estimate into age-specific estimates based on the distribution by age.\n",
    "    def calculate_age_specific_forecast(self, number_of_donors):\n",
    "        age_specific_forecast = {}\n",
    "        for age_group, proportion in self.age_group_proportions.items():\n",
    "            age_specific_forecast[age_group] = number_of_donors * proportion\n",
    "        return age_specific_forecast    \n",
    "    \n",
    "    # plot_forecast: Plots the historical and forecasted data.\n",
    "    def plot_forecast(self, forecast_df):\n",
    "        plt.figure(figsize=(14, 7))\n",
    "        plt.plot(self.historical_data['date'].values, self.historical_data['demand'].values, label='Historical Data', color='blue')\n",
    "        plt.plot(forecast_df['date'].values, forecast_df['forecast'].values, label='Forecast with Impacts', color='orange')\n",
    "        plt.xlabel('Date')\n",
    "        plt.ylabel('Daily Receipts')\n",
    "        plt.title('LPA Daily Receipts Forecast with Uncertainty and Short-term Drivers')\n",
    "        plt.legend()\n",
    "        plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6882afa-bd40-489e-aede-ae21fd1c7ed5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# LPA historical data\n",
    "historical_data = count_unique_receipts_daily[['receiptdate', 'daily_demand']].rename(columns={'receiptdate': 'date', 'daily_demand': 'demand'})\n",
    "\n",
    "# Instantiate the ForecastingModel class\n",
    "forecast_model = ForecastingModel(historical_data)\n",
    "\n",
    "# Remove weekends and holidays from historical data\n",
    "historical_data = forecast_model.remove_weekends_and_holidays(historical_data)\n",
    "\n",
    "# Generate short-term forecast\n",
    "short_term_forecast_df = forecast_model.generate_forecast(num_days=365, short_term=True)\n",
    "\n",
    "# Generate long-term forecast (without short-term impacts)\n",
    "long_term_forecast_df = forecast_model.generate_forecast(num_days=365, short_term=False)\n",
    "\n",
    "# Calculate annual total receipts for short-term forecast\n",
    "annual_total_receipts_short_term = forecast_model.calculate_annual_total_receipts(short_term_forecast_df)\n",
    "\n",
    "# Estimate number of donors for short-term forecast\n",
    "number_of_donors_short_term = forecast_model.estimate_number_of_donors(annual_total_receipts_short_term)\n",
    "\n",
    "# Calculate age-specific forecasts for short-term forecast\n",
    "age_specific_forecast_short_term = forecast_model.calculate_age_specific_forecast(number_of_donors_short_term)\n",
    "\n",
    "# Calculate annual total receipts for long-term forecast\n",
    "annual_total_receipts_long_term = forecast_model.calculate_annual_total_receipts(long_term_forecast_df)\n",
    "\n",
    "# Estimate number of donors for long-term forecast\n",
    "number_of_donors_long_term = forecast_model.estimate_number_of_donors(annual_total_receipts_long_term)\n",
    "\n",
    "# Calculate age-specific forecasts for long-term forecast\n",
    "age_specific_forecast_long_term = forecast_model.calculate_age_specific_forecast(number_of_donors_long_term)\n",
    "\n",
    "# Plotting the short-term forecast\n",
    "forecast_model.plot_forecast(short_term_forecast_df)\n",
    "\n",
    "# Plotting the long-term forecast\n",
    "forecast_model.plot_forecast(long_term_forecast_df)\n",
    "\n",
    "# Save the short-term forecast data\n",
    "short_term_forecast_df.to_csv('lpa_short_term_forecast.csv', index=False)\n",
    "\n",
    "# Save the long-term forecast data\n",
    "long_term_forecast_df.to_csv('lpa_long_term_forecast.csv', index=False)\n",
    "\n",
    "# Print annual total receipts, number of donors, and age-specific forecasts\n",
    "print(\"Annual total receipts (short-term):\", annual_total_receipts_short_term)\n",
    "print(\"Number of donors (short-term):\", number_of_donors_short_term)\n",
    "print(\"Age-specific forecast (short-term):\", age_specific_forecast_short_term)\n",
    "\n",
    "print(\"Annual total receipts (long-term):\", annual_total_receipts_long_term)\n",
    "print(\"Number of donors (long-term):\", number_of_donors_long_term)\n",
    "print(\"Age-specific forecast (long-term):\", age_specific_forecast_long_term)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e2b2d81-bf40-4494-ae44-dfc4eeeb2d14",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from scipy.stats import norm\n",
    "import holidays\n",
    "\n",
    "class ForecastingModel:\n",
    "    def __init__(self, historical_data, post_covid_impact=1.1, advertisement_impact=1.2):\n",
    "        self.historical_data = historical_data\n",
    "        self.post_covid_impact = post_covid_impact\n",
    "        self.advertisement_impact = advertisement_impact\n",
    "        self.uk_holidays = holidays.UnitedKingdom()\n",
    "        self.age_group_proportions = {\n",
    "            '50-59': 0.25,\n",
    "            '60-69': 0.30,\n",
    "            '70+': 0.45\n",
    "        }\n",
    "        self.working_days_per_year = 256\n",
    "        self.receipts_to_donors_ratio = 0.75  # Example ratio, should be calculated based on historical data\n",
    "\n",
    "    def remove_weekends_and_holidays(self, data):\n",
    "        data['weekday'] = data['date'].dt.weekday\n",
    "        data['is_holiday'] = data['date'].isin(self.uk_holidays)\n",
    "        return data[(data['weekday'] < 5) & (~data['is_holiday'])]\n",
    "\n",
    "    def calculate_daily_demand_stats(self, data):\n",
    "        mean_demand = data['demand'].mean()\n",
    "        std_dev_demand = data['demand'].std()\n",
    "        return mean_demand, std_dev_demand\n",
    "\n",
    "    def naive_extrapolation_with_uncertainty(self, last_observed, std_dev, num_days=90):\n",
    "        forecast = np.random.normal(loc=last_observed, scale=std_dev, size=num_days)\n",
    "        return forecast\n",
    "\n",
    "    def apply_scenario_analysis(self, base_forecast, impact_factor):\n",
    "        return base_forecast * impact_factor\n",
    "\n",
    "    def update_quarterly_forecast(self, data, num_quarters=4):\n",
    "        forecasts = []\n",
    "        for _ in range(num_quarters):\n",
    "            mean_demand, std_dev_demand = self.calculate_daily_demand_stats(data)\n",
    "            last_observed = data['demand'].iloc[-1]\n",
    "            quarterly_forecast = self.naive_extrapolation_with_uncertainty(last_observed, std_dev_demand)\n",
    "            \n",
    "            new_dates = pd.date_range(start=data['date'].iloc[-1] + pd.Timedelta(days=1), periods=len(quarterly_forecast))\n",
    "            new_data = pd.DataFrame({'date': new_dates, 'demand': quarterly_forecast})\n",
    "            new_data = self.remove_weekends_and_holidays(new_data)\n",
    "            \n",
    "            forecasts.extend(new_data['demand'])\n",
    "            data = pd.concat([data, new_data], ignore_index=True)\n",
    "        return data\n",
    "\n",
    "    def generate_forecast(self, num_days=365, short_term=True):\n",
    "        mean_demand, std_dev_demand = self.calculate_daily_demand_stats(self.historical_data)\n",
    "        base_forecast = self.naive_extrapolation_with_uncertainty(mean_demand, std_dev_demand, num_days)\n",
    "        \n",
    "        if short_term:\n",
    "            forecast_post_covid = self.apply_scenario_analysis(base_forecast, self.post_covid_impact)\n",
    "            combined_forecast = self.apply_scenario_analysis(forecast_post_covid, self.advertisement_impact)\n",
    "        else:\n",
    "            combined_forecast = base_forecast\n",
    "        \n",
    "        forecast_dates = pd.date_range(start=self.historical_data['date'].iloc[-1] + pd.Timedelta(days=1), periods=num_days)\n",
    "        forecast_df = pd.DataFrame({'date': forecast_dates, 'forecast': combined_forecast})\n",
    "        forecast_df = self.remove_weekends_and_holidays(forecast_df)\n",
    "        return forecast_df\n",
    "\n",
    "    def calculate_annual_total_receipts(self, forecast_df):\n",
    "        average_daily_receipts = forecast_df['forecast'].mean()\n",
    "        annual_total_receipts = average_daily_receipts * self.working_days_per_year\n",
    "        return annual_total_receipts\n",
    "\n",
    "    def estimate_number_of_donors(self, annual_total_receipts):\n",
    "        number_of_donors = annual_total_receipts * self.receipts_to_donors_ratio\n",
    "        return number_of_donors\n",
    "\n",
    "    def calculate_age_specific_forecast(self, number_of_donors):\n",
    "        age_specific_forecast = {}\n",
    "        for age_group, proportion in self.age_group_proportions.items():\n",
    "            age_specific_forecast[age_group] = number_of_donors * proportion\n",
    "        return age_specific_forecast\n",
    "\n",
    "    def plot_forecast(self, forecast_df):\n",
    "        plt.figure(figsize=(14, 7))\n",
    "        plt.plot(self.historical_data['date'], self.historical_data['demand'], label='Historical Data', color='blue')\n",
    "        plt.plot(forecast_df['date'], forecast_df['forecast'], label='Forecast with Impacts', color='orange')\n",
    "        plt.xlabel('Date')\n",
    "        plt.ylabel('Daily Receipts')\n",
    "        plt.title('LPA Daily Receipts Forecast with Uncertainty and Short-term Drivers')\n",
    "        plt.legend()\n",
    "        plt.show()\n",
    "\n",
    "# Sample historical data\n",
    "historical_data = count_unique_receipts_daily[['receiptdate', 'daily_demand']].rename(columns={'receiptdate': 'date', 'daily_demand': 'demand'})\n",
    "\n",
    "# Instantiate the ForecastingModel class\n",
    "forecast_model = ForecastingModel(historical_data)\n",
    "\n",
    "# Remove weekends and holidays from historical data\n",
    "historical_data = forecast_model.remove_weekends_and_holidays(historical_data)\n",
    "\n",
    "# Generate short-term forecast\n",
    "short_term_forecast_df = forecast_model.generate_forecast(num_days=365, short_term=True)\n",
    "\n",
    "# Generate long-term forecast (without short-term impacts)\n",
    "long_term_forecast_df = forecast_model.generate_forecast(num_days=365, short_term=False)\n",
    "\n",
    "# Calculate annual total receipts for short-term forecast\n",
    "annual_total_receipts_short_term = forecast_model.calculate_annual_total_receipts(short_term_forecast_df)\n",
    "\n",
    "# Estimate number of donors for short-term forecast\n",
    "number_of_donors_short_term = forecast_model.estimate_number_of_donors(annual_total_receipts_short_term)\n",
    "\n",
    "# Calculate age-specific forecasts for short-term forecast\n",
    "age_specific_forecast_short_term = forecast_model.calculate_age_specific_forecast(number_of_donors_short_term)\n",
    "\n",
    "# Calculate annual total receipts for long-term forecast\n",
    "annual_total_receipts_long_term = forecast_model.calculate_annual_total_receipts(long_term_forecast_df)\n",
    "\n",
    "# Estimate number of donors for long-term forecast\n",
    "number_of_donors_long_term = forecast_model.estimate_number_of_donors(annual_total_receipts_long_term)\n",
    "\n",
    "# Calculate age-specific forecasts for long-term forecast\n",
    "age_specific_forecast_long_term = forecast_model.calculate_age_specific_forecast(number_of_donors_long_term)\n",
    "\n",
    "# Plotting the short-term forecast\n",
    "forecast_model.plot_forecast(short_term_forecast_df)\n",
    "\n",
    "# Plotting the long-term forecast\n",
    "forecast_model.plot_forecast(long_term_forecast_df)\n",
    "\n",
    "# Save the short-term forecast data\n",
    "short_term_forecast_df.to_csv('lpa_short_term_forecast.csv', index=False)\n",
    "\n",
    "# Save the long-term forecast data\n",
    "long_term_forecast_df.to_csv('lpa_long_term_forecast.csv', index=False)\n",
    "\n",
    "# Print annual total receipts, number of donors, and age-specific forecasts\n",
    "print(\"Annual total receipts (short-term):\", annual_total_receipts_short_term)\n",
    "print(\"Number of donors (short-term):\", number_of_donors_short_term)\n",
    "print(\"Age-specific forecast (short-term):\", age_specific_forecast_short_term)\n",
    "\n",
    "print(\"Annual total receipts (long-term):\", annual_total_receipts_long_term)\n",
    "print(\"Number of donors (long-term):\", number_of_donors_long_term)\n",
    "print(\"Age-specific forecast (long-term):\", age_specific_forecast_long_term)\n",
    "# Explanation\n",
    "# Class Definition:\n",
    "\n",
    "# Added working_days_per_year and receipts_to_donors_ratio attributes to the ForecastingModel class.\n",
    "# Added calculate_annual_total_receipts, estimate_number_of_donors, and calculate_age_specific_forecast methods.\n",
    "# Calculations:\n",
    "\n",
    "# calculate_annual_total_receipts: Calculates the annual total receipts by multiplying the average daily receipts by the number of working days in the year.\n",
    "# estimate_number_of_donors: Estimates the number of donors based on the annual total receipts and the receipts-to-donors ratio.\n",
    "# calculate_age_specific_forecast: Converts the donor estimate into age-specific estimates based on the distribution by age.\n",
    "# Execution:\n",
    "\n",
    "# Generate short-term and long-term forecasts.\n",
    "# Calculate the annual total receipts and the number of donors for both short-term and long-term forecasts.\n",
    "# Calculate and print the age-specific forecasts for both short-term and long-term.\n",
    "# This updated script includes the calculations to convert the receipts forecast to an annual total, estimate the number of donors, and convert the donor estimate into age-specific estimates based on the distribution by age."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19a383ca-cf7f-4636-bdb3-f2fc6d0c91bb",
   "metadata": {},
   "source": [
    "# implement the above Python code in Excel, \n",
    "create the Excel worksheet, and upload the above Excel model for me here? \n",
    "by creating a drop down list with the average daily receipts of LPA application demands in 2024 in the range of 4000–7000 in increments of 100. \n",
    "Then this should be used as an estimate to apply uncertainty and be converted into an age-specific annual donor forecast?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "939b27f9-7d99-4b67-9f8d-92c3f1fcd448",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from openpyxl import Workbook\n",
    "from openpyxl.utils.dataframe import dataframe_to_rows\n",
    "from openpyxl.worksheet.datavalidation import DataValidation\n",
    "from datetime import timedelta\n",
    "import holidays\n",
    "\n",
    "# Initialize the workbook and worksheet\n",
    "wb = Workbook()\n",
    "ws = wb.active\n",
    "ws.title = \"LPA Forecast Model\"\n",
    "\n",
    "# Create a dropdown list for average daily receipts\n",
    "dv = DataValidation(type=\"list\", formula1='\"4000,4100,4200,4300,4400,4500,4600,4700,4800,4900,5000,5100,5200,5300,5400,5500,5600,5700,5800,5900,6000,6100,6200,6300,6400,6500,6600,6700,6800,6900,7000\"', showDropDown=True)\n",
    "ws.add_data_validation(dv)\n",
    "ws['A1'] = \"Average Daily Receipts\"\n",
    "ws['B1'] = 5600  # Default value\n",
    "dv.add(ws['B1'])\n",
    "\n",
    "# Setup headers for the historical data\n",
    "ws.append([\"Date\", \"Demand\"])\n",
    "# Generate sample historical data\n",
    "np.random.seed(42)\n",
    "dates = pd.date_range(start='2020-01-01', end='2023-12-31', freq='D')\n",
    "historical_data = pd.DataFrame({\n",
    "    'date': dates,\n",
    "    'demand': np.random.poisson(lam=100, size=len(dates))  # Simulating daily demands\n",
    "})\n",
    "\n",
    "# Add historical data to worksheet\n",
    "for r in dataframe_to_rows(historical_data, index=False, header=False):\n",
    "    ws.append(r)\n",
    "\n",
    "# Calculate and add working days for forecast\n",
    "forecast_dates = pd.date_range(start='2024-01-01', end='2024-12-31', freq='D')\n",
    "forecast_data = pd.DataFrame({'date': forecast_dates})\n",
    "forecast_data['weekday'] = forecast_data['date'].dt.weekday\n",
    "forecast_data['is_holiday'] = forecast_data['date'].astype(str).isin(holidays.UnitedKingdom(years=2024))\n",
    "forecast_data['working_day'] = (forecast_data['weekday'] < 5) & (~forecast_data['is_holiday'])\n",
    "working_days_2024 = forecast_data['working_day'].sum()\n",
    "\n",
    "# Insert forecast model structure and calculations in Excel\n",
    "ws['A10'] = \"Forecast Parameters\"\n",
    "ws['A11'] = \"Post-COVID Impact\"\n",
    "ws['B11'] = 1.1\n",
    "ws['A12'] = \"Advertisement Impact\"\n",
    "ws['B12'] = 1.2\n",
    "\n",
    "ws['A14'] = \"Working Days in 2024\"\n",
    "ws['B14'] = working_days_2024\n",
    "\n",
    "ws['A16'] = \"Receipts to Donors Ratio\"\n",
    "ws['B16'] = 0.75\n",
    "\n",
    "ws['A18'] = \"Age Group\"\n",
    "ws['B18'] = \"Proportion\"\n",
    "ws['A19'] = \"50-59\"\n",
    "ws['B19'] = 0.25\n",
    "ws['A20'] = \"60-69\"\n",
    "ws['B20'] = 0.30\n",
    "ws['A21'] = \"70+\"\n",
    "ws['B21'] = 0.45\n",
    "\n",
    "ws['A23'] = \"Estimated Average Daily Receipts\"\n",
    "ws['B23'] = \"=B1\"\n",
    "\n",
    "ws['A24'] = \"Annual Total Receipts\"\n",
    "ws['B24'] = \"=B23 * B14\"\n",
    "\n",
    "ws['A25'] = \"Estimated Number of Donors\"\n",
    "ws['B25'] = \"=B24 * B16\"\n",
    "\n",
    "ws['A27'] = \"Age-Specific Annual Donor Forecast\"\n",
    "ws['A28'] = \"50-59\"\n",
    "ws['B28'] = \"=B25 * B19\"\n",
    "ws['A29'] = \"60-69\"\n",
    "ws['B29'] = \"=B25 * B20\"\n",
    "ws['A30'] = \"70+\"\n",
    "ws['B30'] = \"=B25 * B21\"\n",
    "\n",
    "# Save the workbook to a file\n",
    "file_path = '/mnt/data/LPA_Forecast_Model.xlsx'\n",
    "wb.save(file_path)\n",
    "file_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e42e55f-3489-4f49-b00d-fdfffb32dd23",
   "metadata": {},
   "outputs": [],
   "source": [
    "Step-by-Step Guide for Excel Model\n",
    "1. Historical Data\n",
    "Create a Data Sheet:\n",
    "Name a sheet Historical Data.\n",
    "In column A, input your historical dates.\n",
    "In column B, input your historical daily demand.\n",
    "2. Remove Weekends and Holidays\n",
    "Create a Holiday List:\n",
    "\n",
    "Name a sheet Holidays.\n",
    "List UK holidays in column A.\n",
    "Add Formulas to Check for Weekends and Holidays:\n",
    "\n",
    "In Historical Data, add columns for Weekday and IsHoliday.\n",
    "Weekday: Use =WEEKDAY(A2, 2) to get the weekday number (1 for Monday, 7 for Sunday).\n",
    "IsHoliday: Use =IF(COUNTIF(Holidays!$A$2:$A$100, A2) > 0, TRUE, FALSE) to check if the date is a holiday.\n",
    "Filter out weekends and holidays: Use =AND(B2<>6, B2<>7, NOT(C2)) to filter only working days.\n",
    "3. Calculate Average and Standard Deviation\n",
    "Calculate Statistics:\n",
    "Use =AVERAGEIFS(B2:B1000, C2:C1000, TRUE) to calculate the mean of working day demands.\n",
    "Use =STDEV.P(IF(D2:D1000, B2:B1000)) to calculate the standard deviation, using an array formula.\n",
    "4. Naïve Extrapolation with Uncertainty\n",
    "Create a Forecast Sheet:\n",
    "\n",
    "Name a sheet Forecast.\n",
    "In column A, list future dates for the forecast period.\n",
    "Use a formula like =A2 + 1 to generate sequential dates.\n",
    "Generate Naïve Forecast:\n",
    "\n",
    "In column B, use the average calculated earlier as the base forecast.\n",
    "Add random noise to the forecast to introduce uncertainty using =NORMINV(RAND(), $B$1, $C$1), where $B$1 is the mean and $C$1 is the standard deviation.\n",
    "5. Apply Short-Term Impacts\n",
    "Post-COVID and Advertisement Impact:\n",
    "Use two separate columns to apply impacts.\n",
    "Post-COVID Impact: Multiply the base forecast by a factor, e.g., =B2 * 1.1.\n",
    "Advertisement Impact: Multiply the post-COVID forecast by another factor, e.g., =C2 * 1.2.\n",
    "6. Combine Impacts and Filter Weekends and Holidays\n",
    "Combined Impact Forecast:\n",
    "\n",
    "Combine impacts in a new column: =D2 * E2.\n",
    "Filter out Weekends and Holidays:\n",
    "\n",
    "Repeat the steps used in the historical data to remove weekends and holidays from the forecast.\n",
    "7. Quarterly Updates\n",
    "Create Quarterly Forecasts:\n",
    "Use separate sections in the Forecast sheet for each quarter.\n",
    "Update the base forecast using the last observed demand.\n",
    "Excel Implementation Example\n",
    "Here is an example layout of the Excel sheet with formulas.\n",
    "\n",
    "Historical Data Sheet\n",
    "Date\tDemand\tWeekday\tIsHoliday\tWorkingDay\n",
    "01/01/2020\t100\t=WEEKDAY(A2, 2)\t=IF(COUNTIF(Holidays!$A$2:$A$100, A2) > 0, TRUE, FALSE)\t=AND(B2<>6, B2<>7, NOT(C2))\n",
    "02/01/2020\t120\t...\t...\t...\n",
    "...\t...\t...\t...\t...\n",
    "Forecast Sheet\n",
    "Date\tBase Forecast\tPost-COVID Impact\tAdvertisement Impact\tCombined Impact\tWeekday\tIsHoliday\tWorkingDay\n",
    "01/01/2024\t=AVERAGE($B$2:$B$1000)\t=B2 * 1.1\t=C2 * 1.2\t=D2 * E2\t=WEEKDAY(A2, 2)\t=IF(COUNTIF(Holidays!$A$2:$A$100, A2) > 0, TRUE, FALSE)\t=AND(F2<>6, F2<>7, NOT(G2))\n",
    "02/01/2024\t=NORMINV(RAND(), $B$1, $C$1)\t...\t...\t...\t...\t...\t...\n",
    "...\t...\t...\t...\t...\t...\t...\t...\n",
    "Tips\n",
    "Use Excel's Data Analysis Toolpak to assist with statistical functions if needed.\n",
    "Create dynamic named ranges for holidays to easily update the list.\n",
    "Use Excel's conditional formatting and filters to highlight weekends and holidays.\n",
    "By following these steps and formulas, you can build an Excel model that captures the essence of the Python script, including handling uncertainty and applying short-term impacts on your forecasting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95bc06a6-5d35-4c55-9b61-ea311570cb4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import statsmodels.api as sm\n",
    "from statsmodels.tsa.arima.model import ARIMA\n",
    "\n",
    "# Load your LPA demand data (replace with your actual data)\n",
    "# Assume 'daily_demand' is a pandas Series with daily demand values\n",
    "\n",
    "# Short-term forecasting (Naïve Extrapolation)\n",
    "naive_forecast = daily_demand.shift(1)\n",
    "\n",
    "# Calculate prediction intervals (adjust alpha as needed)\n",
    "model = ARIMA(daily_demand, order=(1, 1, 1))\n",
    "model_fit = model.fit()\n",
    "forecast, stderr, conf_int = model_fit.forecast(steps=1, alpha=0.05)\n",
    "\n",
    "# Incorporate advertising impact (adjust as needed)\n",
    "# Example: Multiply forecast by advertising factor (low, medium, high)\n",
    "\n",
    "# Reflect post-COVID-19 impact (adjust as needed)\n",
    "# Example: Adjust forecast based on historical COVID-19 data\n",
    "\n",
    "# Long-term age-specific model (create your own)\n",
    "# Apply age-specific factors to overall forecast\n",
    "\n",
    "# Quarterly updates (revisit and adjust)\n",
    "\n",
    "# Print results\n",
    "print(\"Naïve Forecast for t+1:\", naive_forecast.iloc[-1])\n",
    "print(\"Forecast for t+1:\", forecast[0])\n",
    "print(\"Prediction Interval (95%):\", conf_int[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a45a2ef-770b-442e-803d-9ec86f2aebb0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "960bec51-a9dd-426a-8c3c-ca240292d4da",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.stats import norm\n",
    "import holidays\n",
    "\n",
    "class ForecastingModel:\n",
    "    def __init__(self, historical_data, post_covid_impact=1.1, advertisement_impact=1.2):\n",
    "        self.historical_data = historical_data\n",
    "        self.post_covid_impact = post_covid_impact\n",
    "        self.advertisement_impact = advertisement_impact\n",
    "        self.uk_holidays = holidays.UnitedKingdom()\n",
    "\n",
    "    def remove_weekends_and_holidays(self, data):\n",
    "        data['weekday'] = data['date'].dt.weekday\n",
    "        data['is_holiday'] = data['date'].isin(self.uk_holidays)\n",
    "        return data[(data['weekday'] < 5) & (~data['is_holiday'])]\n",
    "\n",
    "    def calculate_daily_demand_stats(self, data):\n",
    "        mean_demand = data['demand'].mean()\n",
    "        std_dev_demand = data['demand'].std()\n",
    "        return mean_demand, std_dev_demand\n",
    "\n",
    "    def naive_extrapolation_with_uncertainty(self, last_observed, std_dev, num_days=90):\n",
    "        forecast = np.random.normal(loc=last_observed, scale=std_dev, size=num_days)\n",
    "        return forecast\n",
    "\n",
    "    def apply_scenario_analysis(self, base_forecast, impact_factor):\n",
    "        return base_forecast * impact_factor\n",
    "\n",
    "    def update_quarterly_forecast(self, data, num_quarters=4):\n",
    "        forecasts = []\n",
    "        for _ in range(num_quarters):\n",
    "            mean_demand, std_dev_demand = self.calculate_daily_demand_stats(data)\n",
    "            last_observed = data['demand'].iloc[-1]\n",
    "            quarterly_forecast = self.naive_extrapolation_with_uncertainty(last_observed, std_dev_demand)\n",
    "            \n",
    "            new_dates = pd.date_range(start=data['date'].iloc[-1] + pd.Timedelta(days=1), periods=len(quarterly_forecast))\n",
    "            new_data = pd.DataFrame({'date': new_dates, 'demand': quarterly_forecast})\n",
    "            new_data = self.remove_weekends_and_holidays(new_data)\n",
    "            \n",
    "            forecasts.extend(new_data['demand'])\n",
    "            data = pd.concat([data, new_data], ignore_index=True)\n",
    "        return data\n",
    "\n",
    "    def generate_forecast(self, num_days=365, short_term=True):\n",
    "        mean_demand, std_dev_demand = self.calculate_daily_demand_stats(self.historical_data)\n",
    "        base_forecast = self.naive_extrapolation_with_uncertainty(mean_demand, std_dev_demand, num_days)\n",
    "        \n",
    "        if short_term:\n",
    "            forecast_post_covid = self.apply_scenario_analysis(base_forecast, self.post_covid_impact)\n",
    "            combined_forecast = self.apply_scenario_analysis(forecast_post_covid, self.advertisement_impact)\n",
    "        else:\n",
    "            combined_forecast = base_forecast\n",
    "        \n",
    "        forecast_dates = pd.date_range(start=self.historical_data['date'].iloc[-1] + pd.Timedelta(days=1), periods=num_days)\n",
    "        forecast_df = pd.DataFrame({'date': forecast_dates, 'forecast': combined_forecast})\n",
    "        forecast_df = self.remove_weekends_and_holidays(forecast_df)\n",
    "        return forecast_df\n",
    "\n",
    "    def plot_forecast(self, forecast_df):\n",
    "        plt.figure(figsize=(14, 7))\n",
    "        plt.plot(self.historical_data['date'], self.historical_data['demand'], label='Historical Data', color='blue')\n",
    "        plt.plot(forecast_df['date'], forecast_df['forecast'], label='Forecast with Impacts', color='orange')\n",
    "        plt.xlabel('Date')\n",
    "        plt.ylabel('Daily Receipts')\n",
    "        plt.title('LPA Daily Receipts Forecast with Uncertainty and Short-term Drivers')\n",
    "        plt.legend()\n",
    "        plt.show()\n",
    "\n",
    "# Sample historical data\n",
    "np.random.seed(42)\n",
    "dates = pd.date_range(start='2020-01-01', end='2023-12-31', freq='D')\n",
    "historical_data = pd.DataFrame({\n",
    "    'date': dates,\n",
    "    'demand': np.random.poisson(lam=100, size=len(dates))  # Simulating daily demands\n",
    "})\n",
    "\n",
    "# Instantiate the ForecastingModel class\n",
    "forecast_model = ForecastingModel(historical_data)\n",
    "\n",
    "# Remove weekends and holidays from historical data\n",
    "historical_data = forecast_model.remove_weekends_and_holidays(historical_data)\n",
    "\n",
    "# Generate short-term forecast\n",
    "short_term_forecast_df = forecast_model.generate_forecast(num_days=365, short_term=True)\n",
    "\n",
    "# Generate long-term forecast (without short-term impacts)\n",
    "long_term_forecast_df = forecast_model.generate_forecast(num_days=365, short_term=False)\n",
    "\n",
    "# Plotting the short-term forecast\n",
    "forecast_model.plot_forecast(short_term_forecast_df)\n",
    "\n",
    "# Plotting the long-term forecast\n",
    "forecast_model.plot_forecast(long_term_forecast_df)\n",
    "\n",
    "# Save the short-term forecast data\n",
    "short_term_forecast_df.to_csv('lpa_short_term_forecast.csv', index=False)\n",
    "\n",
    "# Save the long-term forecast data\n",
    "long_term_forecast_df.to_csv('lpa_long_term_forecast.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db530cfa-5968-4c6b-866e-4f15154ff771",
   "metadata": {},
   "outputs": [],
   "source": [
    "Explanation\n",
    "Class Definition:\n",
    "\n",
    "ForecastingModel: Encapsulates all forecasting-related methods and attributes.\n",
    "__init__: Initializes the class with historical data and impact factors for short-term scenarios.\n",
    "remove_weekends_and_holidays: Removes weekends and holidays from the data.\n",
    "calculate_daily_demand_stats: Computes mean and standard deviation of demand.\n",
    "naive_extrapolation_with_uncertainty: Generates a Naïve forecast with uncertainty.\n",
    "apply_scenario_analysis: Applies short-term impact factors to the base forecast.\n",
    "update_quarterly_forecast: Updates the forecast quarterly and removes weekends/holidays.\n",
    "generate_forecast: Generates a forecast for a specified number of days, with optional short-term impacts.\n",
    "plot_forecast: Plots the historical and forecasted data.\n",
    "Data Simulation and Forecasting:\n",
    "\n",
    "Generates sample historical data.\n",
    "Instantiates the ForecastingModel class.\n",
    "Removes weekends and holidays from the historical data.\n",
    "Generates both short-term and long-term forecasts.\n",
    "Plots the forecasts.\n",
    "Saves the forecast data to CSV files.\n",
    "This OOP-based approach allows you to easily extend and customize the forecasting model, and to generate and plot forecasts for different scenarios (short-term and long-term) in a structured manner."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55fb3368-4eb7-4c6c-91de-f9aa3060b8e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.stats import norm\n",
    "\n",
    "# Generate sample historical data\n",
    "np.random.seed(42)\n",
    "dates = pd.date_range(start='2020-01-01', end='2023-12-31', freq='D')\n",
    "historical_data = pd.DataFrame({\n",
    "    'date': dates,\n",
    "    'demand': np.random.poisson(lam=100, size=len(dates))  # Simulating daily demands\n",
    "})\n",
    "\n",
    "# Function to calculate average daily demand and uncertainty\n",
    "def calculate_daily_demand_stats(data):\n",
    "    mean_demand = data['demand'].mean()\n",
    "    std_dev_demand = data['demand'].std()\n",
    "    return mean_demand, std_dev_demand\n",
    "\n",
    "# Function to perform Naïve extrapolation with uncertainty\n",
    "def naive_extrapolation_with_uncertainty(last_observed, std_dev, num_days=90):\n",
    "    forecast = np.random.normal(loc=last_observed, scale=std_dev, size=num_days)\n",
    "    return forecast\n",
    "\n",
    "# Function to update quarterly forecasts\n",
    "def update_quarterly_forecast(data, num_quarters=4):\n",
    "    forecasts = []\n",
    "    for _ in range(num_quarters):\n",
    "        mean_demand, std_dev_demand = calculate_daily_demand_stats(data)\n",
    "        last_observed = data['demand'].iloc[-1]\n",
    "        quarterly_forecast = naive_extrapolation_with_uncertainty(last_observed, std_dev_demand)\n",
    "        forecasts.extend(quarterly_forecast)\n",
    "        new_dates = pd.date_range(start=data['date'].iloc[-1] + pd.Timedelta(days=1), periods=len(quarterly_forecast))\n",
    "        new_data = pd.DataFrame({'date': new_dates, 'demand': quarterly_forecast})\n",
    "        data = pd.concat([data, new_data], ignore_index=True)\n",
    "    return data\n",
    "\n",
    "# Function to apply scenario analysis\n",
    "def apply_scenario_analysis(base_forecast, impact_factor):\n",
    "    return base_forecast * impact_factor\n",
    "\n",
    "# Apply short-term drivers (post-COVID and advertisement impacts)\n",
    "post_covid_impact = 1.1  # 10% increase\n",
    "advertisement_impact = 1.2  # 20% increase\n",
    "\n",
    "# Generate base forecast\n",
    "mean_demand, std_dev_demand = calculate_daily_demand_stats(historical_data)\n",
    "base_forecast = naive_extrapolation_with_uncertainty(mean_demand, std_dev_demand, num_days=365)\n",
    "\n",
    "# Apply scenario impacts\n",
    "forecast_post_covid = apply_scenario_analysis(base_forecast, post_covid_impact)\n",
    "forecast_advertisement = apply_scenario_analysis(base_forecast, advertisement_impact)\n",
    "\n",
    "# Combine impacts\n",
    "combined_forecast = apply_scenario_analysis(forecast_post_covid, advertisement_impact)\n",
    "\n",
    "# Update quarterly forecasts\n",
    "updated_forecast_data = update_quarterly_forecast(historical_data)\n",
    "\n",
    "# Plotting the results\n",
    "plt.figure(figsize=(14, 7))\n",
    "plt.plot(historical_data['date'], historical_data['demand'], label='Historical Data', color='blue')\n",
    "plt.plot(pd.date_range(start=historical_data['date'].iloc[-1] + pd.Timedelta(days=1), periods=365), \n",
    "         combined_forecast, label='Forecast with Impacts', color='orange')\n",
    "plt.xlabel('Date')\n",
    "plt.ylabel('Daily Receipts')\n",
    "plt.title('LPA Daily Receipts Forecast with Uncertainty and Short-term Drivers')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "print(\"Forecast with combined impacts for the next year:\")\n",
    "print(combined_forecast)\n",
    "\n",
    "# Save the forecast data\n",
    "forecast_dates = pd.date_range(start=historical_data['date'].iloc[-1] + pd.Timedelta(days=1), periods=365)\n",
    "forecast_df = pd.DataFrame({'date': forecast_dates, 'forecast': combined_forecast})\n",
    "forecast_df.to_csv('lpa_forecast.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b7f3282-a186-42ec-a232-bdd401c30b6d",
   "metadata": {},
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.stats import norm\n",
    "import holidays\n",
    "\n",
    "# Generate sample historical data\n",
    "np.random.seed(42)\n",
    "dates = pd.date_range(start='2020-01-01', end='2023-12-31', freq='D')\n",
    "historical_data = pd.DataFrame({\n",
    "    'date': dates,\n",
    "    'demand': np.random.poisson(lam=100, size=len(dates))  # Simulating daily demands\n",
    "})\n",
    "\n",
    "# UK holidays\n",
    "uk_holidays = holidays.UnitedKingdom()\n",
    "\n",
    "# Function to remove weekends and holidays\n",
    "def remove_weekends_and_holidays(data):\n",
    "    data['weekday'] = data['date'].dt.weekday\n",
    "    data['is_holiday'] = data['date'].isin(uk_holidays)\n",
    "    return data[(data['weekday'] < 5) & (~data['is_holiday'])]\n",
    "\n",
    "# Remove weekends and holidays from historical data\n",
    "historical_data = remove_weekends_and_holidays(historical_data)\n",
    "\n",
    "# Function to calculate average daily demand and uncertainty\n",
    "def calculate_daily_demand_stats(data):\n",
    "    mean_demand = data['demand'].mean()\n",
    "    std_dev_demand = data['demand'].std()\n",
    "    return mean_demand, std_dev_demand\n",
    "\n",
    "# Function to perform Naïve extrapolation with uncertainty\n",
    "def naive_extrapolation_with_uncertainty(last_observed, std_dev, num_days=90):\n",
    "    forecast = np.random.normal(loc=last_observed, scale=std_dev, size=num_days)\n",
    "    return forecast\n",
    "\n",
    "# Function to update quarterly forecasts\n",
    "def update_quarterly_forecast(data, num_quarters=4):\n",
    "    forecasts = []\n",
    "    for _ in range(num_quarters):\n",
    "        mean_demand, std_dev_demand = calculate_daily_demand_stats(data)\n",
    "        last_observed = data['demand'].iloc[-1]\n",
    "        quarterly_forecast = naive_extrapolation_with_uncertainty(last_observed, std_dev_demand)\n",
    "        \n",
    "        # Generate new dates for the forecast period\n",
    "        new_dates = pd.date_range(start=data['date'].iloc[-1] + pd.Timedelta(days=1), periods=len(quarterly_forecast))\n",
    "        new_data = pd.DataFrame({'date': new_dates, 'demand': quarterly_forecast})\n",
    "        \n",
    "        # Remove weekends and holidays from new data\n",
    "        new_data = remove_weekends_and_holidays(new_data)\n",
    "        \n",
    "        forecasts.extend(new_data['demand'])\n",
    "        data = pd.concat([data, new_data], ignore_index=True)\n",
    "    return data\n",
    "\n",
    "# Function to apply scenario analysis\n",
    "def apply_scenario_analysis(base_forecast, impact_factor):\n",
    "    return base_forecast * impact_factor\n",
    "\n",
    "# Apply short-term drivers (post-COVID and advertisement impacts)\n",
    "post_covid_impact = 1.1  # 10% increase\n",
    "advertisement_impact = 1.2  # 20% increase\n",
    "\n",
    "# Generate base forecast\n",
    "mean_demand, std_dev_demand = calculate_daily_demand_stats(historical_data)\n",
    "base_forecast = naive_extrapolation_with_uncertainty(mean_demand, std_dev_demand, num_days=365)\n",
    "\n",
    "# Apply scenario impacts\n",
    "forecast_post_covid = apply_scenario_analysis(base_forecast, post_covid_impact)\n",
    "forecast_advertisement = apply_scenario_analysis(base_forecast, advertisement_impact)\n",
    "\n",
    "# Combine impacts\n",
    "combined_forecast = apply_scenario_analysis(forecast_post_covid, advertisement_impact)\n",
    "\n",
    "# Update quarterly forecasts\n",
    "updated_forecast_data = update_quarterly_forecast(historical_data)\n",
    "\n",
    "# Plotting the results\n",
    "plt.figure(figsize=(14, 7))\n",
    "plt.plot(historical_data['date'], historical_data['demand'], label='Historical Data', color='blue')\n",
    "plt.plot(pd.date_range(start=historical_data['date'].iloc[-1] + pd.Timedelta(days=1), periods=len(combined_forecast)), \n",
    "         combined_forecast, label='Forecast with Impacts', color='orange')\n",
    "plt.xlabel('Date')\n",
    "plt.ylabel('Daily Receipts')\n",
    "plt.title('LPA Daily Receipts Forecast with Uncertainty and Short-term Drivers')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "print(\"Forecast with combined impacts for the next year:\")\n",
    "print(combined_forecast)\n",
    "\n",
    "# Save the forecast data\n",
    "forecast_dates = pd.date_range(start=historical_data['date'].iloc[-1] + pd.Timedelta(days=1), periods=len(combined_forecast))\n",
    "forecast_df = pd.DataFrame({'date': forecast_dates, 'forecast': combined_forecast})\n",
    "forecast_df = remove_weekends_and_holidays(forecast_df)\n",
    "forecast_df.to_csv('lpa_forecast.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9487d5c-0430-4a8f-9db5-92eddf6ff043",
   "metadata": {},
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.stats import norm\n",
    "import holidays\n",
    "\n",
    "# Generate sample historical data\n",
    "np.random.seed(42)\n",
    "dates = pd.date_range(start='2020-01-01', end='2023-12-31', freq='D')\n",
    "historical_data = pd.DataFrame({\n",
    "    'date': dates,\n",
    "    'demand': np.random.poisson(lam=100, size=len(dates))  # Simulating daily demands\n",
    "})\n",
    "\n",
    "# UK holidays\n",
    "uk_holidays = holidays.UnitedKingdom()\n",
    "\n",
    "# Function to remove weekends and holidays\n",
    "def remove_weekends_and_holidays(data):\n",
    "    data['weekday'] = data['date'].dt.weekday\n",
    "    data['is_holiday'] = data['date'].isin(uk_holidays)\n",
    "    return data[(data['weekday'] < 5) & (~data['is_holiday'])]\n",
    "\n",
    "# Remove weekends and holidays from historical data\n",
    "historical_data = remove_weekends_and_holidays(historical_data)\n",
    "\n",
    "# Function to calculate average daily demand and uncertainty\n",
    "def calculate_daily_demand_stats(data):\n",
    "    mean_demand = data['demand'].mean()\n",
    "    std_dev_demand = data['demand'].std()\n",
    "    return mean_demand, std_dev_demand\n",
    "\n",
    "# Function to perform Naïve extrapolation with uncertainty\n",
    "def naive_extrapolation_with_uncertainty(last_observed, std_dev, num_days=90):\n",
    "    forecast = np.random.normal(loc=last_observed, scale=std_dev, size=num_days)\n",
    "    return forecast\n",
    "\n",
    "# Function to update quarterly forecasts\n",
    "def update_quarterly_forecast(data, num_quarters=4):\n",
    "    forecasts = []\n",
    "    for _ in range(num_quarters):\n",
    "        mean_demand, std_dev_demand = calculate_daily_demand_stats(data)\n",
    "        last_observed = data['demand'].iloc[-1]\n",
    "        quarterly_forecast = naive_extrapolation_with_uncertainty(last_observed, std_dev_demand)\n",
    "        \n",
    "        # Generate new dates for the forecast period\n",
    "        new_dates = pd.date_range(start=data['date'].iloc[-1] + pd.Timedelta(days=1), periods=len(quarterly_forecast))\n",
    "        new_data = pd.DataFrame({'date': new_dates, 'demand': quarterly_forecast})\n",
    "        \n",
    "        # Remove weekends and holidays from new data\n",
    "        new_data = remove_weekends_and_holidays(new_data)\n",
    "        \n",
    "        forecasts.extend(new_data['demand'])\n",
    "        data = pd.concat([data, new_data], ignore_index=True)\n",
    "    return data\n",
    "\n",
    "# Function to apply scenario analysis\n",
    "def apply_scenario_analysis(base_forecast, impact_factor):\n",
    "    return base_forecast * impact_factor\n",
    "\n",
    "# Apply short-term drivers (post-COVID and advertisement impacts)\n",
    "post_covid_impact = 1.1  # 10% increase\n",
    "advertisement_impact = 1.2  # 20% increase\n",
    "\n",
    "# Generate base forecast\n",
    "mean_demand, std_dev_demand = calculate_daily_demand_stats(historical_data)\n",
    "base_forecast = naive_extrapolation_with_uncertainty(mean_demand, std_dev_demand, num_days=365)\n",
    "\n",
    "# Apply scenario impacts\n",
    "forecast_post_covid = apply_scenario_analysis(base_forecast, post_covid_impact)\n",
    "forecast_advertisement = apply_scenario_analysis(base_forecast, advertisement_impact)\n",
    "\n",
    "# Combine impacts\n",
    "combined_forecast = apply_scenario_analysis(forecast_post_covid, advertisement_impact)\n",
    "\n",
    "# Update quarterly forecasts\n",
    "updated_forecast_data = update_quarterly_forecast(historical_data)\n",
    "\n",
    "# Plotting the results\n",
    "plt.figure(figsize=(14, 7))\n",
    "plt.plot(historical_data['date'], historical_data['demand'], label='Historical Data', color='blue')\n",
    "plt.plot(pd.date_range(start=historical_data['date'].iloc[-1] + pd.Timedelta(days=1), periods=len(combined_forecast)), \n",
    "         combined_forecast, label='Forecast with Impacts', color='orange')\n",
    "plt.xlabel('Date')\n",
    "plt.ylabel('Daily Receipts')\n",
    "plt.title('LPA Daily Receipts Forecast with Uncertainty and Short-term Drivers')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "print(\"Forecast with combined impacts for the next year:\")\n",
    "print(combined_forecast)\n",
    "\n",
    "# Save the forecast data\n",
    "forecast_dates = pd.date_range(start=historical_data['date'].iloc[-1] + pd.Timedelta(days=1), periods=len(combined_forecast))\n",
    "forecast_df = pd.DataFrame({'date': forecast_dates, 'forecast': combined_forecast})\n",
    "forecast_df = remove_weekends_and_holidays(forecast_df)\n",
    "forecast_df.to_csv('lpa_forecast.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e246d0e6-0ba9-479e-916f-91e9da2d3eb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "count_unique_receipts_monthly = count_unique_receipts_daily.groupby(['year', 'month_year'], as_index=False).mean()\n",
    "count_unique_receipts_monthly = count_unique_receipts_monthly.rename(columns={'daily_demand': 'avg_monthly_demand'})\n",
    "\n",
    "# Display the result\n",
    "print(count_unique_receipts_monthly)\n",
    "\n",
    "count_unique_receipts_annual = count_unique_receipts_monthly.groupby(['year'], as_index=False).mean()\n",
    "count_unique_receipts_annual = count_unique_receipts_annual.rename(columns={'avg_monthly_demand': 'avg_annual_demand'})\n",
    "\n",
    "# Sort\n",
    "count_unique_receipts_annual = count_unique_receipts_annual.sort_values(by=['year'])\n",
    "\n",
    "# Display the result\n",
    "print(count_unique_receipts_annual)\n",
    "\n",
    "#count_unique_receipts_daily['Overall_Percentage'] = count_unique_receipts_daily.iloc[:, 1].mean(axis=1)\n",
    "\n",
    "# Save the result into a csv file\n",
    "#lpa_reciepts.to_csv(r'lpa_reciepts.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b790d55-dc45-4085-8b7a-197e8ba2c4d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "lpa_actuals = lpa_unique.query('year > 2021')\n",
    "lpa_actuals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef0070df-bdd4-4b38-b6f0-6604d1742370",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Set index\n",
    "lpa_actuals['receiptdate'] = pd.to_datetime(lpa_actuals['receiptdate'])\n",
    "\n",
    "lpa_actuals = lpa_actuals.set_index('receiptdate')#.asfreq('D')\n",
    "\n",
    "print(lpa_actuals.head())\n",
    "print(lpa_actuals.tail())\n",
    "\n",
    "## Select the appropriate variable to be forecasted\n",
    "lpa_actuals_data = lpa_actuals['unique_key']\n",
    "\n",
    "## infer the frequency of the data:\n",
    "#lpa_actuals_data = lpa_actuals_data.asfreq(pd.infer_freq(lpa_actuals_data.index))\n",
    "\n",
    "# lim_divorce_data = divorce_data[start_date:end_date]\n",
    "\n",
    "# start_date_years = datetime.strptime(start_date, \n",
    "#                                      '%Y-%m-%d') + relativedelta(years = 0)\n",
    "# print(start_date_years)\n",
    "\n",
    "# start_date_formatted = start_date_years.date()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "886d04ee-4369-4264-8d47-c56accb21dea",
   "metadata": {},
   "outputs": [],
   "source": [
    "lpa_unique.info()\n",
    "lpa_unique.shape\n",
    "lpa_unique.describe()\n",
    "lpa_unique.corr()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5bcd6356-df32-4a8f-8197-d82756375d7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Visualisation of the time series\n",
    "#Plot the Actual Divorces\n",
    "\n",
    "plt.figure(figsize=(20, 8))\n",
    "plt.plot(lpa_actuals_data)\n",
    "plt.title('UK Actual LPA Data', fontsize=20)\n",
    "plt.ylabel('Demands', fontsize=16)\n",
    "\n",
    "for year in range(2023,2024): #datetime.strptime(end_date, '%Y-%m-%d').year):\n",
    "    plt.axvline(pd.to_datetime(str(year) + '-01-01'), color = 'k', \n",
    "                linestyle='--', alpha = 0.2)\n",
    "    \n",
    "plt.savefig('ActualLPAData.png', dpi=300, bbox_inches='tight')\n",
    "plt.show()  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce045b7c-19b9-4733-98d5-e62d35b95d61",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Average daily receipts for 2024\n",
    "\n",
    "# create a drop down list with average daily receipts of LPA application in 2024 in the range say from 4000 – 7000 in increments of 100. \n",
    "# Then this should be used as an estimate to apply unceratinty and to be converted into an age-specific annual donor forecast.\n",
    "\n",
    "# Filter data to involve Registered status and Post-covid data from 2022 onwards\n",
    "unique_receipts_post_covid = lpa_unique\n",
    "unique_receipts_post_covid[unique_receipts_post_covid['status'].str.contains(\"Registered\")]\n",
    "unique_receipts_post_covid = unique_receipts_post_covid.query('year > 2023')\n",
    "#df[df['Overall_Percentage'].isin([value for value in df['Overall_Percentage'] if value > 60])]\n",
    "#df[df.apply(lambda row: row['Overall_Percentage'] > 55, axis=1)]\n",
    "# # The “loc” method is used to access a group of rows and columns by label(s) or a boolean array. \n",
    "# #We can utilise it to filter a DataFrame based on specific column values.\n",
    "#df.loc[df['Overall_Percentage'] > 40]\n",
    "# # The “iloc” method is similar to “loc” but uses integer-based indexing instead of labels. \n",
    "# #It allows us to filter a DataFrame by specifying the row and column indices.\n",
    "#df[df.iloc[:, -1] > 40]\n",
    "\n",
    "\n",
    "#count_unique_receipts_monthly = unique_receipts_post_covid.groupby(['month_year'])['receiptdate'].agg('count').reset_index()\n",
    "\n",
    "unique_receipts_post_covid = unique_receipts_post_covid.groupby(['receiptdate'])['unique_key'].agg('count').reset_index()\n",
    "\n",
    "#count_unique_receipts_monthly = unique_receipts_post_covid.groupby(['year', 'month_year'])['uid'].agg('count').reset_index()\n",
    "\n",
    "#count_unique_receipts_monthly = unique_receipts_post_covid.groupby(['month_year'])['receiptdate'].nunique().reset_index(name='count')\n",
    "\n",
    "# Calculating the overall percentage for each donor and adding a new column\n",
    "#count_unique_receipts_daily['Overall_Percentage'] = count_unique_receipts_daily.iloc[:, 1:].mean(axis=1)\n",
    "\n",
    "\n",
    "\n",
    "count_unique_receipts_daily = unique_receipts_post_covid.rename(columns={'unique_key': 'daily_count'})\n",
    "\n",
    "count_unique_receipts_daily['avg_daily_count'] = count_unique_receipts_daily['daily_count'].mean()\n",
    "\n",
    "\n",
    "# Extract month letter and year \n",
    "###lpa_reciepts['month_year'] = lpa_reciepts['receiptdate'].dt.strftime('%b-%y')\n",
    "# Calculate the number of unique records by month and year\n",
    "#count_reciepts_month = lpa_reciepts.groupby(['year', 'month_year'])['receiptdate'].nunique().reset_index(name='count')\n",
    "###Count_daily_reciepts = lpa_reciepts.groupby(['receiptdate']).count()\n",
    "\n",
    "#unique_records = df.groupby('unique_key').agg('count').reset_index()  #.groupby(['year'])['unique_key'].nunique().reset_index(name='count')\n",
    "###Count_daily_reciepts = Count_daily_reciepts.rename(columns={\"count\": \"Count_of_daily_reciepts\"})\n",
    "\n",
    "#\n",
    "\n",
    "\n",
    "# Extract month letter and year \n",
    "count_unique_receipts_daily['month_year'] = count_unique_receipts_daily['receiptdate'].dt.strftime('%b-%y')\n",
    "count_unique_receipts_daily['year'] = count_unique_receipts_daily['receiptdate'].dt.strftime('%Y')\n",
    "print(count_unique_receipts_daily)\n",
    "\n",
    "count_unique_receipts_monthly = count_unique_receipts_daily.groupby(['year', 'month_year'], as_index=False).mean()\n",
    "count_unique_receipts_monthly = count_unique_receipts_monthly.rename(columns={'daily_count': 'avg_monthly_count'})\n",
    "\n",
    "# Display the result\n",
    "print(count_unique_receipts_monthly)\n",
    "\n",
    "count_unique_receipts_annual = count_unique_receipts_monthly.groupby(['year'], as_index=False).mean()\n",
    "count_unique_receipts_annual = count_unique_receipts_annual.rename(columns={'avg_monthly_count': 'avg_annual_count'})\n",
    "\n",
    "# Display the result\n",
    "print(count_unique_receipts_annual)\n",
    "\n",
    "#count_unique_receipts_daily['Overall_Percentage'] = count_unique_receipts_daily.iloc[:, 1].mean(axis=1)\n",
    "\n",
    "# Save the result into a csv file\n",
    "#lpa_reciepts.to_csv(r'lpa_reciepts.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b17122b-d651-4845-84c6-fc191e153db2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "f4441107-af2e-41fc-b26e-d97bebbefda1",
   "metadata": {},
   "source": [
    "# Work out how many people applied for lpa and recieved the power of atthorney and how many applications in a year/month/week by age group since 2007?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d91646b3-755c-405d-9cf0-ffbe25a26fee",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1ce0cc6-03b8-46d6-b28c-ce8a13503efc",
   "metadata": {},
   "outputs": [],
   "source": [
    "d = lpa_unique.sort_values(by='unique_key')\n",
    "count_unique_grouped_year = d.groupby(['year'])['unique_key'].nunique().reset_index(name='count')\n",
    "count_unique_grouped_year"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b12b129-dea6-44e6-9dc3-8052e77a37f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "d = lpa_unique.sort_values(by='unique_key')\n",
    "count_unique_grouped_age = d.groupby(['age'])['unique_key'].nunique().reset_index(name='count')\n",
    "count_unique_grouped_age"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33b3f949-855d-4ed5-aa5b-c5aa170cd6f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "g = lpa_unique\n",
    "# Group by 'item' and 'color', then count the occurrences\n",
    "count_unique_grouped_all = g.groupby(['receiptdate', 'uid', 'casesubtype', 'status', 'donor_dob', 'donor_postcode', 'donor_gender', 'age', 'year'])['unique_key'].count().reset_index(name='count')\n",
    "count_unique_grouped_all"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c18be8a5-3357-4f91-9d26-af397dabcc02",
   "metadata": {},
   "outputs": [],
   "source": [
    "g = lpa_unique\n",
    "# Group by 'item' and 'color', then count the occurrences\n",
    "count_unique_grouped_age_year = g.groupby(['age', 'year'])['unique_key'].count().reset_index(name='count')\n",
    "count_unique_grouped_age_year = count_unique_grouped_age_year.rename(columns={\"count\": \"Count_of_CASEID\"})\n",
    "count_unique_grouped_age_year\n",
    "\n",
    "# Save the result into a csv file\n",
    "count_unique_grouped_age_year.to_csv(r'count_unique_grouped_age_year.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d3301ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "d = lpa_unique.sort_values(by='unique_key')\n",
    "count_unique_grouped_year = d.groupby(['year'])['unique_key'].nunique().reset_index(name='count')\n",
    "count_unique_grouped_yearcount_unique_grouped_age_year.to_csv(r'count_unique_grouped_age_year.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8fda9dcf",
   "metadata": {},
   "outputs": [],
   "source": [
    "d = lpa_unique.sort_values(by='unique_key')\n",
    "count_unique_grouped_age = d.groupby(['age'])['unique_key'].nunique().reset_index(name='count')\n",
    "count_unique_grouped_age"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ecbaf322",
   "metadata": {},
   "outputs": [],
   "source": [
    "g = lpa_unique\n",
    "# Group by 'item' and 'color', then count the occurrences\n",
    "count_unique_grouped_all = g.groupby(['receiptdate', 'uid', 'casesubtype', 'status', 'donor_dob', 'donor_postcode', 'donor_gender', 'age', 'year'])['unique_key'].count().reset_index(name='count')\n",
    "count_unique_grouped_all"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09307402",
   "metadata": {},
   "outputs": [],
   "source": [
    "g = lpa_unique\n",
    "# Group by 'item' and 'color', then count the occurrences\n",
    "count_unique_grouped_age_year = g.groupby(['age', 'year'])['unique_key'].count().reset_index(name='count')\n",
    "count_unique_grouped_age_year = count_unique_grouped_age_year.rename(columns={\"count\": \"Count_of_CASEID\"})\n",
    "count_unique_grouped_age_year\n",
    "\n",
    "# Save the result into a csv file\n",
    "count_unique_grouped_age_year.to_csv(r'count_unique_grouped_age_year.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5baac2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract the year of reciept from the receiptdate\n",
    "#receipt_year = int(record[\"receiptdate\"].split(\"-\")[0])\n",
    "\n",
    "# Calculate the number of unique records by age and year\n",
    "count_unique_records = lpa_data_no_index.groupby(['year', 'donor_gender', 'age'])['unique_key'].nunique().reset_index(name='count')\n",
    "#####count_unique_records = lpa_data.reset_index(name='count')\n",
    "#####count_unique_records = lpa_data.reset_index(name='count')\n",
    "\n",
    "\n",
    "#unique_records = df.groupby('unique_key').agg('count').reset_index()  #.groupby(['year'])['unique_key'].nunique().reset_index(name='count')\n",
    "####count_unique_records = count_unique_records.rename(columns={\"count\": \"Count_of_CASEID\"})\n",
    "\n",
    "# Display the result\n",
    "####print(count_unique_records)\n",
    "\n",
    "# Save the result into a csv file\n",
    "#count_unique_records.to_csv(r'count_unique_records.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a6408eb-1305-4ed9-8701-40a6249f2883",
   "metadata": {},
   "source": [
    "# Dermine Whether the application type [casesubtype] is hw=health and welfare or pfa=property and finance"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "066db357-3848-4fdc-86fe-12294383e61a",
   "metadata": {},
   "source": [
    "# How many certificate provider (cp) for each lpa application?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06ebb3ab-4ae6-405f-95de-6c7b259c344a",
   "metadata": {},
   "source": [
    "# Location based data and geographical data for the donor can be used to identify the financial situation and wherether they are located in England or Wales"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6889b3cc-d296-4eed-8270-17ca0c3f9d4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract the year from the receiptdate\n",
    "#receipt_year = birth_year        \n",
    "\n",
    "# Calculate the age of the person\n",
    "#age = receiptdate - birth_year\n",
    "#lpa_df['a'] = \n",
    "############(pd.to_datetime(lpa_df['receiptdate'], errors = 'coerce').dt.day - pd.to_datetime(lpa_df['donor_dob'], errors = 'coerce').dt.day) # / 365.25\n",
    "#lpa_df\n",
    "\n",
    "# Create an Excel writer\n",
    "writer = pd.ExcelWriter('LPA_Data_actuals_Years.xlsx', engine='xlsxwriter')\n",
    "\n",
    "# Iterate through unique years and save data to separate sheets\n",
    "for year in count_unique_grouped_age_year['year'].unique():\n",
    "    year_data = count_unique_grouped_age_year[count_unique_grouped_age_year['year'] == year]\n",
    "    year_data.to_excel(writer, sheet_name=str(year), index=False)\n",
    "\n",
    "\n",
    "# Save the Excel file\n",
    "writer.save()\n",
    "writer.close()  # Close the ExcelWriter\n",
    "\n",
    "year_data\n",
    "# # Iterate through unique years and save data to separate sheets\n",
    "# for year in lpa_df['year'].unique():\n",
    "#     year_data = lpa_df[lpa_df['year'] == year]\n",
    "#     chunk_size = 100000  # Adjust as needed\n",
    "#     num_chunks = len(year_data) // chunk_size + 1\n",
    "#     for i in range(num_chunks):\n",
    "#         start_idx = i * chunk_size\n",
    "#         end_idx = (i + 1) * chunk_size\n",
    "#         chunk_data = lpa_df.iloc[start_idx:end_idx]\n",
    "#         chunk_data.to_excel(writer, sheet_name=f'Sheet{i}', index=False)\n",
    "\n",
    "# # Save the Excel file\n",
    "# writer.save()\n",
    "# writer.close()  # Close the ExcelWriter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1881181d-7089-429b-8237-005d6314f8bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pivot table with count aggregation\n",
    "pivot_table = pd.pivot_table(lpa_unique,\n",
    "                              values='unique_key',\n",
    "                              index='age',\n",
    "                              columns='year',\n",
    "                              aggfunc='count')\n",
    "\n",
    "# Replace NaN values with zeros\n",
    "pivot_table_filled = pivot_table.fillna(0)\n",
    "\n",
    "print(pivot_table_filled)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce3a3741-b69e-48c3-b9b6-94fc816c8de1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the result into a csv file\n",
    "pivot_table_filled.to_csv(r'count_unique_records.csv')\n",
    "\n",
    "# Define the source path of the CSV file (assuming it's in the current directory)\n",
    "source_csv_path = \"count_unique_records.csv\"\n",
    "\n",
    "# Define the target directory where the CSV file should be placed\n",
    "target_directory = \"csv_files\"\n",
    "\n",
    "# Create the target directory if it doesn't exist\n",
    "if not os.path.exists(target_directory):\n",
    "    os.makedirs(target_directory)\n",
    "\n",
    "# Move the CSV file to the target directory\n",
    "shutil.move(source_csv_path, os.path.join(target_directory, \"count_unique_records.csv\"))\n",
    "\n",
    "# Print a success message\n",
    "print(f\"The CSV file {source_csv_path} was successfully moved to {target_directory}.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77c7b3db-0582-4c65-8ca5-f682932c572d",
   "metadata": {},
   "source": [
    "# Mortality Statistics\n",
    "## Source Data For Mortality Statistics and Modelled Age Specific Survival Rates (Model Input Set By Control Assumptions)\n",
    "\n",
    "# LPA Control Assumptions\n",
    "## Specific Key Assumptions that control expected demand , LPA market size and saturation.\n",
    "\n",
    "\n",
    "# Meta data and Variable selection and Data Cleaning for the Mortality statastics data based on population projections:\n",
    "\n",
    "## Goal: \n",
    "### What proportion of the UK population are likely to buy LPA and still alive?\n",
    "*How many people are still alive (Living Donors bought LPA)*\n",
    "*Based on ONS Data of Population of Engalnd and Wales, how many people are still alive and how many of them are dead?*\n",
    "*e.g., if there are 1000 people and 100 of them are still alive and bought LPA,\n",
    "so there are 900 of them still didn't buy LPA.\n",
    "\n",
    "\n",
    "\n",
    "**1. These rates are standardised to the 2013 European Standard Population, expressed per million population; \n",
    "they allow comparisons between populations with different age structures, including between males and females and over time. \n",
    "**2.  Deaths per 1,000 live births. \n",
    "**3.  Death figures are based on deaths registered rather than deaths occurring in a calendar year.\n",
    "\n",
    "### For information on registration delays for a range of causes, see: \n",
    "    https://webarchive.nationalarchives.gov.uk/ukgwa/20160106020016/http://www.ons.gov.uk/ons/guide-method/user-guidance/health-and-life-events/impact-of-registration-delays-on-mortality-statistics/index.html\n",
    "\n",
    "A limiting factor in modelling numbers of surving LPA holders aged 90+ has been the absence of single age specific mortality rates \n",
    "for this group. Estimates* suggested that previously applied mortality rates were too low increasing the apparent numbers of \n",
    "surviving LPA holder saged 90+ and therefore over-estimating the \"sauration of this market.\n",
    "\n",
    "For the 2018 LPA forecast , Age specific mortality rates for those aged 90+ have therefore been extrapolated based on \n",
    "a standard log power law that best fits existing mortality rates to age. \n",
    "\n",
    "*numbers of surviving LPA holders were estimated to exceed the total projected  population in each age group which was \n",
    "clearly not possible.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b287ad4-7294-4a75-a9e7-71d4f397b8da",
   "metadata": {},
   "source": [
    "# LPA SURVIVAL TABLES:\n",
    " LPA MODEL/LPA SURVIVAL TABLES\n",
    "percentage of people are died in one year\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8aa6b618-1161-4001-94c2-df536c5a7953",
   "metadata": {},
   "source": [
    "# if a 1000 40 years old male bought an LPA in 2008, what proportion of are still alove today?\n",
    "\n",
    "# The model taking each age categories (categorical variable) and assumed that they are \n",
    "# singe age-specifics in the age category 18 to 90 and provide figure what percentage of people for male died within one year?\n",
    "\n",
    "## e.g., in the 15-19 age category, 0.3 percent of males died within one year in the UK and 0.03 per 1000\n",
    "## e.g., in the 25-29 age category, 0.6 percent of males died within one year in the UK and 0.06 per 1000\n",
    "## e.g., in the 70-74 age category, 23.7 percent of males died within one year in the UK or 2.37 per 1000\n",
    "\n",
    "## if you started at age 18, 7 years and become 25 years old ahead, \n",
    "## as the ages goes up you will fall into a higher mortality category (from 0.3 to 0.6)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96ae6aa8-4340-45f6-8ba4-d231f3a29a5e",
   "metadata": {},
   "source": [
    "# calculate naïve extrapolation for demand forecasting and calculate low planning estimate, centeral planning estimate, and high planning estimate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4fd8723-2c72-44a2-8278-a3d1ddfe5907",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract the month of reciept from the receiptdate\n",
    "#lpa_data_no_index['month'] = lpa_data_no_index['receiptdate'].dt.month\n",
    "#lpa_data_no_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a60479e1-8b7e-4565-af0d-4f59f6b33678",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# Calculate the number of unique records by month and year\n",
    "count_unique_month = lpa_data.groupby(['year', 'month_year', 'age'])['uid'].nunique().reset_index(name='count')\n",
    "\n",
    "\n",
    "#unique_records = df.groupby('unique_key').agg('count').reset_index()  #.groupby(['year'])['unique_key'].nunique().reset_index(name='count')\n",
    "count_unique_month = count_unique_month.rename(columns={\"count\": \"Count_of_CASEID_month\"})\n",
    "\n",
    "# Display the result\n",
    "print(count_unique_month)\n",
    "\n",
    "# Save the result into a csv file\n",
    "#count_unique_month.to_csv(r'count_unique_month.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa18b9fd-73d7-46d1-9616-3cb0b6229292",
   "metadata": {},
   "outputs": [],
   "source": [
    "#df_count_unique_month = pd.DataFrame(count_unique_month)\n",
    "age_lower_limit = 50\n",
    "age_upper_limit = 70\n",
    "count_unique_month1 = count_unique_month.loc[(count_unique_month[\"age\"] >= age_lower_limit) &\n",
    "                 (count_unique_month[\"age\"] <= age_upper_limit)]\n",
    "\n",
    "#age_range = [50:70]\n",
    "#count_unique_month.loc[count_unique_month[\"month_year\"].isin(age_range)]\n",
    "\n",
    "count_unique_month2 = count_unique_month1[count_unique_month1['month_year'].isin(['Jan-24', 'Feb-24', 'Mar-24'])]\n",
    "#count_unique_month2 = count_unique_month1[count_unique_month1['month_year'].isin([50:70])]\n",
    "#df_count_unique_month['month_year'] = pd.to_datetime(df_count_unique_month['month_year'], format='%b-%y')\n",
    "#df_count_unique_month = df_count_unique_month.sort_values(df_count_unique_month.columns[1])\n",
    "#df_count_unique_month = df_count_unique_month.set_index(['month_year'])\n",
    "\n",
    "count_unique_month_age = count_unique_month2\n",
    "# Save the result into a csv file\n",
    "count_unique_month_age.to_csv(r'count_unique_month_age.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7546fbde-74e9-40f6-b03b-fea16e93e1af",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# Calculate the naïve forecast (previous month's sales)\n",
    "count_unique_month_age['Naive_Forecast'] = count_unique_month_age['Count_of_CASEID_month'].shift(1)\n",
    "\n",
    "# Define planning estimate factors\n",
    "low_factor = 0.9\n",
    "high_factor = 1.1\n",
    "\n",
    "# Calculate planning estimates\n",
    "count_unique_month_age['Low_Planning_Estimate'] = count_unique_month_age['Naive_Forecast'] * low_factor\n",
    "count_unique_month_age['Central_Planning_Estimate'] = count_unique_month_age['Naive_Forecast']\n",
    "count_unique_month_age['High_Planning_Estimate'] = count_unique_month_age['Naive_Forecast'] * high_factor\n",
    "\n",
    "# Calculate MAPE (Mean Absolute Percentage Error)\n",
    "count_unique_month_age['Absolute_Percentage_Error'] = abs(count_unique_month_age['Count_of_CASEID_month'] - count_unique_month_age['Naive_Forecast']) / count_unique_month_age['Count_of_CASEID_month']\n",
    "mape = count_unique_month_age['Absolute_Percentage_Error'].mean() * 100\n",
    "\n",
    "# Calculate MAD (Mean Absolute Deviation)\n",
    "count_unique_month_age['Absolute_Deviation'] = abs(count_unique_month_age['Count_of_CASEID_month'] - count_unique_month_age['Naive_Forecast'])\n",
    "mad = count_unique_month_age['Absolute_Deviation'].mean()\n",
    "\n",
    "# Display results\n",
    "print(count_unique_month_age)\n",
    "print(f\"MAPE: {mape:.2f}%\")\n",
    "print(f\"MAD: {mad:.2f}\")\n",
    "print(\"\\nPlanning Estimates:\")\n",
    "print(f\"Low Planning Estimate: {count_unique_month_age['Low_Planning_Estimate'].iloc[-1]:.2f}\")\n",
    "print(f\"Central Planning Estimate: {count_unique_month_age['Central_Planning_Estimate'].iloc[-1]:.2f}\")\n",
    "print(f\"High Planning Estimate: {count_unique_month_age['High_Planning_Estimate'].iloc[-1]:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99ce5ae4-fa0c-499e-aaee-9c38e47bd090",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a line plot\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.plot(df_count_unique_month['Count_of_CASEID_month'], df_count_unique_month['Low_Planning_Estimate'], label='Low Estimate', marker='o')\n",
    "plt.plot(df_count_unique_month['Count_of_CASEID_month'], df_count_unique_month['Central_Planning_Estimate'], label='Central Estimate', marker='s')\n",
    "plt.plot(df_count_unique_month['Count_of_CASEID_month'], df_count_unique_month['High_Planning_Estimate'], label='High Estimate', marker='^')\n",
    "\n",
    "# Add labels and title\n",
    "plt.xlabel('Month')\n",
    "plt.ylabel('Sales Estimate')\n",
    "plt.title('Demand Forecasting Estimates')\n",
    "plt.grid(True)\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe089b8b-61ca-424d-b7dd-adb9092ca2b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Number of LPA reciepts\n",
    "lpa_reciepts = lpa_unique\n",
    "# Extract month letter and year \n",
    "lpa_reciepts['month_year'] = lpa_reciepts['receiptdate'].dt.strftime('%b-%y')\n",
    "# Calculate the number of unique records by month and year\n",
    "count_reciepts_month = lpa_reciepts.groupby(['year', 'month_year'])['receiptdate'].nunique().reset_index(name='count')\n",
    "\n",
    "\n",
    "#unique_records = df.groupby('unique_key').agg('count').reset_index()  #.groupby(['year'])['unique_key'].nunique().reset_index(name='count')\n",
    "count_reciepts_month = count_reciepts_month.rename(columns={\"count\": \"Count_of_reciepts_month\"})\n",
    "\n",
    "# Display the result\n",
    "print(count_reciepts_month)\n",
    "\n",
    "# Save the result into a csv file\n",
    "#count_reciepts_month.to_csv(r'count_reciepts_month.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8257dabd-234c-49eb-bd04-7e3157e87e35",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Number of LPA reciepts\n",
    "lpa_reciepts = lpa_data\n",
    "\n",
    "# Extract month letter and year \n",
    "lpa_reciepts['month_year'] = lpa_reciepts['receiptdate'].dt.strftime('%b-%y')\n",
    "\n",
    "# Calculate the number of unique records by month and year\n",
    "Count_of_reciepts_annual = lpa_reciepts.groupby(['year'])['receiptdate'].nunique().reset_index(name='count')\n",
    "\n",
    "\n",
    "#unique_records = df.groupby('unique_key').agg('count').reset_index()  #.groupby(['year'])['unique_key'].nunique().reset_index(name='count')\n",
    "Count_of_reciepts_annual = Count_of_reciepts_annual.rename(columns={\"count\": \"Count_of_reciepts_annual\"})\n",
    "\n",
    "\n",
    "# Display the result\n",
    "print(Count_of_reciepts_annual)\n",
    "\n",
    "# Save the result into a csv file\n",
    "####Count_of_reciepts_annual.to_csv(r'Count_of_reciepts_annual.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c1c8fe0-7a2d-4cd8-bccc-4b302daaaa1c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
