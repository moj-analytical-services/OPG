# OPG: Future Development and Strategies for Demand Forecast modelling for LPA - Documentation
**This document serves as a specification documentation and user guide to the advanced forecasting models could be applied to incorporate uncertainty in the future demands of Living Power of Attorney (LPA) applications in the Office of the Public Guardian (OPG) based on diffrent drivers having impact on the time sries forecasting e.g., covid, social media etc. 

#==============================================================================
# @author: Dr. Leila Yousefi 
# MoJ Modelling Hub
#==============================================================================
&nbsp;
&nbsp;

# Contents
<<<<<<< HEAD
=======

>>>>>>> main
* The instructions on how to do some potential model developments.

* How to incorporate covid period demands and uncertainty around the data in long-term and short-term forecasting of demands (Living Power of Authorney (LPA) applications in Office of Public Guardian (OPG)) to provide accurate forecats for the LPA demands?

* How to Utilise demand intelligence tools and data science models to incorporate real-time data and predictive analytics into forecasts, to help in understanding the impact of covid period on demand (Living Power of Authorney (LPA) applications in Office of Public Guardian (OPG)) in long-term forecasting model (e.g., cohort-based model) and short-term (e.g., exponential smoothing)?

* Which AI / Machine Learning techniques can be used to incorporate the impact of covid period demands (e.g., mortality rate, population projection, number of LPA applications) and uncertainty around the data in long-term and short-term forecasting of LPA demands to provide accurate forecats for the LPA demands?

<<<<<<< HEAD

&nbsp;
*Note: By combining these strategies, you can create a more robust and adaptable forecasting model that accounts for the unique challenges posed by the COVID period and beyond; 
forecasting is not about predicting the future with certainty but about preparing for it with flexibility and insight.

&nbsp;
# **Strategies for incorporating the demands and uncertainties of the COVID period into both long-term and short-term forecasting for LPA applications at the OPG:**

=======
* OPG strategy: 
- Definition of supervision: The definition of supervision going forward will therefore be the ongoing monitoring and tailored oversight that the OPG provides to deputies appointed by the Court of Protection and Guardians, appointed by the High Court to ensure that they carry out their responsibilities in the best interests of P or the missing persons or the person they are deputy or guardian for. This includes, but is not limited to, ensuring compliance with their court order, assessing that they are acting in accordance with the requirements of the Mental Capacity Act 2005 or guardianship, missing persons 2017 and overseeing adherence to the deputy standards for deputies.  
- Interpretation of Statutory function: Compliance - lay public authority, professional and health and welfare or personal welfare cases and property and financial AFFAIRS. and Infrastructure: that is that we develop and build a supervision infrastructure that promotes excellence and sustainability.
- Policy Objectives: Compliance, Infrastructure, and Evidence-led policymaking: provide experts etc.
- Manage any kind of prioritisation and understanding resource and to be involved in any deadline setting in OPG.
    1. **L priority** guardianships work, simply because we have only ever had a handful of cases.
    4. **M priority**: fees, although it's medium priority here and here it only mentions supervision fees only because it's the policy strategy for supervision.
    3. **H priority**: Evidence-led policymaking - it is important to build a high quality, high quality evidence base that we can use and draw on to develop and design our policies. A high quality evidence base around Public authority deputyships, professional deputyships and health and welfare or personal welfare deputyships, monitoring and evaluation and then third is sort of longer term aspiration is around establishing the OPG as a thought leader and with the ones that are high priority and this is the bit where it would be really good to get your thoughts on how best to take it forward. And so figuring out what we have at the moment in terms of data, what are the gaps when it comes to supervision and deputyships and how can we fill those in order to improve our evidence base and increase our evidence base for it comes to supervision and deputyships and guardianships too and then monitoring evaluation is and that one's a bit more around understanding the impact of our policies because that is one of the things that's missing at the moment and it is also about tracking the implementation of our policies as well.
    4. There is an urgent need to provide a report, data requirement, and update the forecasting models.
    5. What are the data requirement and scope to work around fees and minister ideas? 
    6. In terms of the data requirements and it would be really helpful to understand what data is currently available in that space or deputyships, whether and it is around a sort of anything related to protected characteristics, whether it is possible to link that or demographics. And then the other aspect is now forecasting really helpful to understand sort of what our deputyships or caseload will look like going forward and how that would look as well, because that will help us plan our policy work and going forward in terms of what areas we should be focusing on. To break down deputyship volumes by demographics and age and gender and location and deprivation by data engineering.
    7. In terms of governance, at the moment when it comes to supervision, it has a slightly different governance arrangement compared with other policy work and the OPG which goes through the policy Implementation Committee and Executive Committee where it needs to be escalated. To pick policy and implementation Committee and Xcode Executive Committee and every month there will be a policy working group.
    8. To consider from other parts of the organisation and what this all bigger picture big ticket items that we need to think about as well and it is the quarterly advisory group that,, we need to close down the current governance route before the new one can be put in place and that is take the transition in the transition period at the moment.
    9.  The vast majority of cases with deputyships people live in care homes. Thus, we could be more accurate with the LPA data about inferring kind of characteristics because people, those private households. So, collect that data up front if we need it rather than trying to model.
    10. Investigate more regarding where supervision fees are subsidised by our enhanced LPA fee. Also understanding the level of legal challenge to it and considering the legal risks.
    11. Concern around scope for looking at and fees, and who pays a fee and what fee they pay, then you know the moment the data is not adequate to be able to do that.
    12. There are a Cost recovery model when it comes to fees, we need to understand the cost of our services. This is being led by OPG finance colleagues and MoJ as well, Pippa Jones. Legal will be heavily involved in the fees, work and generally across all of our policy work. 
    13. How many cases there are, the dates they were born, the details about the individuals concerned. Be cautioned linking that on to external data sets without thinking very carefully about, I mean, what can you tell about somebody based on their name and the care home that they are in their age? Manual data collection and missing information in the data and check whether they are fit for the purpose. because the alternative was to have a very expensive and complex data collection exercise where you then have to go and ask lots of vulnerable people, all their deputies, lots of personal information about themselves. On the example on the court order, because they might have had a deputyship for 10-15 years or more, whatever their financial status was, then is probably not what it is now.
    

&nbsp;
&nbsp;

# Methodology
**Strategies for incorporating the demands and uncertainties of the COVID period into both long-term and short-term forecasting for LPA applications at the OPG:**
*Note: By combining these strategies, you can create a more robust and adaptable forecasting model that accounts for the unique challenges posed by the COVID period and beyond; 
forecasting is not about predicting the future with certainty but about preparing for it with flexibility and insight.

>>>>>>> main
- Data Analysis: 
    - Begin by analysing historical data, but with a focus on identifying patterns that emerged specifically during the COVID period. 
    - This includes changes in application rates, processing times, and any other relevant metrics.

- Scenario Planning: 
    - Develop multiple forecasting scenarios to account for various levels of COVID-19 impact and recovery rates. 
    - This approach helps in preparing for different possible futures.

- Continuous Monitoring: 
    - Set up a system for continuous monitoring of demand signals and indicators that could suggest shifts in LPA application rates.

- Feedback Loop: 
    - Create a feedback loop where the forecast is regularly compared against actual demand, and adjustments are made accordingly.

- Stakeholder Communication: 
    - Keep communication open with stakeholders to understand potential changes in demand due to policy shifts or public sentiment.

- Technology Adoption: 
    - Embrace digital solutions that can streamline the application process and potentially alter demand. 
    - The OPG has plans to allow for completely online LPA applications, which could change the demand landscape.

- Resilience Building: 
    - Invest in building resilience into your models to account for decremental demand causal factors, such as health warnings or other emergencies.

- Demand Intelligence: 
    - Utilise demand intelligence tools and data science models to incorporate real-time data and predictive analytics into your forecasts. 
    - This can help in understanding the impact of current events on demand.



* ref: https://www.mckinsey.com/capabilities/operations/our-insights/ai-driven-operations-forecasting-in-data-light-environments

<<<<<<< HEAD
# Incorporating demand intelligence tools, data science models, and machine learning techniques into the long-term and short-term forecasting for Living Power of Attorney (LPA) demands can significantly enhance your understanding of demand patterns. 
=======
**Incorporating demand intelligence tools, data science models, and machine learning techniques into the long-term and short-term forecasting for Living Power of Attorney (LPA) demands can significantly enhance your understanding of demand patterns. 
>>>>>>> main

*Which machine learning techniques can be used to incorporate the impact of covid period demands (e.g., mortality rate, population projection, number of LPA applications) and uncertainty around the data in long-term and short-term forecasting of LPA demands to provide accurate forecats for the LPA demands?

*how to Utilise demand intelligence tools and data science models to incorporate real-time data and predictive analytics into forecasts, to help in understanding the impact of covid period on demand (Living Power of Authorney (LPA) applications in Office of Public Guardian (OPG)) in long-term forecasting model (e.g.,cohort-based model) and short-term (e.g., exponential smoothing). 

# External Factors: 
How to incorporate external factors to deal with covid-period effects on the outcome into a LPA demands long-term forecating model using cohort model in excel as well as LPA demands short-term forecating model using exponentional smoothing model in excel?
- Consider external factors that could influence demand, such as changes in legislation, economic conditions, and societal trends. 
- The Powers of Attorney Act 2023, for example, aims to digitise the LPA process, which could affect demand patterns.
- **Incorporating external factors into the forecasting model is a multi-step process that enhances the model’s accuracy and robustness.**
- By following the following steps, you can create a forecasting model that not only reflects historical trends but also adapts to the dynamic nature of external influences, providing a more accurate forecast for LPA demands: 

- Identify Relevant Factors: 
    - Determine which external factors are likely to influence the demand for LPA applications. 
    - This could include economic indicators, demographic trends, policy changes, and health advisories.

- Data Collection: 
    - Gather data on these factors over a period that aligns with your historical demand data. 
    - This will help in establishing correlations.

- Feature Engineering: 
    - Transform these external factors into features that your model can interpret. 
    - For example, you can create binary flags for events (like a pandemic onset) or continuous variables for economic indicators.

- Model Integration: 
    - Use statistical models like ARIMA (Autoregressive Integrated Moving Average) or advanced techniques like machine learning algorithms to integrate these features into your forecasting model. 
    - This could involve adding the features as additional variables in your dataset.

- Scenario Analysis: 
    - Develop scenarios based on different combinations and intensities of external factors to see how they would impact demand.

- Model Training: 
    - Train your model using the historical data along with the new external features. This will allow the model to learn the relationship between these factors and LPA demand.

- Validation and Testing: 
    - Validate your model against a separate dataset to ensure it accurately predicts demand considering the external factors.

- Continuous Improvement: 
    - Regularly update your model with new data and review the external factors to ensure they remain relevant and accurately represented in the model.

- Monitor Real-Time Data: 
    - Use real-time data to adjust your forecasts promptly. This helps in responding quickly to sudden changes in external conditions3.


<<<<<<< HEAD
# Incorporating external factors to address the effects of the COVID-period on outcomes into both long-term and short-term forecasting models for LPA demands in Excel requires a structured approach. 
*How to incorporate external factors to deal with covid-period effects on the outcome into a LPA demands long-term forecating model using cohort model in excel as well as LPA demands short-term forecating model using exponentional smoothing model in excel? 

## Long-Term Forecasting Using a Cohort Model in Excel

### Identify External Factors: 
Determine which external factors such as government policies, economic conditions, or public health advisories impacted LPA demands during the COVID period.

### Data Collection: 
Collect data on these factors for the period you are analysing.

### Cohort Analysis: 
Group your data into cohorts based on the time period of LPA applications. This could be monthly or quarterly cohorts.

### Regression Analysis: 
Use regression analysis to understand the impact of external factors on each cohort.

### Adjust Cohorts: 
Adjust the cohort data based on the regression analysis to reflect the impact of COVID-period effects.

### Forecasting: 
=======
**Incorporating external factors to address the effects of the COVID-period on outcomes into both long-term and short-term forecasting models for LPA demands in Excel requires a structured approach.**
*How to incorporate external factors to deal with covid-period effects on the outcome into a LPA demands long-term forecating model using cohort model in excel as well as LPA demands short-term forecating model using exponentional smoothing model in excel? 

# Long-Term Forecasting Using a Cohort Model in Excel

## Identify External Factors: 
Determine which external factors such as government policies, economic conditions, or public health advisories impacted LPA demands during the COVID period.

## Data Collection: 
Collect data on these factors for the period you are analysing.

## Cohort Analysis: 
Group your data into cohorts based on the time period of LPA applications. This could be monthly or quarterly cohorts.

## Regression Analysis: 
Use regression analysis to understand the impact of external factors on each cohort.

## Adjust Cohorts: 
Adjust the cohort data based on the regression analysis to reflect the impact of COVID-period effects.

## Forecasting: 
>>>>>>> main
Use the adjusted cohort data to forecast long-term demand, use Excel’s built-in functions or create a custom model using the cohort data.

## ####################################################################### ##

# Bayesian (probabilistic) models
- Incorporating uncertainly friendly models in Lasting Power of Attorney (LPA) forecasting can enhance accuracy and provide a probabilistic framework for handling uncertainty. 
- Applying uncertainty around volatile data (application demands) affected by COVID pandemic in log-term time series forecasting model.

## Understanding Bayesian Models:
- Bayesian models are based on Bayes’ theorem, which updates our beliefs (probabilities) based on new evidence.
- These models incorporate prior knowledge (prior distribution) and update it with observed data to obtain a posterior distribution.
- In the context of LPA forecasting, Bayesian models allow us to quantify uncertainty and make informed predictions.

## Dynamic Linear Models (DLMs):
- DLMs are a class of Bayesian state space models commonly used for time series forecasting.
- DLMs can handle time-varying parameters, seasonality, and irregularities in data.
- DLMs consist of two components:
    - State Equation: Describes how the underlying state (e.g., capacity fluctuations) evolves over time.
    - Observation Equation: Relates the observed data (e.g., LPA applications) to the underlying state.


## Steps to Incorporate Bayesian Models:

## Prior Specification:
- Define prior distributions for model parameters (e.g., capacity loss rates, trend coefficients).
- Priors can be informative (based on domain knowledge) or non-informative (flat priors).

## Likelihood Function:
- Specify the likelihood function that relates observed data to the model parameters.
- For LPA forecasting, this could be based on historical LPA application data.

## Posterior Inference:
- Use Bayes’ theorem to update the prior distribution based on observed data.
- Markov Chain Monte Carlo (MCMC) methods or variational inference can estimate the posterior distribution.

## Prediction:
- Simulate from the posterior distribution to obtain predictive samples.
- These samples represent possible future scenarios, accounting for uncertainty.


## Model Selection: Choose an appropriate DLM structure:
Local Level Model: Represents a random walk (e.g., gradual capacity decline).
Local Linear Trend Model: Includes both level and slope components.
Seasonal Models: Capture seasonal patterns.
Regression Models: Incorporate external predictors (e.g., health indicators).
Model selection can be guided by cross-validation or information criteria (e.g., Bayesian Information Criterion).

## Updating Over Time:
As new LPA application data becomes available, update the model using Bayesian methods.
This allows the model to adapt to changing conditions (e.g., shifts in capacity fluctuations).

## Scenario Analysis:
Generate probabilistic forecasts for different scenarios:
Gradual capacity decline.
Sudden capacity loss due to health events.
Capacity improvement (if relevant).
Assess the impact of these scenarios on LPA applications.
Remember that Bayesian models provide a flexible framework for incorporating prior knowledge, handling uncertainty, and adapting to changing conditions. While implementing Bayesian models in Excel directly may be challenging, consider using specialized statistical software (e.g., Python with libraries like pymc3 or Stan) for more complex modeling12. 


## Bayesian models Limitation:
While Bayesian models offer several advantages, they also come with limitations, especially when applied to LPA forecasting as followings:
- Computational Complexity:
    - Bayesian models involve complex calculations, especially when estimating posterior distributions using Markov Chain Monte Carlo (MCMC) methods.
    - For large datasets or high-dimensional models, the computational burden can be significant.

- Subjectivity in Prior Selection:
    - Bayesian models require specifying prior distributions for model parameters.
    - The choice of priors can impact the results, and different analysts may choose different priors based on their beliefs or domain knowledge.
    - Subjective priors can introduce bias if not carefully considered.

- Data Requirements:
    - Bayesian models perform well when sufficient data is available.
    - Sparse or noisy data can lead to unreliable posterior estimates.
    - In the case of LPAs, historical data may be limited, especially for specific subgroups (e.g., rare health conditions).

- Model Misspecification:
    - If the chosen Bayesian model does not accurately represent the underlying process (e.g., capacity fluctuations), the results may be misleading.
    - Model misspecification can lead to biased parameter estimates.

- Assumptions of Independence:
    - Many Bayesian models assume independence between observations.
    - In reality, dependencies may exist (e.g., correlations between LPAs within the same family).
    - Ignoring dependencies can affect the accuracy of forecasts.

- Interpretability:
    - Bayesian models provide posterior distributions, which are more informative than point estimates.
    - However, interpreting complex posterior distributions can be challenging for non-experts.
    - Communicating uncertainty effectively to stakeholders may require additional effort.

- Limited Excel Integration:
    - Implementing Bayesian models directly in Excel can be cumbersome due to its limitations in handling probabilistic calculations.
    - Specialised statistical software (e.g., Python, R) is better suited for Bayesian modeling.

- Assumption of Stationarity:
    - Some Bayesian time series models assume stationarity (constant statistical properties over time).
    - In practice, capacity fluctuations may exhibit non-stationary behaviour (e.g., trends, seasonality).

- Model Complexity vs. Parsimony:
    - Bayesian models can become overly complex if too many parameters are included.
    - Balancing model complexity with parsimony is essential to avoid overfitting.

- Updating Models Over Time:
    - Incorporating new data into Bayesian models requires re-estimating posterior distributions.
    - Real-time updates can be computationally intensive.
    - Despite these limitations, Bayesian models remain valuable tools for handling uncertainty, incorporating prior knowledge, and making informed predictions. 



## Addressing model misspecification in Bayesian LPA forecasting?
- What are the alternatives to MCMC for estimating posterior distributions?
- Can you explain how to perform sensitivity analysis on priors in a Bayesian model?
- What are the legal implications of capacity fluctuations in LPAs?
- Can you provide an example of a sensitivity analysis for LPA applications?**


- Model Misspecification in Bayesian LPA Forecasting:
Model misspecification occurs when the chosen Bayesian model does not accurately represent the underlying process (e.g., capacity fluctuations).To address this:

- Prior Sensitivity Analysis: 
Explore different prior distributions for model parameters. Assess how changes in priors impact the posterior distribution and predictions.

- Alternative Models: 
Consider alternative Bayesian models (e.g., different likelihood functions, non-parametric models) and compare their performance.

- Gaussian-Process Approximations: 
Use Gaussian-process approximations to improve posterior estimates12.

## Alternatives to MCMC for Estimating Posterior Distributions:
- While MCMC is widely used, other methods exist:
    - Variational Inference: Solves an optimization problem to approximate the posterior faster than simple MCMC.
    - Importance Sampling: Estimates properties of posteriors by sampling from an approximation.
    - Analytical Solutions: In some cases, posterior distributions can be computed analytically34.

- Performing Sensitivity Analysis on Priors in a Bayesian Model:
Sensitivity analysis assesses how changes in priors affect model outcomes.

### Steps:
- Define a range of prior values (e.g., mean, variance).
- Run the model with different priors.
- Observe how posterior distributions and predictions vary.
- Compare estimated parameters and make conclusions based on context5.
- Legal Implications of Capacity Fluctuations in LPAs:
- LPAs grant decision-making authority to attorneys when the donor lacks capacity.
- Legal implications:
- Freezing of Accounts: Without an LPA, banks may freeze accounts if a signatory lacks capacity.
- Deputyship Applications: If no LPA exists, a third party can apply to be appointed as a deputy by the Court of Protection.
- Business LPAs: For business accounts, lacking an LPA can impact financial operations678.
- Example of Sensitivity Analysis for LPA Applications:
- Suppose we have an LPA model with priors for mean1 and mean2 (related to capacity).
- Vary the priors (e.g., mean1, tau1, tau2) within reasonable ranges.
- Observe how estimated parameters (e.g., A, mean1) change.
- Conclude based on the impact of different priors on LPA decisions5.
- Remember that sensitivity analysis helps assess the robustness of Bayesian models and informs decision-making. 




## Handling missing data in Bayesian models is essential for accurate inference.
### Conceptual Understanding of Missing Data:
Missing data can arise due to various reasons, such as design issues or factors beyond researchers’ control.
We’ll focus on a hypothetical regression problem where we predict voting intention (YY) using people’s age (XX).

### Types of Missing Data Mechanisms:

#### MCAR (Missing Completely at Random):
Missingness is unrelated to any research question.
Example: Interviewer accidentally erases responses (unrelated to age or voting intention).
Under MCAR, using cases with no missing values provides valid inferences and unbiased estimations.

#### MAR (Missing at Random):
Missingness depends on observed variables (e.g., XX) but not the unobserved outcome (YY).
Example: Older people more likely to give a missing response (related to XX).

#### NMAR (Not Missing at Random):
Missingness depends on unobserved factors (e.g., ZZ) related to neither XX nor YY.
Example: People with lower voting intention less likely to respond (related to YY itself).

#### Bayesian Approaches to Handle Missing Data:

##### Treat Missing Data as Parameters:
Assign priors to missing values (kid_score in our example).
Estimate their posterior distributions.
Incorporate all available information in the analysis.

##### Multiple Imputation:
Generate multiple imputed datasets by imputing missing values.
Analyze each dataset separately and combine results.

##### Bayesian origin: 
Impute missing values using predictive distributions.

##### Checking for MCAR:
Compare the distribution of XX for cases with and without missing data on YY.
If means and variances of XX are similar, it suggests MCAR.
Regression lines remain stable with or without missing data.
Remember that Bayesian models allow us to treat missing data as parameters, leveraging all available information. Multiple imputation is another powerful technique to handle missingness. Choose the approach based on the specific context and assumptions about missing data12. 



### Multiple imputation (MI) technique for handling missing data
Multiple imputation (MI) is a powerful technique for handling missing data, and incorporating it into a Bayesian framework provides a robust and flexible approach. 

#### Conceptual Understanding of Missing Data:
Missing data can arise due to various reasons, such as design issues or factors beyond researchers’ control.
In Bayesian modeling, we treat missing values as unknown parameters and estimate their posterior distributions.

#### Types of Missing Data Mechanisms:
There are three main types of missing data mechanisms:
MCAR (Missing Completely at Random): Missingness unrelated to any research question.
MAR (Missing at Random): Missingness depends on observed variables but not the unobserved outcome.
NMAR (Not Missing at Random): Missingness depends on unobserved factors unrelated to observed variables.

#### Bayesian Approach to Handling Missing Data:
Treat Missing Data as Parameters:
In Bayesian models, we incorporate missing data directly into the model.
Missing values become parameters with prior distributions.
The posterior distribution accounts for both observed and missing data.
This approach leverages all available information and avoids discarding cases with missing values.

#### Multiple Imputation (MI):
MI generates multiple synthetic datasets by imputing missing values.
Each dataset represents a plausible completion of the missing data.

#### Bayesian MI involves the following steps:
Impute Missing Values: Impute missing data using predictive distributions (e.g., regression models).
Analyze Each Imputed Dataset Separately: Perform Bayesian analysis on each imputed dataset.
Combine Results: Combine parameter estimates, credible intervals, and other inferences across multiple imputed datasets.

#### Account for Uncertainty: 
The variability across imputed datasets reflects the uncertainty due to missing data.

#### Benefits of Bayesian MI:
Incorporates Uncertainty: Bayesian methods naturally propagate uncertainty.
Integrates Prior Information: Prior distributions provide additional context.
Flexible and Robust: Handles different missing data mechanisms.
Valid Inferences: Provides valid parameter estimates and credible intervals.

#### Example Application:
Suppose we have a regression model predicting voting intention (YY) based on age (XX).
Impute missing YY values using Bayesian regression models.
Analyze each imputed dataset separately (e.g., compute posterior distributions for regression coefficients).
Combine results across imputed datasets to obtain overall parameter estimates.
Remember that Bayesian MI allows us to handle missing data while maintaining the richness of uncertainty. It’s a powerful tool for robust statistical inference.


### Bayesian multiple imputation is a powerful technique for handling missing data, but it relies on certain assumptions.
#### Missing at Random (MAR):
The MAR assumption is crucial for multiple imputation.
It implies that the probability of missingness depends only on observed variables (not the unobserved outcome) after accounting for other observed variables.
In other words, missingness is related to the available information in the dataset.
If data are MAR, imputing missing values based on observed variables can lead to unbiased parameter estimates.

#### Predictive Model Assumption:
Bayesian multiple imputation imputes missing values using predictive models.
The assumption is that the predictive model accurately captures the relationship between observed and missing data.
If the model is misspecified, imputed values may be biased.

#### Assumption of Ignorable Missingness Mechanism:
Ignorable missingness means that the missing data mechanism does not introduce systematic bias.
If data are MCAR or MAR, the missingness mechanism is considered ignorable.
Ignorable missingness allows valid inferences using multiple imputation.

#### Appropriate Choice of Imputation Model:
Selecting an appropriate predictive model for imputation is essential.
The model should reflect the underlying data-generating process.
Consider linear regression, logistic regression, or other relevant models.

#### Sufficient Number of Imputations:
Multiple imputation generates several imputed datasets.
The number of imputations affects the precision of estimates.
More imputations reduce uncertainty due to missing data.

#### Assumption of Exchangeability:
Exchangeability assumes that the imputed datasets are exchangeable (i.e., interchangeable).
This allows combining results across imputed datasets.
In practice, exchangeability is often reasonable.

#### Sensitivity to Prior Distributions:
Bayesian imputation involves specifying prior distributions for model parameters.
Sensitivity analysis explores how different priors impact results.
Robustness to prior choices is desirable.
Remember that while Bayesian multiple imputation is a powerful tool, understanding and validating these assumptions are critical for reliable results




## ##################################################################### ##

# LSTM
**How to use LSTM method for the LPA demands short-term and long-term forecasting model to deal with covid-period effects on the outcome and deal with uncertatinty?**

*Using the LSTM (Long Short-Term Memory) method for forecasting LPA demands can be particularly effective for capturing complex patterns and dealing with uncertainties, 
such as those introduced during the COVID-period. 

**Applying LSTM for both short-term and long-term forecasting:

- Data Preparation: 
Organise LPA demand data into a sequence that can be used for training the LSTM model. This includes normalising the data and possibly transforming it into a supervised learning problem.

- Feature Selection: 
Choose relevant features that could influence LPA demands, including historical demand data, COVID-period indicators, and possibly external factors like economic indicators or policy changes.

- Model Architecture: 
Design your LSTM network architecture. This typically involves defining the number of layers, the number of neurons in each layer, and the connections between the layers.

- Incorporate COVID-period Effects: 
Integrate COVID-period effects by including them as input features or by creating a separate model that specifically predicts the impact of COVID-related variables on LPA demands.

- Train the Model: 
Use historical data to train your LSTM model. This involves feeding the input features into the network and adjusting the weights through backpropagation based on the error between the predicted and actual values.

- Validation: 
Validate your model using a separate dataset to ensure that it generalizes well to new, unseen data.

- Uncertainty Handling: 
To deal with uncertainty, use techniques like Monte Carlo simulations or Bayesian methods to get a distribution of possible outcomes rather than a single point estimate.

- Forecasting: 
Use the trained LSTM model to forecast future LPA demands. 
    - For short-term forecasting, might use a smaller window of historical data,
    - while for long-term forecasting, would use a larger window to capture more trends and cycles.

- Model Updating: 
Regularly update your model with new data to capture the latest trends and improve the accuracy of your forecasts.

- Interpretation: 
- Interpret the model’s outputs carefully, considering the confidence intervals or prediction intervals to understand the range of possible future scenarios.
- **LSTM models are powerful because they can capture long-term dependencies and are capable of learning patterns in time series data, which is essential when dealing with the irregularities and uncertainties of the COVID-period123. 
- To monitor the performance of your LSTM model and be prepared to adjust the architecture or retrain the model as more data becomes available or as the situation evolves.

**How to choose the right architecture for my LSTM model?
- Choosing the right architecture for your LSTM (Long Short-Term Memory) model is essential for effective time series forecasting. 
- Select the appropriate architecture for your LPA demands forecasting model:

- Define Your Problem: 
Understand the specifics of your forecasting task, including the nature of the LPA demand data, the impact of the COVID-period, and the level of uncertainty.

- Data Preprocessing: 
<<<<<<< HEAD
Prepare your data by normalising or standardizing it, and structure it into sequences that are suitable for LSTM.
=======
Prepare your data by normalising or standardising it, and structure it into sequences that are suitable for LSTM.
>>>>>>> main

- Determine Sequence Length: 
The length of the input sequences should capture the relevant temporal dependencies. For LPA demands, consider the typical cycles and seasonality.

- Select Number of Layers: 
Start with one or two LSTM layers. Deep LSTMs, with more layers, can model more complex patterns but may also require more data and training time.

- Choose Number of Neurons: 
The number of neurons in each layer should reflect the complexity of the problem. More neurons can capture more information but can also lead to overfitting.

- Decide on Dropout: 
Implement dropout to prevent overfitting, especially if you have a lot of data or a complex model.

- Batch Size and Epochs: 
Choose a batch size and number of epochs that balance the speed of learning with the stability of the convergence.
Optimization and Loss Functions: Select an optimizer like Adam or RMSprop, which are generally good choices for LSTMs. 
Use a loss function that matches your specific forecasting objective, such as mean squared error for regression tasks.

- Regularisation: 
Apply L1 or L2 regularization if you’re dealing with overfitting.

- Hyperparameter Tuning: 
Use techniques like grid search, random search, or Bayesian optimization to systematically explore different model architectures.

- Validation Strategy: 
Use a hold-out validation set or k-fold cross-validation to evaluate the performance of different architectures.

- Model Evaluation: 
Assess the model using appropriate metrics for forecasting, such as MAE (Mean Absolute Error) or RMSE (Root Mean Squared Error).

- Iterative Refinement: 
It’s often an iterative process. You may need to adjust the architecture based on the performance of the model and refine it until you achieve satisfactory results.
There’s no one-size-fits-all architecture for LSTM models. It’s a process of experimentation and refinement to find the architecture that works best for your specific dataset and forecasting needs123. 
Be prepared to iterate and possibly combine different approaches to handle the complexities introduced by the COVID-period effects.


## ##################################################################### ##

# Short-Term Forecasting Using an Exponential Smoothing Model in Excel

#### Time Series Data: 
Organise LPA demand data in a time series format suitable for exponential smoothing.

#### External Factor Adjustment: 
Integrate external factors by adjusting the smoothing factor or by adding a correction term to the forecast equation based on the impact of these factors2.

#### Forecasting: 
Apply the exponential smoothing formula to forecast short-term demand. The formula in Excel would look something like this:Forecast=α×Actual+(1−α)×Previous Forecast

#### Model Validation: 
Compare your forecasts against actual data to validate the model and adjust as necessary.

*Note: the key to successful forecasting is not only in the technical application of these models but also in the continuous refinement and adjustment based on new data and changing external conditions. 
Regularly update your models to incorporate the latest data and external factors to maintain accuracy in your forecasts.

#### Smoothing Factor: 
Determine the smoothing factor (α) that gives more weight to recent observations.

*How do I choose the right smoothing factor (α)?
Choosing the right smoothing factor (α) for exponential smoothing is crucial as it determines how much weight is given to recent observations in the forecasting model.

**Guidelines to help you select an appropriate α:**

##### Understand the Range: 
The smoothing factor α ranges from 0 to 1. A value close to 1 gives more weight to recent observations, making the forecast more responsive to changes. 
Conversely, a value close to 0 makes the forecast more stable and less sensitive to recent fluctuations1.

##### Historical Data Analysis: Look at your historical data. If the data shows that recent trends are a good predictor of future values, 
consider a higher α. If the data is more random or if older values are also important, consider a lower α2.

##### Error Measurement: 
Use error metrics such as Mean Squared Error (MSE) to evaluate the accuracy of forecasts with different α values. 
The α that minimizes the error is typically the best choice3.

##### Trial and Error: 
Start with a value of α, such as 0.5, and adjust it incrementally. Observe how these adjustments affect the forecast accuracy and adjust accordingly4.

##### Model Validation: 
Validate your chosen α by comparing the model’s forecasts against actual outcomes. Adjust the α if the model’s performance can be improved1.

##### Consider External Factors: 
If your model needs to respond quickly to external shocks, such as those caused by the COVID-period, a higher α might be necessary. 
However, be cautious as a high α can also make the model react to random noise2.

##### Use Optimization Techniques: 
Some software packages can automatically optimize α based on historical data. 
In Excel, need to manually test different α values or use Solver to find the optimal value1.
Remember, there is no one-size-fits-all value for α, and it may need to be adjusted over time as the characteristics of the data change. 
It’s important to regularly review and potentially update the smoothing factor to ensure your forecasts remain accurate.

# Which alternative method works the best for the LPA demands forecasting model to deal with covid-period effects on the outcome?

When considering alternative methods for forecasting LPA demands, especially in the context of the COVID-period effects, 
it’s important to choose a method that can adapt to rapid changes and incorporate various external factors. 

*Note: Each of the following methods has its strengths and can be chosen based on the specific needs of your forecasting model, the availability of data, and the level of complexity you’re willing to manage. It’s also beneficial to combine different methods to create a hybrid model that leverages the advantages of each approach.

## Consumer Sentiment Models: 
Incorporating consumer sentiment into models can be particularly effective. This involves using qualitative data on consumer behavior and attitudes, 
which can be critical during unpredictable periods like the COVID-19 pandemic1.
* ref: https://hbr.org/2020/11/a-better-model-for-economic-forecasting-during-the-pandemic
Scenario Analysis: Create scenarios based on different sentiment trends to understand how extreme changes in consumer sentiment could affect LPA demand.

## SPRINT Re-plan Approach: 
McKinsey suggests a SPRINT re-plan approach for rapidly forecasting demand during crises. 
This involves tracking shifts in consumer behavior across several dimensions and adjusting commercial strategies accordingly2.
* ref: https://www.mckinsey.com/industries/consumer-packaged-goods/our-insights/rapidly-forecasting-demand-and-adapting-commercial-plans-in-a-pandemic

## Adaptive Short-Term Models: 
For short-term forecasting, adaptive methods like Generalized Additive Models (GAM) or machine learning algorithms can be useful. 
These models can adjust quickly to new data and are capable of handling non-linear relationships4.
* ref: https://ieeexplore.ieee.org/stampPDF/getPDF.jsp?tp=&arnumber=9382417
* ref: https://hbr.org/2020/11/a-better-model-for-economic-forecasting-during-the-pandemic
* ref: https://www.mckinsey.com/industries/consumer-packaged-goods/our-insights/rapidly-forecasting-demand-and-adapting-commercial-plans-in-a-pandemic

## Simple Models with Lift Adjustment:
In times of disruption, simple models like lift-adjusted seasonal naive methods are recommended. 
These models measure change from the baseline at higher levels of hierarchy and then propagate this lift to lower levels5.

## Enhanced Long-Term Models: 
For long-term forecasting, models that capture the physics of transmission, project human behavioral reactions, 
and reset state variables to account for randomness have been found to correlate with better predictions3.

**How do I incorporate Enhanced Long-Term Models into my cohort forecasting model?**
Incorporating Enhanced Long-Term Models into your cohort forecasting model involves integrating features that have been identified as beneficial for long-term prediction accuracy.
By incorporating these features into your cohort forecasting model, you can enhance its long-term predictive power and 
make it more responsive to the complexities introduced by the COVID-period effects and other external factors.
* ref: https://journals.plos.org/ploscompbiol/article?id=10.1371/journal.pcbi.1010100
* ref: https://towardsdatascience.com/forecasting-with-cohort-based-models-e71003bc7ecd

### Capture the Physics of Transmission: 
Incorporate mechanisms that reflect the actual process of LPA application and approval.
This could involve understanding and modeling the flow of applications through different stages and the factors that influence transition rates at each stage1.

### Project Human Behavioral Reactions:
Include variables that account for how people’s behaviors might change over time in response to external events, such as policy changes or public health crises.
This can be done by analyzing past behaviors in similar situations or by conducting surveys1.

### Reset State Variables: 
Before making long-term projections, reset the state variables in your model to account for randomness and uncertainty not captured in the model. 
This might involve re-evaluating initial conditions or incorporating stochastic elements into the model1.

### Data Transformation: 
Reformat your time series data into tabular data that aligns with cohorts based on registration or application dates. 
This allows you to apply regression models that can offer additional insights regarding the attribution of future LPA demands to specific cohorts2.

### Model Enhancement: 
Consider using lightweight models enhanced with hierarchical decomposition, which can provide precise forecasting with fewer computations and parameters. 
This approach can be particularly useful when dealing with large datasets or complex systems3.
* ref: https://arxiv.org/abs/2401.11929

### Behavioral Feedbacks: 
Integrate endogenous behavioral responses into your model. 
This means that the model should allow for the perceived risk or awareness of LPAs to continuously change the demand through the adoption and relaxation of various behaviors1.

**How do I incorporate Behavioral Feedbacks into my cohort forecasting model?**
Incorporating enhanced behavioral feedbacks into your cohort forecasting model involves integrating behavioral dynamics that can influence the demand for LPAs. 
By following these steps, you can enhance your cohort forecasting model to more accurately reflect the dynamic nature of human behavior and its impact on LPA demand.
* ref: https://journals.plos.org/ploscompbiol/article?id=10.1371/journal.pcbi.1010100
* ref: https://towardsdatascience.com/forecasting-with-cohort-based-models-e71003bc7ecd

#### Behavioral Data Collection: 
Gather data on behaviors that are likely to impact LPA demand, such as changes in legal advice seeking, public awareness of LPAs, and shifts in health-related behaviors due to COVID-19.

#### Quantify Behaviors: 
Convert qualitative behavioral data into quantitative metrics. 
For example, use survey data to create indices representing the level of public awareness or concern about health issues.

#### Cohort Segmentation: 
Segment your cohorts based on behavioral characteristics. This could mean creating sub-cohorts within your data that reflect different behavioral responses.

#### Modeling Behavioral Trends: 
Use regression analysis or other statistical methods to model how these behaviors have impacted LPA demand historically. 
This will help you understand the relationship between behavior and demand1.

#### Incorporate Feedback Loops: 
Create feedback loops in your model where the demand for LPAs influences future behaviors, which in turn affect future demand. 
This can be done by setting up equations or algorithms that simulate this interaction.

#### Adjust for COVID-19 Effects: 
Specifically adjust your behavioral feedbacks to account for the unique impacts of the COVID-19 period. 
This may involve analyzing how behaviors changed during the pandemic and integrating this into your model.

#### Scenario Analysis: 
Run scenario analyses to see how different behavioral responses could impact LPA demand. 
This helps in understanding the potential range of outcomes and preparing for various possibilities.

#### Continuous Updating: 
Keep your model updated with the latest behavioral data to ensure that the feedbacks remain relevant and accurate.

#### Validation and Testing: 
Regularly validate and test your model against actual data to ensure that the behavioral feedbacks are correctly influencing the forecast.


## ##################################################################### ##

# SARIMA
**How to use SARIMA method for the LPA demands short-term and long-term forecasting model to deal with covid-period effects on the outcome and deal with uncertatinty?

The SARIMA (Seasonal Autoregressive Integrated Moving Average) method is a powerful tool for forecasting time series data that exhibits seasonal patterns. 
It’s particularly useful for dealing with uncertainties like those introduced during the COVID-period. 
By following these steps, you can create a robust SARIMA forecasting model that accounts for both seasonal patterns and the uncertain effects of the COVID-period on LPA demand outcomes. 
The key to effective forecasting is not only in the initial model development but also in ongoing model refinement and adaptation to new data.

**Using SARIMA for both short-term and long-term forecasting of LPA demands:**
Understand SARIMA: 
SARIMA extends the ARIMA model by including seasonal terms. It is represented as SARIMA(p, d, q)(P, D, Q, s), where:
( p, d, q ) are the non-seasonal orders for autoregression, differencing, and moving average, respectively.
( P, D, Q ) are the seasonal orders for autoregression, differencing, and moving average.
( s ) is the number of periods in a season1.

#### Data Preparation: 
Organize your LPA demand data into a time series format. Include a timestamp for each data point to identify seasonal patterns.

#### Stationarity Check: 
Ensure your data is stationary, meaning its statistical properties do not change over time. Use differencing to stabilize the mean if necessary.

#### Identify Seasonality: 
Determine the seasonality in your data (e.g., monthly, quarterly) and use seasonal differencing if required to remove seasonal trends and achieve stationarity.

#### Model Selection: 
Choose the SARIMA model parameters (p, d, q, P, D, Q, s) based on your data’s autocorrelation and partial autocorrelation functions. You may need to experiment with different combinations to find the best fit.

#### Incorporate COVID-period Effects: 
To account for the COVID-period effects, you can include dummy variables representing pre-COVID, during-COVID, and post-COVID periods or use external regressors that capture related impacts.

#### Uncertainty Handling: 
Use the confidence intervals provided by the SARIMA model to understand the range of possible outcomes. This helps in dealing with uncertainty in your forecasts.

#### Model Fitting: 
Fit the SARIMA model to your historical LPA demand data, including any COVID-period effects you’ve identified.

#### Validation: 
Validate your model by comparing its forecasts against actual data. Adjust the model parameters if necessary to improve accuracy.

#### Forecasting: 
Once validated, use the SARIMA model to forecast short-term and long-term LPA demands. The model will provide a point forecast along with confidence intervals for each predicted value.

#### Update Regularly: 
Continuously update your model with new data to refine the forecasts and adjust for any changes in trends or seasonal patterns.

### **How do I choose the right SARIMA parameters?**
Choosing the right parameters for a SARIMA model is a critical step in time series forecasting. , the goal is to find a parsimonious model 
that adequately captures the patterns in the data without overfitting. It’s a balance between model complexity and forecast accuracy123.
Here’s a structured approach to selecting the appropriate parameters:

#### Seasonal Identification: 
Determine if your data exhibits a strong seasonal pattern and identify the length of the season (s). This could be based on domain knowledge or exploratory data analysis.

#### Stationarity Check: 
Ensure that your time series is stationary, as SARIMA requires this. You may need to apply differencing to achieve stationarity.

#### ACF and PACF Analysis: 
Use Autocorrelation Function (ACF) and Partial Autocorrelation Function (PACF) plots to get initial estimates of the parameters. The ACF shows the correlation of the time series with its own lagged values, while the PACF shows the partial correlation of the time series with its own lagged values, controlling for the values of the time series at all shorter lags.

#### Parameter Estimation:
Non-seasonal parameters (p, d, q): Look for the lag after which the PACF cuts off for the AR parameter (p), and the lag after which the ACF tails off for the MA parameter (q). The differencing parameter (d) is determined based on the number of differences required to achieve stationarity.
Seasonal parameters (P, D, Q): Similar to the non-seasonal parameters, but you look at the seasonal lags in the ACF and PACF plots. For example, if your data has a seasonal period of 12, you would look at lags 12, 24, 36, etc.

#### Model Fitting: 
Fit the SARIMA model with the chosen parameters and evaluate its performance. Look at the AIC (Akaike Information Criterion) or BIC (Bayesian Information Criterion) values to compare different models.

#### Grid Search: 
Perform a grid search over a range of parameter values to find the optimal combination that minimizes the AIC or BIC.

#### Residual Analysis: 
After fitting the model, analyze the residuals to ensure that there are no patterns (which would suggest that the model can be improved) and that the residuals are approximately normally distributed.

#### Validation: 
Validate the model using a hold-out sample or cross-validation to ensure that it performs well on unseen data.

#### Iterative Refinement: 
It is often an iterative process and may need to go back and forth adjusting the parameters and refitting the model until you find the best parameters for your specific dataset.

<<<<<<< HEAD
=======

# ##################################################################### #

# ##################################################################### #
# Uncertainty in Demand Forecasting:

- The sources of uncertainty are missing information, unreliable information, conflicting information, noisy information, and confusing information.

- Forecasting plays a crucial role in managing demand uncertainty. It involves estimating future demand based on historical data, market trends, and other relevant factors. Accurate forecasting enables businesses to make informed decisions regarding production, procurement, and inventory management.

- Demand uncertainty refers to the external factors that cause demand to unexpectedly increase or decrease. This situation can be caused by a public health crisis or even a sudden shift in the customers' tastes.

- Deterministic risk considers the impact of a single risk scenario, whereas probabilistic risk considers all possible scenarios, their likelihood and associated impacts. The opposite of probabilistic is “deterministic”, which uses exact numbers to approximate uncertain amounts, often some historical average. A symptom of a probabilistic plan or forecast is that its results are generally also expressed as probability distributions.

- Calculate uncertainty by subtracting your average measurement by each measurement calculated, squaring each result and calculating the average of those numbers. With this variance result, calculate its standard deviation by finding the square root of your result. The final result is the uncertainty level of your equation.



# ##################################################################### #
# Different type of forecast (Point / Range (Short-Long) / Demand)
- Point Forecast: associate the future with a single expected outcome, usually an average expected value (not to be confused with the most likely outcome). Example: We forecast to sell 1000 units next month. Probabilistic Forecast: allocates a probability for different events to happen.

- A point forecast only provides a partial view of the future, such as (i.e., the most likely demand in the upcoming time). In contrast, a probability forecast provides even the proper insight and the outcome of demand ( i.e., how much is the risk of demand shortage).

- Range forecasts are more realistic and informative than point forecasts, as they capture the uncertainty and variability of the future, and they can help you assess the risks and opportunities of different scenarios. However, range forecasts also have some challenges. Range forecasts are a type of forecast that can help assess the risks and opportunities of different scenarios by capturing the uncertainty and variability of the future. They can be more realistic and informative than point forecasts.
    - Short-range forecasts typically look ahead three to twelve months, while long-range forecasts look ahead one to five years.
    - Short range forecasts are based off of observed and extrapolated data and how systems are moving. Long range forecasting are created off of computer models.

- However, demand forecasting is also subject to uncertainty, which can arise from various factors such as market fluctuations, customer behavior, seasonal variations, or external shocks.
- The Probabilistic forecasting tool (Monte Carlo) creates reliable forecasts at a fraction of the effort of traditional methods. It also has the added benefit of obtaining “estimate input data” from existing team processes, reducing the frequency or eliminating the need for expensive estimation sessions


# ##################################################################### #
# Economic & Finance: Forecasting productivity

- To forecast of productivity based on average historical growth rates as well as judgments about factors that may cause productivity to deviate from its historical trend in the short-term.
- The most common approach for forecasting productivity is to estimate the trend growth in productivity using aggregate data.
- Productivity growth is very volatile and has undergone large historical shifts with productivity growth averaging around 1% between 1800-1950 followed by an increase in the average annual growth to 3% between 1950-1975. Since the mid-1970’s productivity growth has gradually declined in many developed economies; see panel B of Figure 7. In the decade since 2009, 2% annual productivity growth was an upper bound for most G7 countries.
- Test for shifts in productivity growth rates in other advanced economies did not find evidence of a changes in productivity growth until well after the financial crisis in 2007

- A more recent approach by Martinez et al. (2021) allows for a time-varying long-run trend in UK productivity. They show that are able to broadly replicate the OBR’s forecasts using a quasi-transformed autoregressive model with one lag, a constant, and a trend. The estimated long-run trend is just over 2% per year through 2007 Q4 which is consistent with the OBR’s assumptions about the long-run growth rate of productivity (OBR, 2019). However, it is possible to dramatically improve upon OBR’s forecasts in real-time by allowing for the long-term trend forecast to adjust based on more recent historical patterns. By taking a local average of the last four years of growth rates, Martinez et al. (2021) generate productivity forecasts whose RMSE is on average more than 75% smaller than OBR’s forecasts extending five-years-ahead and is 84% smaller at the longest forecast horizon.

The range-based (RB) volatility models is a general term for the models constructed with high and low prices, and most often with their difference i.e., the price range. A short review and classification of such models is contained in §2.3.14. From practical point of view, it is important that low and high prices are almost always available with daily closing prices for financial series. The price range (or its logarithm) is a significantly more efficient estimator of volatility than the estimator based on closing prices (Alizadeh et al., 2002). Similarly the co-range (the covariance based on price ranges) is a significantly more efficient estimator of the covariance of returns than the estimator based on closing prices (Brunetti & Lildholdt, 2002). For these reasons models based on the price range and the co-range better describe variances and covariances of financial returns than the ones based on closing prices.

## GARCH: Financial time series forecasting with range-based volatility models
The forecasts of volatility of financial returns from the univariate RB models are more accurate than the forecasts from standard GARCH models based on closing prices (see, for example, Mapa (2003) for the GARCH-PARK-R model; Chou (2005) for the CARR model; Fiszeder (2005) for the GARCH-TR model; Brandt & Jones (2006) for the REGARCH model; Chen et al. (2008) for the TARR model; Lin et al. (2012) for the STARR model; Fiszeder & Perczak (2016) for the GARCH model estimated with low, high and closing prices during crisis periods; Molnár (2016) for the RGARCH model)


## Demand Planning Tips for Calculating Forecast Uncertainty
1. Set expectations about error: Sometimes  managers have unreasonable expectations about reducing forecast error to zero. You can point out that error is only one of the dimensions on which a forecasting process must be judged; you may be doing fine on both timeliness and cost. Also point out that zero error is no more realistic a goal than 100% conversion of prospects into customers, perfect supplier performance, or zero stock price volatility.

2. Track down sources of error: Double check the accuracy of demand histories. Use statistical methods to identify outliers in demand histories and react appropriately, replacing verified anomalies with more typical values and omitting data from before major changes in the character of the demand. If you use a collaborative forecasting process, compare its accuracy against a purely statistical approach to identify items for which collaboration does not reduce error.

3. Evaluate the error of alternative statistical methods: There may be off-the-shelf techniques that do better than your current methods, or do better for some subsets of your items. The key is to be empirical, using the idea of holdout analysis. Gather your data and do a “bake off” between different methods to see which work better for you. If you are not already using statistical forecasting methods, compare them against whoever’s “golden gut” is your current standard. Use the naïve forecast as a benchmark in the comparisons.

4. Investigate the use of new data sources: Especially if you have items that are heavily promoted, test out statistical methods that incorporate promotional data into the forecasting process. Also check whether information from outside your company can be exploited; for instance, see whether macroeconomic indicators for your sector can be combined with company data to improve forecast accuracy (this is usually done using a method called multiple regression analysis).

5. Use prediction intervals: Plots of prediction intervals can improve your feel for the uncertainty in your forecasts, helping you select items for additional scrutiny. While it’s true that what you don’t know can hurt you, it’s also true that knowing what you don’t know can help you.

### Criteria for Assessing Forecasts:
Forecast error alone is not reason enough to reject forecasting as a management tool. To twist a famous aphorism by George Box, “All forecasts are wrong, but some are useful.” Of course, business professionals will always search for ways to make forecasts more useful. This usually involves work to reduce forecast error. But while forecast accuracy is the most obvious criterion by which to judge forecasts, but it is not the only one. Here’s a list of criteria for evaluating forecasts:

### Accuracy:
Forecasts of future values should, in retrospect, be very close to the actual values that eventually reveal themselves. But there may be diminishing returns to squeezing another half percent of accuracy out of forecasts otherwise good enough to use in decision making.

### Timeliness: 
Fighter pilots refer to the OODA Loop (Observe, Orient, Decide, and Act) and the “need to get inside the enemy’s OODA loop” so they can shoot first. Businesses too have decision cycles. Delivering a perfectly accurate forecast the day after it was needed is not helpful. Better is a good forecast that arrives in time to be useful.

### Cost: 
Forecasting data, models, processes and people all cost money.  A less expensive forecast might be fueled by data that are readily available; more expensive would be a forecast that runs on data that have to be collected in a special process outside the scope of a firm’s information infrastructure.  A classic, off-the-shelf forecasting technique will be less costly to acquire, feed and exploit than a complex, custom, consultant-supplied method. Forecasts could be mass-produced by software overseen by a single analyst, or they might emerge from a collaborative process requiring time and effort from large groups of people, such as district sales managers, production teams, and others. Technically advanced forecasting techniques often require hiring staff with specialized technical expertise, such as a master’s degree in statistics, who tend to cost more than staff with less advanced training.

### Credibility: 
Ultimately, some executive has to accept and act on each forecast. Executives have a tendency to distrust or ignore recommendations that they can neither understand nor explain to the next person above them in the hierarchy. For many, believing in a “black box” is too severe a test of faith, and they reject the black box’s forecasts in favor of something more transparent.

### Sources of Forecast Error
1. The data that goes into a forecasting model
2. The model itself
3. The context of the forecasting exercise

#### There are several ways in which data problems can lead to forecast error.

- Gross errors: 
Wrong data produce wrong forecasts. We have seen an instance in which computer records of product demand were wrong by a factor of two! Those involved spotted that problem immediately, but a less egregious situation can easily slip through to poison the forecasting process. In fact, just organizing, acquiring and checking data is often the largest source of delay in the implementation of forecasting software. Many data problems seem to derive from the data having been unimportant until a forecasting project made them important.

- Anomalies: 
Even with perfectly curated forecasting databases, there are often “needle in a haystack” type data problems. In these cases, it is not data errors but demand anomalies that contribute to forecast error. In a set of, say, 50,000 products, some number of items are likely to have odd details that can distort forecasts.

- Holdout analysis is a simple but powerful method of analysis:
To see how well a method forecasts, use it with older known data to forecast newer data, then see how it would have turned out! For instance, suppose you have 36 months of demand data and need to forecast 3 months ahead. You can simulate the forecasting process by holding out (i.e., hiding) the most recent 3 months of data, forecasting using only data from months 1 to 33, then comparing the forecasts for months 34-36 against the actual values in months 34-36. Sliding simulation merely repeats the holdout analysis, sliding along the demand history. The example above used the first 33 months of data to get 3 estimates of forecast error. Suppose we start the process by using the first 12 months to forecast the next 3. Then we slide forward and use the first 13 months to forecast the next 3. We continue until finally we use the first 35 months to forecast the last month, giving us one more estimate of the error we make when forecasting one month ahead. Summarizing all the 1-step ahead, 2-step ahead and 3-step ahead forecast errors provides a way to calculate prediction intervals.

### Calculating Prediction Intervals

- The final step in calculating prediction intervals is to convert the estimates of average absolute error into the upper and lower limits of the prediction interval. The prediction interval at any future time is computed as:
     - Prediction interval = Forecast ± Multiplier x Average absolute error.

- The final step is the choice of the multiplier. 
The typical approach is to imagine some probability distribution of error around the forecast, then estimate the ends of the prediction interval using appropriate percentiles of that distribution. Usually, the assumed distribution of error is the Normal distribution, also called the Gaussian distribution or the “bell-shaped curve”.

### Use of Prediction Intervals
- The most immediate, informal use of prediction intervals is to convey a sense of how “squishy” a forecast is. 
- Prediction intervals that are wide compared to the size of the forecasts indicate high uncertainty.

- There are two more formal uses in demand forecasting: 
    - Hedging your bets: 
        The forecast values themselves approximate the most likely values of future demand. A more ominous way to say the same thing is that there is about a 50% chance that the actual value will be above (or below) the forecast. If the forecast is being used to plan future production (or raw materials purchase or hiring), you might want to build in a cushion to keep from being caught short if demand spikes (assuming that under-building is worse than over-building). If the forecast is converted from units to dollars for revenue projections, you might want to use a value below the forecast to be conservative in projecting cash flow. In either case, you first have to choose the coverage of the prediction interval. A 90% prediction interval is a range of values that covers 90% of the possibilities. This implies that there is a 5% chance of a value falling above the upper limit of the 90% prediction interval. In other words, the upper limit of a 90% prediction interval marks the 95th percentile of the distribution of predicted demand at that time period. Similarly, there is a 5% chance of falling below the lower limit, which marks the 5th percentile of the demand distribution.

    - Guiding forecast adjustment: 
        It is quite common for statistical forecasts to be revised by some sort of collaborative process. These adjustments are based on information not recorded in an item’s demand history, such as intelligence about competitor actions. Sometimes they are based on a more vaporous source, such as sales force optimism. When the adjustments are made on-screen for all to see, the prediction intervals provide a useful reference: If someone wants to move the forecasts outside the prediction intervals, they are crossing a fact-based line and should have a good story to justify their argument that things will be really different in the future.

### Prediction Intervals and Inventory Optimisation

#### Python code:
*ref: https://towardsdatascience.com/time-series-forecasting-prediction-intervals-360b1bf4b085

- Finally, the concept behind prediction intervals play an essential role in a problem related to demand forecasting: Inventory Optimization.
The core analytic task in setting reorder points (also called Mins) is to forecast total demand over a replenishment lead time. This total is called the lead time demand. When on-hand inventory falls down to or below the reorder point, a replenishment order is triggered. If the reorder point is high enough, there will be an acceptably small risk of a stockout, i.e., of lead time demand driving inventory below zero and creating either lost sales or backorders.

- SDP_Screenshot new statistical methods planning 
New statistical methods, and we can start planning more effectively.

- The forecasting task is to determine all the possible values of cumulative demand over the lead time and their associated probabilities of occurring. In other words, the basic task is to determine a prediction interval for some future random variable. Suppose you have computed a 90% prediction interval for lead time demand. Then the upper end of the interval represents the 95th percentile of the distribution. Setting the reorder point at this level will accommodate 95% of the possible lead time demand values, meaning there will be only a 5% chance of stocking out before replenishment arrives to re-stock the shelves. Thus there is an intimate relationship between prediction intervals in demand forecasting and calculation of reorder points in inventory optimization.


# ##################################################################### #
# Data-driven Forecasting methods to consider uncertainty:


*ref: https://www.sciencedirect.com/science/article/pii/S0169207021001758?via%3Dihub
*ref: https://forecasting-encyclopedia.com/theory.html#Forecasting_on_distributed_systems
## ##################################################################### ##
## Scenarios and judgmental forecasting


## ##################################################################### ##
## Multi-step ahead forecasting


## ##################################################################### ##
## Agent-based models


## ##################################################################### ##
# Age-specific Forecasting: 

## naive freezing of age-specific rates
, and those that can differ greatly in method complexity (see also §2.5.2). A recent survey of fertility forecasting practice in European statistical offices (Gleditsch & Syse, 2020) found that forecasts tend to be deterministic and make use of expert panels (see §2.11.4). Expert elicitation techniques are gaining in sophistication, highlighted by the protocol of Statistics Canada (Dion, Galbraith, & Sirag, 2020) which requests a full probability distribution of the TFR.

A promising avenue is the development of forecasting methods that incorporate birth order (parity) information, supported by evidence from individual-level analyses (for example, Fiori, Graham, & Feng, 2014). 

- Another underexplored area is the integration of survey data into fertility forecasting models, which tend to use vital statistics alone when they are of sufficient quality (see Rendall, Handcock, & Jonsson, 2009; Zhang & Bryant, 2019 for Bayesian fertility estimation with imperfect census data). 

- To predict the effect of COVID-19 on US fertility in the absence of vital statistics, Wilde, Chen, & Lohmann (2020) use Google data to. 

- investigation of the possible long-term impacts of delayed motherhood in high-income countries, alongside developments in assisted reproduction technology such as egg freezing, is required (see, for example, Sobotka & Beaujouan, 2018).

- Bayesian hierarchical models is useful for forecasting data structured by age (Raymer & Wiśniowski, 2018). In some cases, the methods additionally involve selection and combining forecasts through Bayesian model selection and averaging (Bijak, 2010 see also §2.5 and §2.6). Such models can be expected to produce reasonable forecasts (and errors) for up to a decade ahead (Bijak & Wiśniowski, 2010), although this depends on the migration flows being forecast, with some processes (e.g., family migration) more predictable than other (e.g., asylum). Another recognised problem with models using covariates is that those can be endogenous to migration (e.g., population) and also need predicting, which necessitates applying structured models to prevent uncertainty from exploding.

- The methodological gaps and current work in migration forecasting concentrate in a few key areas, notably including causal (mechanistic) forecasting based on the process of migrant decision making (Willekens, 2018); as well as early warnings and ‘nowcasting’ of rapidly changing trends, for example in asylum migration (Napierała, Hilton, Forster, Carammia, & Bijak, 2021). In the context of early warnings, forays into data-driven methods for changepoint detection, possibly coupled with the digital trace and other high-frequency ‘Big data’, bear particular promise. At the same time, coherent uncertainty description across a range of time horizons, especially in the long range (Azose & Raftery, 2015), remains a challenge, which needs addressing for the sake of proper calibration of errors in the population forecasts, to which these migration components contribute.

## time-series extrapolation
time-series extrapolation approach has the advantage of obtaining a forecast probability distribution rather than a deterministic point forecast and, also, enable the determination of forecast intervals

## Models for population processes

Over the past two centuries, formal demography has established its own, discipline-specific body of methods for predicting (or projecting15) populations. Population sciences, since their 17th century beginnings, have been traditionally very empirically focused, with strong links with probability theory (Courgeau, 2012). Given the observed regularities in population dynamics, and that populations are somewhat better predictable than many other socio-economic processes, with reasonable horizons possibly up to one generation ahead (Keyfitz, 1972, 1981), demographic forecasts have become a bestselling product of the discipline (Xie, 2000). Since the 20th century, methodological developments in human demography have been augmented by the work carried out in mathematical biology and population ecology (Caswell, 2019a).

The theoretical strength of demography also lies almost exclusively in the formal mathematical description of population processes (Burch, 2018), typically growth functions and structural changes. Historically, such attempts started from formulating the logistic model of population dynamics, inspired by the Malthusian theory (Pearl & Reed, 1920; Verhulst, 1845). Lotka (1907)’s work laid the foundations of the stable population theory with asymptotic stability under constant vital rates, subsequently extended to modelling of interacting populations by using differential equations (Lotka, 1925; V Volterra, 1926). By the middle of the 20th century, the potential and limitations of demographic forecasting methods were already well recognised in the literature (Brass, 1974; Hajnal, 1955).

In the state-of-the-art demographic forecasting, the core engine is provided by matrix algebra. The most common approach relies on the cohort-component models, which combine the assumptions on fertility, mortality and migration, in order to produce future population by age, sex, and other characteristics. In such models, the deterministic mechanism of population renewal is known, and results from the following demographic accounting identity (population balancing equation, see Rees & Wilson, 1973; Bryant & Zhang, 2018):
 , where  P t is the population vector structured by age (and other characteristics), and G is an appropriately chosen growth matrix (Leslie matrix), closely linked with the life table while reflecting the relationship above, expressed in terms of rates rather than events (Caswell, 2019a; Leslie, 1945, 1948; Preston, Heuveline, & Guillot, 2000).

In the cohort-component approach, even though the mechanism of population change is known, the individual components still need forecasting. The three main drivers of population dynamics — fertility, mortality, and migration — differ in terms of their predictability (National Research Council, 2000): mortality, which is mainly a biological process moderated by medical technology, is the most predictable; migration, which is purely a social and behavioural process is the least; while the predictability of fertility — part-biological, part-behavioural – is in the middle (for component forecasting methods, see §3.6.3, §3.6.4, and §3.6.5). In practical applications, the components can be either projected deterministically, following judgment-based or expert assumptions (for example, Lutz, Butz, & Samir, 2017), or extrapolated by using probabilistic methods, either for the components or for past errors of prediction (Alho & Spencer, 1985, 2005; De Beer, 2008). An impetus to the use of stochastic methods has been given by the developments in the UN World Population Prospects (Azose, Ševčı́ková, & Raftery, 2016; Gerland et al., 2014). Parallel, theoretical advancements included a stochastic version of the stable population theory (Keiding & Hoem, 1976), as well as coupling of demographic uncertainty with economic models (Alho, Hougaard Jensen, & Lassila, 2008).

Since its original formulation, the cohort-component model has been subject to several extensions (see, for example, Stillwell & Clarke, 2011). The multiregional model (Rogers, 1975) describes the dynamics of multiple regional populations at the same time, with regions linked through migration. The multistate model (Schoen, 1987) generalises the multiregional analysis to any arbitrary set of states (such as educational, marital, health, or employment statuses, and so on; see also state-space models in §2.3.6). The multiregional model can be in turn generalised to include multiple geographic levels of analysis in a coherent way (Kupiszewski & Kupiszewska, 2011). Recent developments include multifocal analysis, with an algebraic description of kinship networks (Caswell, 2019b, 2020). For all these extensions, however, data requirements are very high: such models require detailed information on transitions between regions or states in a range of different breakdowns. For pragmatic reasons, microsimulation-based methods offer an appealing alternative, typically including large-sample Monte Carlo simulations of population trajectories based on available transition rates (Bélanger & Sabourin, 2017; Zaidi, Harding, & Williamson, 2009).

Aside of a few extensions listed above, the current methodological developments in the forecasting of human populations are mainly concentrated on the approaches for predicting individual demographic components (see §3.6.3, §3.6.4, and §3.6.5), rather than the description of the population renewal mechanism. Still, the continuing developments in population ecology, for example on the algebraic description of transient and asymptotic population growth (Nicol-Harper et al., 2018), bear substantial promise of further advancements in this area, which can be additionally helped by strengthened collaboration between modellers and forecasters working across the disciplinary boundaries on the formal descriptions of the dynamics of human, as well as other populations.


## ##################################################################### ##
## Bagging for time series forecasting:
For data-driven methods, to forecasting and simulation time series and deal with predictors ensembles, bagging has shown as a powerful tool.
A general framework for ensemble forecasting methods involves four main stages: (i) data treatment, (ii) resampling, (iii) forecasting, and (iv) aggregation. However, for time series, bootstrap should be done carefully, as the serial dependence and non-stationarity must be considered.

The bagging can handle all of three sources of uncertainty: model form, data, and parameter, Petropoulos et al. (2018a) showed that simply tackling model uncertainty is enough for achieving a superior performance, leading to the proposal of a Bootstrap Model Combination (BMC) approach, where different model forms are identified in the ensemble and fitted to the original data.

## ##################################################################### ##
## Clustering-based forecasting
The robustness of the forecasting process depends mainly on the characteristics of the target variable. In cases of high nonlinear and volatile time series, a forecasting model may not be able to fully capture and simulate the special characteristics, a fact that may lead to poor forecasting accuracy (Pradeepkumar & Ravi, 2017). Contemporary research has proposed some approaches to increase the forecasting performance (Sardinha-Lourenço, Andrade-Campos, Antunes, & Oliveira, 2018). Clustering-based forecasting refers to the application of unsupervised machine learning in forecasting tasks. The scope is to increase the performance by employing the information of data structure and of the existing similarities among the data entriee.

### Pattern Sequence Based Forecasting (PSF)

It takes univariate time series data as input and assist to forecast its future values. This algorithm forecasts the behavior of time series based on similarity of pattern sequences. Initially, clustering is done with the labeling of samples from database. The labels associated with samples are then used for forecasting the future behaviour of time series data.

The Algorithm Pattern Sequence based Forecasting (PSF) was first proposed by Martinez Alvarez, et al., 2008 and then modified and suggested improvement by Martinez Alvarez, et al., 2011. The technical detailes are mentioned in referenced articles. PSF algorithm consists of various statistical operations like:

Data Normalization/ Denormalization
Calculation of optimum Window size (W)
Calculation of optimum cluster size (k)
Pattern Sequence based Forecasting
RMSE/MAE Calculation, etc..
## ##################################################################### ##
>>>>>>> main
