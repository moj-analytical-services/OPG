{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f9dce19a-0b8e-4890-a9d2-d22dc9a1aba4",
   "metadata": {},
   "source": [
    "# OPG : LPA Data Pre-processing and Cleaning tool\n",
    "\n",
    "==============================================================================\n",
    "\n",
    " OPG Demand Forecast modelling for LPA\n",
    " \n",
    " @author: Leila Yousefi\n",
    " \n",
    " MoJ Modelling Hub\n",
    " \n",
    "============================================================================== "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7517bdf9-08ba-4cc1-8c2f-b01ffdf7971d",
   "metadata": {},
   "source": [
    "## Setup\n",
    "\n",
    "Before you can run this project, you need to install some Python packages using the terminal:\n",
    "\n",
    "\n",
    "### create and activate  a virtual environment\n",
    "1. cd OPG\n",
    "2. python3 -m venv venv\n",
    "3. source venv/bin/activate\n",
    "\n",
    "### install the python packages required\n",
    "4. pip install --upgrade pip\n",
    "5. pip install -r requirements.txt\n",
    "\n",
    "### Updating your branch with main\n",
    "When working on your models it is likely that your branch will get out of date with the main branch. To update you branch with the latest changes from main open a terminal and run the following:\n",
    "\n",
    "Check your working tree, commit/push any changes if required\n",
    "\n",
    "git status\n",
    "Switch to the main branch and collect the latest changes, if any\n",
    "\n",
    "git switch main\n",
    "git fetch\n",
    "git pull\n",
    "Switch back to your branch and merge in the changes from main\n",
    "\n",
    "git switch <your initial>/model-a-development\n",
    "git merge main -m \"update branch with main\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf48e8d4-54cd-4d6e-aa17-92621c1bef97",
   "metadata": {},
   "source": [
    "# Installing the required packages:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e765f151-ee2c-44b2-8fcd-0a438f11633c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Un-comment and Run the below code if there is an error with packages installation:\n",
    "\n",
    "#!pip install arrow_pd_parser\n",
    "#!pip install pydbtools\n",
    "#!pip install arrow_pd_parser\n",
    "#!pip install pydbtools\n",
    "#!pip install xlsxwriter\n",
    "\n",
    "\n",
    "##You can add lines to install the required packages"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62438500-01c6-4434-89b1-37ebddfa540b",
   "metadata": {},
   "source": [
    "# Importing the required packages:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83bed50b-9b72-4d69-9326-fd1f690522fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import sys\n",
    "print(sys.path)\n",
    "\n",
    "import os.path\n",
    "from os import path\n",
    "os.getcwd()\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "#import awswrangler as wr\n",
    "#import statsmodels.api as sm\n",
    "#import tensorflow as tf\n",
    "import boto3\n",
    "import getpass\n",
    "import pytz\n",
    "#import openpyxl\n",
    "#import matplotlib\n",
    "import csv\n",
    "from arrow_pd_parser import reader, writer\n",
    "import shutil\n",
    "import pydbtools as pydb\n",
    "import datetime as dt\n",
    "from datetime import timedelta\n",
    "from datetime import datetime\n",
    "from datetime import date\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "%matplotlib inline\n",
    "from dateutil.relativedelta import relativedelta\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from io import StringIO\n",
    "\n",
    "#from tensorflow import keras\n",
    "import matplotlib as mpl\n",
    "mpl.rcParams['agg.path.chunksize'] = 10000\n",
    "from matplotlib import rc\n",
    "import statsmodels.api as sm\n",
    "\n",
    "# consistent plot size wherever not specifiied\n",
    "from pylab import rcParams\n",
    "mpl.rcParams['figure.figsize'] = (15,8)\n",
    "mpl.rcParams['axes.grid'] = False\n",
    "rcParams['xtick.labelsize'] = 14\n",
    "rcParams['ytick.labelsize'] = 14\n",
    "rcParams['axes.labelsize'] = 14\n",
    "\n",
    "%matplotlib inline\n",
    "%config InlineBackend.figure_format='retina'\n",
    "\n",
    "import xlsxwriter\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib as mpl\n",
    "from matplotlib import rc\n",
    "import statsmodels.api as sm\n",
    "\n",
    "# consistent plot size wherever not specifiied\n",
    "from pylab import rcParams\n",
    "mpl.rcParams['figure.figsize'] = (15,8)\n",
    "mpl.rcParams['axes.grid'] = False\n",
    "rcParams['xtick.labelsize'] = 14\n",
    "rcParams['ytick.labelsize'] = 14\n",
    "rcParams['axes.labelsize'] = 14\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd602cf8-4885-43a5-82a9-429f8ce730b0",
   "metadata": {},
   "source": [
    "# LPA Data Import from Dom1\n",
    "\n",
    "This was used previously before transfering data to AWS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07194ab2-9571-406a-85f9-af8d699de142",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ## Enter the corresponding bucket name\n",
    "# bucketName = \"alpha-opg-analytical\"\n",
    "\n",
    "# ##For Automation import getpass\n",
    "# #bucketName = getpass.getpass()\n",
    "\n",
    "\n",
    "# ## Select the folder\n",
    "# folderPath = \"sirius_data_cuts_3\"\n",
    "\n",
    "\n",
    "# ## Set the folder in which the final output will be uploade to in S3\n",
    "# #output_path = f\"s3://alpha-opg-analytical/\" + folderPath + \"/\"\n",
    "\n",
    "# ## Then create a new excel file and copy the previous record from the S3 buckets and add the newly copied raws\n",
    "# ## Finaly convert the excel file to csv and upload it in the following path:\n",
    "# ## s3://alpha-opg-analytical/sirius_data_cuts_3/\n",
    "\n",
    "\n",
    "# ## Explore the s3 bucket path\n",
    "# path_s3 = f\"s3://{bucketName}/{folderPath}/*.csv\"\n",
    "# print ([path_s3])\n",
    "\n",
    "\n",
    "# ## Listing CSV Files in an S3 Bucket Folder: \n",
    "# ### To list all CSV files in a specific folder within an S3 bucket, we can use the AWS CLI or the boto3 Python library. \n",
    "# ###list all files in a specific folder within an S3 bucket Using AWS CLI:\n",
    "# #aws s3 ls s3://your-bucket-name/your-folder-name/ --recursive\n",
    "\n",
    "# ### lists all CSV files in a specific folder within an S3 bucket using boto3:\n",
    "# def list_csv_files(bucketName, folderPath):\n",
    "#     s3 = boto3.resource('s3')\n",
    "#     bucket = s3.Bucket(bucketName)\n",
    "#     for obj in bucket.objects.filter(Prefix=folderPath):\n",
    "#         if obj.key.endswith('.csv'):\n",
    "#             print(obj.key)\n",
    "\n",
    "\n",
    "# ## list all csv in the current folder in s3 bucket:\n",
    "# ### Getting the List of CSV Files\n",
    "# csv_files = list_csv_files(bucketName, folderPath)\n",
    "# print([csv_files])\n",
    "\n",
    "\n",
    "# ## Check if the path exists:\n",
    "# s3 = boto3.resource('s3')\n",
    "# bucket = s3.Bucket(bucketName)\n",
    "\n",
    "# def IsObjectExists(path):\n",
    "#     for object_summary in bucket.objects.filter(Prefix=path):\n",
    "#         return True\n",
    "#     return False\n",
    "\n",
    "# for fileName in [csv_files]:\n",
    "#     if(IsObjectExists(f\"{folderPath}/{fileName}\")):\n",
    "#         print(\"Path for the actual LPA data exists\")\n",
    "#     else:\n",
    "#         print(\"Path for the actual LPA data doesn't exists\")\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19756c99-6632-4f3f-b123-5f9394198786",
   "metadata": {},
   "source": [
    "# S3 Bucket Data Extraction for LPA Data (actuals)\n",
    "\n",
    "These will be used when extracting the raw data from the S3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abb24022-7229-4377-828f-44437c18cf9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download and upload the LPA actual data into the S3 bucket\n",
    "\n",
    "## Enter the corresponding S3 bucket name\n",
    "bucketName = \"alpha-opg-analytical\"\n",
    "\n",
    "##For Automation import getpass\n",
    "#bucketName = getpass.getpass()\n",
    "\n",
    "\n",
    "## Select the corresponding folder includes new LPA data in S3 bucket:\n",
    "folderPath = \"sirius_data_cuts_3\"\n",
    "\n",
    "\n",
    "## Set the folder in which the final output will be uploade to in S3\n",
    "#output_path = f\"s3://alpha-opg-analytical/\" + folderPath + \"/\"\n",
    "\n",
    "## Then create a new excel file and copy the previous record from the S3 buckets and add the newly copied raws\n",
    "## Finaly convert the excel file to csv and upload it in the following path:\n",
    "## s3://alpha-opg-analytical/sirius_data_cuts_3/\n",
    "\n",
    "# Initialize the S3 client\n",
    "s3_client = boto3.client('s3')\n",
    "\n",
    "# Specify your bucket name and folder (prefix)\n",
    "bucket_name = bucketName\n",
    "folder_prefix = 'sirius_data_cuts_3/'\n",
    "\n",
    "# List objects in the specified folder\n",
    "response = s3_client.list_objects_v2(Bucket=bucket_name, Prefix=folder_prefix)\n",
    "\n",
    "# Extract the keys (file names) from the response\n",
    "file_keys = [obj['Key'] for obj in response.get('Contents', [])]\n",
    "\n",
    "# Filter out None values (if any)\n",
    "non_none_file_keys = [key for key in file_keys if key is not None]\n",
    "#print(non_none_file_keys)\n",
    "\n",
    "# Remove folder prefix from file keys\n",
    "file_names = [os.path.basename(key) for key in non_none_file_keys]\n",
    "#print(file_names)\n",
    "\n",
    "csv_extension = '.csv'\n",
    "filtered_file_names = [fn for fn in file_names if fn.lower().endswith(csv_extension)]\n",
    "\n",
    "print(filtered_file_names)\n",
    "\n",
    "# ## Explore the s3 bucket path\n",
    "# path_s3 = f\"s3://{bucketName}/{folderPath}/\"\n",
    "# print ([path_s3])\n",
    "\n",
    "# ## Check if the path exists:\n",
    "# s3 = boto3.resource('s3')\n",
    "# bucket = s3.Bucket(bucketName)\n",
    "\n",
    "# def IsObjectExists(path):\n",
    "#     for object_summary in bucket.objects.filter(Prefix=path):\n",
    "#         return True\n",
    "#     return False\n",
    "\n",
    "# if(IsObjectExists(path_s3)):\n",
    "#     print(\"Path for the actual data exists\")\n",
    "# else:\n",
    "#     print(\"Path for the actual data doesn't exists\")\n",
    "\n",
    "\n",
    "## Listing CSV Files in an S3 Bucket Folder: \n",
    "### To list all CSV files in a specific folder within an S3 bucket, we can use the AWS CLI or the boto3 Python library. \n",
    "###list all files in a specific folder within an S3 bucket Using AWS CLI:\n",
    "#aws s3 ls s3://your-bucket-name/your-folder-name/ --recursive\n",
    "\n",
    "# ### lists all CSV files in a specific folder within an S3 bucket using boto3:\n",
    "# def list_csv_files(bucketName, folderPath):\n",
    "#     s3 = boto3.resource('s3')\n",
    "#     bucket = s3.Bucket(bucketName)\n",
    "#     for obj in bucket.objects.filter(Prefix=folderPath):\n",
    "#         if obj.key.endswith('.csv'):\n",
    "#             print(obj.key)\n",
    "\n",
    "\n",
    "# ## list all csv in the current folder in s3 bucket:\n",
    "# ### Getting the List of CSV Files\n",
    "# csv_files = list_csv_files(bucketName, folderPath)\n",
    "# print([csv_files])\n",
    "\n",
    "\n",
    "# ## Opening CSV Files Based on Selected Column and Condition: \n",
    "# def process_csv_file(file_path, selected_column, condition):\n",
    "#     df = pd.read_csv(file_path)\n",
    "#     filtered_df = df[df[selected_column] == condition]\n",
    "#     # Do further processing with the filtered data\n",
    "\n",
    "# ## pre-process the csv file to only show the required columns (important variable for the BAU):\n",
    "# process_csv_file('path/to/your-csv-file.csv', 'column_name', 'desired_value')\n",
    "\n",
    "\n",
    "# # ## Enter your file name\n",
    "# # fileName = \"d24_2.csv\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3556dfa4-cdcc-49b2-be16-811d5528b39c",
   "metadata": {},
   "source": [
    "# Query the warehouse tables directly from Python/R"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6db194d-65ed-489c-b1d5-1e4f13a3a7aa",
   "metadata": {},
   "source": [
    "\n",
    "    \"\"\"\n",
    "    with events as (\n",
    "        select *\n",
    "        from \"dim_guardianship_dev_dbt\".\"fct_case_receipts\"\n",
    "        where extract_type = 'latest_extract'\n",
    "            and receipt_date >= date_parse('01-01-2008', '%d-%m-%Y')\n",
    "    ),\n",
    "    dates as (\n",
    "        select *\n",
    "        from \"common_lookup_dev_dbt\".\"dim_date\"\n",
    "    ),\n",
    "    donors as (\n",
    "        select *\n",
    "        from \"dim_guardianship_dev_dbt\".\"dim_donors\"\n",
    "    ),\n",
    "    cases as (\n",
    "        select *\n",
    "        from \"dim_guardianship_dev_dbt\".\"dim_cases\"\n",
    "    ),\n",
    "    attributes as (\n",
    "        select dates.calendar_year as receipt_year,\n",
    "            events.receipt_date,\n",
    "            cases.case_id,\n",
    "            cases.case_type,\n",
    "            cases.case_subtype,\n",
    "            cases.case_status,\n",
    "            cases.donor_age_at_receipt,\n",
    "            donors.gender,\n",
    "            donors.region_name,\n",
    "            events.extract_date\n",
    "        from events\n",
    "            left join dates on events.receipt_date = dates.date_name\n",
    "            left join cases on events.extract_case_id = ces.extract_case_id\n",
    "            left join donors on events.extract_donor_id = donors.extract_donor_id\n",
    "    )\n",
    "    select *\n",
    "    from attributes\n",
    "    \"\"\"\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e82badb8-20ff-494f-8f18-ea433d17f6a0",
   "metadata": {},
   "source": [
    "# Reading in Data\n",
    "\n",
    "This extracts a list of Power of Attorney receipts with the following columns: ['receiptdate', 'uid', 'type', 'casesubtype', 'status', 'donor_postcode', 'donor_gender', 'age']."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92662597-d37b-417e-b335-5bfb590c8b39",
   "metadata": {},
   "outputs": [],
   "source": [
    "# read_csv_files function for Reading in CSV Files in an S3 Bucket Folder\n",
    "\n",
    "def read_csv_files(bucket_name, file_names, selected_columns):\n",
    "    \"\"\"\n",
    "        This function is written to read in the data from all of CSV files in the corresponding directory in the S3 bucket\n",
    "        by using input variables:\n",
    "        the S3 bucket name,\n",
    "        file_names \n",
    "        and the selected_columns \n",
    "        The output are the CSV files in the list of dataframes: dfs \n",
    "    \"\"\"\n",
    "    dfs = {}  # Dictionary to store DataFrames\n",
    "\n",
    "    # Initialize the S3 client\n",
    "    s3_client = boto3.client('s3')\n",
    "\n",
    "    for file_name in file_names:\n",
    "        s3_path = f's3://{bucket_name}/{file_name}'\n",
    "        try:\n",
    "            # Read the CSV data into a Pandas DataFrame\n",
    "            csv_obj = s3_client.get_object(Bucket=bucket_name, Key=f'{folderPath}/{file_name}')\n",
    "            csv_string = csv_obj['Body'].read().decode('utf-8')\n",
    "            df = pd.read_csv(StringIO(csv_string))\n",
    "\n",
    "            # Select specific columns\n",
    "            df_selected = df[selected_columns]\n",
    "            dfs[file_name] = df_selected\n",
    "        except Exception as e:\n",
    "            print(f\"Error reading {file_name}: {e}\")\n",
    "\n",
    "    return dfs\n",
    "\n",
    "\n",
    "#bucket_name = bucketName\n",
    "#file_names = ['file1.csv', 'file2.csv']  # Replace with your actual file names\n",
    "\n",
    "## Filter the required variables from the datafarame:\n",
    "selected_columns = [\"receiptdate\",\"cases_glueexporteddate\",\"uid\",\"type\",\"casesubtype\",\"status\",\"donor_dob\",\"donor_postcode\",\"donor_gender\"]  # Replace with desired column names\n",
    "\n",
    "## The read_csv_files function for Reading in CSV Files in an S3 Bucket Folder:\n",
    "dataframes = read_csv_files(bucket_name, filtered_file_names, selected_columns)\n",
    "\n",
    "## Access individual DataFrames by file name\n",
    "for file_name, df_selected in dataframes.items():\n",
    "    print(f\"DataFrame for {file_name}:\")\n",
    "    print(df_selected.head())\n",
    "    \n",
    "## Concatenating DataFrames: \n",
    "### After reading all CSV files, you can concatenate the DataFrames using pd.concat:\n",
    "combined_df = pd.concat(dataframes, ignore_index=True)\n",
    "print(combined_df)\n",
    "\n",
    "## Writing Back to S3: Finally, write the combined DataFrame back to S3:\n",
    "#combined_data_encoded = combined_df.to_csv(None, index=False).encode('utf-8')\n",
    "#combined_file_name = 'combined_data.csv'  # Choose a suitable file name\n",
    "#s3_client.put_object(Body=combined_data_encoded, Bucket=bucket_name, Key=combined_file_name\n",
    "\n",
    "## Identify the type of data set and pre-processing: \n",
    "## Import, manipulate, and clean the data and impute missing values\n",
    "\n",
    "## Column renaming:\n",
    "#df1.rename(columns={'old_col1': 'common_col1', 'old_col2': 'common_col2'}, inplace=True)\n",
    "\n",
    "## Handling Data Mismatch:\n",
    "###Be cautious when combining data with different structures. If a column has incompatible data types (e.g., mixing strings and numbers), you may need to convert or handle them appropriately.\n",
    "#combined_df['numeric_col'] = pd.to_numeric(combined_df['numeric_col'], errors='coerce')\n",
    "\n",
    "## Aggregating Data:\n",
    "###If the DataFrames have different structures, consider aggregating them based on a common identifier (e.g., date or unique ID).\n",
    "#combined_df = df1.groupby('product_id').sum()  # Aggregate by product ID\n",
    "\n",
    "## merge DataFrames based on a common identifier:\n",
    "#merged_df = pd.merge(df1, df2, on='ID', how='inner')\n",
    "#print(merged_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f991a0c6-dc43-4c09-83e6-773c016d3d7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#     ## Select Date\n",
    "#     start_date = '2018-06-01' # start date for the train set\n",
    "#     start_prediction ='2023-02-01' # The end date for the train set\n",
    "#     end_prediction ='2024-02-01' # test / Validation set\n",
    "\n",
    "\n",
    "\n",
    "## Import the dataset and read in the actual data\n",
    "#df = wr.s3.read_csv([path1_s3], sep = ',', parse_dates=True) #import divorce data\n",
    "#read data\n",
    "#def parser(s):\n",
    "#    return datetime.strptime(s, '%Y-%m-%d')\n",
    "#df = wr.s3.read_csv([path1_s3], parse_dates=[0], index_col=0, squeeze=True, date_parser=parser)\n",
    "## iterating the columns\n",
    "#for col in df.columns:\n",
    "#    print(col)\n",
    "\n",
    "\n",
    "#lpa=LPA_data[[\"receiptdate\",\"cases_glueexporteddate\",\"uid\",\"type\",\"casesubtype\",\"status\",\"donor_dob\",\"donor_postcode\",\"donor_gender\"]]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5268ef2b-4e72-4080-a918-e7bfddc40822",
   "metadata": {},
   "source": [
    "# Automating the input dates to forecast LPAs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93434d88-6814-4c5b-8d96-e33a7eaa70d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the date you want to extract data based on the latest date extrated LPA data\n",
    "\n",
    "## Grab part of filename\n",
    "fist_CSV_fileName = filtered_file_names[1]\n",
    "snapshot_end = fist_CSV_fileName.split('opg-analytical_cases_P')[1].lstrip().split('_S')[0]\n",
    "#snapshot_end\n",
    "\n",
    "#snapshot_end = final_df.values[7].astype(str)[7]\n",
    "\n",
    "## Automating the input dates to forecast\n",
    "p = getpass.getpass(prompt='Do you want to change the starting date for forecasting? (Choose Yes=Y OR No=N)')\n",
    " \n",
    "if (p.lower() == 'n') | (p.lower() == ''): #defult start date\n",
    "    snapshot_start = '2006-12-31'\n",
    "    print('You have not choosen to change the date to: ' + snapshot_start)\n",
    "    ## The first date to be considered:\n",
    "else:    \n",
    "    ## Select Date\n",
    "    print('You have choosen to change the date.')\n",
    "    snapshot_start = input('Enter the period_start date (for training): e.g., \"2006-12-31\"')\n",
    "    print('snapshot_start: ' + snapshot_start)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "197c09c2-54bc-42df-b55b-ea188f2babb5",
   "metadata": {},
   "source": [
    "# Data pre-processing and cleaning - data engineering"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2fc0e1c-b824-4822-a8e5-20041f465a65",
   "metadata": {},
   "source": [
    "## Meta data and Variable selection and Data Cleaning for the LPA data in Data Warehouse:\n",
    "\n",
    "Goal: to work out how many people applied for lpa and recieved the power of atthorney and how many applications in a year/month/week by age group since 2007? \n",
    "\n",
    "### ages over 19 years old\n",
    "\n",
    "#### Unique case reference for each donor = [donor_dob + donor_postcode + donor_gender]\n",
    "\n",
    "##### Sort by the unique id and count how many application\n",
    "\n",
    "###### and then dermine Whether the application type [casesubtype] is hw=health and welfare or pfa=property and finance\n",
    "\n",
    "###### how many certificate provider (cp) for each lpa application?\n",
    "\n",
    "###### Location based data and geographical data for the donor can be used to identify the financial situation and wherether they are located in England or Wales\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50385b9d-958f-4a48-958b-bee05777d4cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Filter the records:\n",
    "df_filtered = combined_df\n",
    "\n",
    "## Convert the receipt date to date format \n",
    "df_filtered['receiptdate'] = pd.to_datetime(df_filtered['receiptdate'], errors = 'coerce').dt.date\n",
    "\n",
    "## Filter records between the selected dates\n",
    "df_filtered = df_filtered.loc[(df_filtered['receiptdate'] > pd.to_datetime(snapshot_start))]\n",
    "df_filtered = df_filtered.loc[(df_filtered['receiptdate'] < pd.to_datetime(snapshot_end))]\n",
    "\n",
    "## Filter the dataframe to select only lpa type records\n",
    "df_filtered = df_filtered.loc[(df_filtered['type'] == 'lpa')]\n",
    "\n",
    "# Create a dataframe of the selected columns\n",
    "## Select the appropriate variable to be forecasted\n",
    "df = df_filtered[[\"receiptdate\",\"uid\",\"casesubtype\",\"status\",\"donor_dob\",\"donor_postcode\",\"donor_gender\"]]\n",
    "\n",
    "## Remove Null values and records\n",
    "lpa_df = df.dropna()\n",
    "\n",
    "# Extract age by subtracting 'receiptdate' and 'donor_dob'\n",
    "lpa_df['age'] = pd.to_datetime(lpa_df['receiptdate'], errors = 'coerce').dt.year - pd.to_datetime(lpa_df['donor_dob'], errors = 'coerce').dt.year\n",
    "#lpa_df['age'] = relativedelta(date, dob).years\n",
    "\n",
    "# Convert the donor_dob column to a datatime format\n",
    "lpa_df['donor_dob'] = pd.to_datetime(lpa_df['donor_dob'], errors = 'coerce').dt.date\n",
    "\n",
    "# Convert the ‘receiptdate’ column to datetime format for proper plotting.\n",
    "# Convert 'receiptdate' to datetime format \n",
    "lpa_df['receiptdate'] = pd.to_datetime(lpa_df['receiptdate'], errors='coerce')\n",
    "\n",
    "# Extract year from 'receiptdate'\n",
    "lpa_df['year'] = lpa_df['receiptdate'].dt.year\n",
    "\n",
    "## Set index\n",
    "#df['receiptdate'] = pd.to_datetime(df['receiptdate'])\n",
    "\n",
    "#df = df.set_index('receiptdate').asfreq('D')\n",
    "\n",
    "####df['receiptdate'] = df.set_index('receiptdate',inplace=True)\n",
    "\n",
    "#df.index = df.index.to_period('D')\n",
    "                            \n",
    "print(lpa_df.head())\n",
    "print(lpa_df.tail())\n",
    "\n",
    "\n",
    "#lpa_df['age'] = pd.to_datetime(df['receiptdate'], errors = 'coerce').dt.date - pd.to_datetime(df['donor_dob'], errors = 'coerce').dt.date\n",
    "#lpa_df['receiptdate'] = pd.to_datetime(lpa_df['receiptdate']).dt.date#.apply(lambda x: x.strftime('%Y-%m-%d'))\n",
    "#print(lpa_df)#['receiptdate']\n",
    "#lpa_df\n",
    "\n",
    "#print(lpa_df['age'])\n",
    "\n",
    "## infer the frequency of the data:\n",
    "###lpa_df = df\n",
    "\n",
    "#lpa_df = df.asfreq(pd.infer_freq(df.index))\n",
    "\n",
    "#lpa_df = lpa_df[start_date:end_date]\n",
    "\n",
    "#start_date_years = datetime.strptime(start_date, \n",
    "#                                     '%Y-%m-%d') + relativedelta(years = 0)\n",
    "#print(start_date_years)\n",
    "\n",
    "#start_date_formatted = start_date_years.date()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "015e48bf-b1c7-4d9c-8521-e4c8619c8327",
   "metadata": {},
   "source": [
    "# Visualisation of the time series\n",
    "## Virtualisation of the LPA Data:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "232c0736-d6df-4b90-9ad4-5f33b5f5bd1e",
   "metadata": {},
   "source": [
    "# Plot 'age' against 'receiptdate'\n",
    "\n",
    "## --------------------------------------------------------------------------------  ##\n",
    "# Create a scatter plot with ‘receiptdate’ as the x-axis and ‘age’ as the y-axis.\n",
    "# Display the plot with appropriate labels and a grid.\n",
    "\n",
    "plt.figure(figsize=(20, 10))\n",
    "plt.scatter(lpa_df['receiptdate'], lpa_df['age'], alpha=0.5)\n",
    "plt.title('Age vs Receipt Date')\n",
    "plt.xlabel('Receipt Date')\n",
    "plt.ylabel('Age')\n",
    "plt.grid(True)\n",
    "plt.show()\n",
    "\n",
    "## --------------------------------------------------------------------------------  ##\n",
    "# Create a histogram of the 'age' column\n",
    "\n",
    "# This code will produce a histogram that displays the frequency distribution of ages in your dataset. \n",
    "# The bins parameter determines the number of bins used in the histogram, and you can adjust this number\n",
    "# to change the granularity of your histogram.\n",
    "\n",
    "plt.figure(figsize=(20, 10))\n",
    "plt.hist(lpa_df['age'], bins=20, alpha=0.7, color='blue')\n",
    "plt.title('Age Distribution')\n",
    "plt.xlabel('Age')\n",
    "plt.ylabel('Frequency')\n",
    "plt.grid(True)\n",
    "plt.show()\n",
    "\n",
    "## --------------------------------------------------------------------------------  ##\n",
    "# Create a line chart of age against receipt date\n",
    "# Sort the DataFrame by 'receiptdate' to ensure the line chart is ordered\n",
    "lpa_df.sort_values('receiptdate', inplace=True)\n",
    "\n",
    "# Plot 'age' against 'receiptdate' using a line chart\n",
    "plt.figure(figsize=(20, 10))\n",
    "plt.plot(lpa_df['receiptdate'], lpa_df['age'], marker='o', linestyle='-', color='blue')\n",
    "plt.title('Age vs Receipt Date')\n",
    "plt.xlabel('Receipt Date')\n",
    "plt.ylabel('Age')\n",
    "plt.grid(True)\n",
    "\n",
    "## --------------------------------------------------------------------------------  ##\n",
    "# Produce a line chart that displays the average age of individuals for each year based on the receipt dates in your dataset.\n",
    "# The data points are connected with a line, which helps in identifying any trends or patterns over the years.\n",
    "\n",
    "# Group the data by year and calculate the average age for each year\n",
    "age_by_year = lpa_df.groupby('year')['age'].mean().reset_index()\n",
    "\n",
    "# Plot 'age' against 'year' using a line chart\n",
    "plt.figure(figsize=(20, 10))\n",
    "plt.plot(age_by_year['year'], age_by_year['age'], marker='o', linestyle='-', color='blue')\n",
    "plt.title('Average Age vs Year')\n",
    "plt.xlabel('Year')\n",
    "plt.ylabel('Average Age')\n",
    "plt.grid(True)\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0817f1a4-4782-4762-bd6e-715e96728f49",
   "metadata": {},
   "source": [
    "## Observations:\n",
    "The trend in the line chart indicates the changes in the average age of individuals over the years, \n",
    "based on the receipt dates from your dataset.\n",
    "Such a visualization can help identify patterns, \n",
    "such as whether the average age is increasing, decreasing, or remaining relatively stable over time.\n",
    "\n",
    "For example:\n",
    "An upward trend would suggest that the average age is increasing each year.\n",
    "A downward trend would indicate that the average age is decreasing.\n",
    "A flat line would imply that there is little to no change in the average age over the years.\n",
    "These trends can be influenced by various factors, such as the demographics of the population being studied, \n",
    "changes in policies, or other external factors that might affect the age distribution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9819b606-ffd9-4e8c-bcc6-ac54e81cf4cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Plot the Actuals\n",
    "\n",
    "# lpa_series = lpa_df['age']\n",
    "# #lpa_series = df.squeeze()\n",
    "# plt.figure(figsize=(28, 14))\n",
    "# plt.plot(lpa_series)\n",
    "# plt.title('UK Actual LPA Data', fontsize=20)\n",
    "# plt.ylabel('Age', fontsize=16)\n",
    "# plt.axvline(pd.to_datetime(df['receiptdate'], errors = 'coerce').dt.year, color = 'k', linestyle='--', alpha = 0.2)\n",
    "# # for year in range(min(pd.to_datetime(df['receiptdate'], errors = 'coerce').dt.year), \n",
    "# #     datetime.strptime(snapshot_end, '%Y-%m-%d').year):\n",
    "# #     #datetime.strptime(\"2024-03-18\", '%Y-%m-%d').year):\n",
    "# #     plt.axvline(pd.to_datetime(df['receiptdate'], errors = 'coerce'), color = 'k', linestyle='--', alpha = 0.2)\n",
    "# #     #plt.axvline(pd.to_datetime(str(year) + '-01-01'), color = 'k', \n",
    "# #     #print(year)\n",
    "# plt.legend()    \n",
    "# #plt.savefig('UK_Actual_LPA_Data.png', dpi=300, bbox_inches='tight')\n",
    "# plt.show()  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d9843c8-2905-43ae-9974-1c184aeba587",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Perform the GROUP BY operation and calculate the count\n",
    "#Cases_by_year_age = lpa_df.groupby(\n",
    "#    ['receiptdate', 'uid', 'type', 'casesubtype', 'status', 'donor_postcode', 'donor_gender', 'age']) \\\n",
    "#    .agg({'No_of_Cases': 'count'}) \\ #['donor_postcode', 'donor_gender', 'age']\n",
    "#    .reset_index()\n",
    "\n",
    "#agg_funcs = dict(No_of_Cases = 'count')\n",
    "#Cases_by_year_age = lpa_df.set_index(['receiptdate', 'uid', 'type', 'casesubtype', 'status', 'donor_postcode', 'donor_gender', 'age']) \\\n",
    "#    .stack() \\\n",
    "#    .groupby(level=0) \\\n",
    "#    .agg(agg_funcs)\n",
    "\n",
    "\n",
    "#Cases_by_year_age\n",
    "#lpa_by_year_age = lpa_df[['receiptdate', 'uid', 'type', 'casesubtype', 'status', 'donor_postcode', 'donor_gender', 'age']] \\\n",
    "#                    .groupby(['donor_postcode', 'donor_gender', 'age'])  \\\n",
    "#                    .agg('count')#.sum()\n",
    "#lpa_by_year_age.to_csv(r'lpa_by_year_age.csv')\n",
    "\n",
    "\n",
    "\n",
    "#lpa_df.to_csv(r'lpa_df.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49ed0a3d-b058-41e4-8cbc-61cd51c67b72",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Missing Data Imputation:\n",
    "\n",
    "There are be some people in the LPA data with missing age (they are represented with negetive numbers in column age). \n",
    "So for missing data (age) imputation, his code is written to use age distribution of cases that they have age and\n",
    "apply this to the total number of doners in that year. \n",
    "Actually, we allocate proportionaly distributed age across each year of these missing ages. \n",
    "E.g., if we get 90% of age distribution for a particular year,\n",
    "we used this age distribution to be applied to the 100% of donors to get the total distribution. \n",
    "\n",
    "The code below: \n",
    "first, loads the data from the CSV file and replaces negative ages \n",
    "with NaN to represent missing data. \n",
    "\n",
    "It then calculates the age distribution for each year. \n",
    "\n",
    "For each year, it finds the indices of the missing ages and imputes \n",
    "them by randomly choosing from the age distribution of that year. \n",
    "\n",
    "The imputed ages are proportional to the age distribution \n",
    "of the donors that year. \n",
    "\n",
    "Finally, it saves the DataFrame with the imputed ages to a new CSV file.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94e0a4fd-f395-40b0-b785-c361f1b11029",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Function to calculate the number of unique records by age, year, gender, and postcode\n",
    "# #def calculate_unique_records_by_age_year_gender_postcode(records):\n",
    "# # Get the current year\n",
    "# #current_year = datetime.now().year  \n",
    "# #Create a dictionary to store counts for each age, year, gender, and postcode combination\n",
    "# age_year_gender_postcode_counts = {}\n",
    "\n",
    "# records = lpa_df\n",
    "    \n",
    "# # Iterate over each record\n",
    "# for record in records:         \n",
    "#     # Extract gender and postcode\n",
    "#     gender = record[\"donor_gender\"]\n",
    "#     postcode = record[\"donor_postcode\"]\n",
    "#     dob = record[\"donor_dob\"]\n",
    "    \n",
    "#     # Create a unique key combining age, gender, and postcode\n",
    "#     key = (dob, gender, postcode)\n",
    "        \n",
    "#     # Increment the count for the key\n",
    "#     age_year_gender_postcode_counts[key] = age_year_gender_postcode_counts.get(key, 0) + 1\n",
    "        \n",
    "# return age_year_gender_postcode_counts\n",
    "\n",
    "# # Call the function and print the results\n",
    "# unique_records_by_age_year_gender_postcode = calculate_unique_records_by_age_year_gender_postcode(records)\n",
    "\n",
    "# print(\"Number of unique records by age, year, gender, and postcode:\")\n",
    "\n",
    "# for key, count in unique_records_by_age_year_gender_postcode.items():\n",
    "#     dob, gender, postcode = key\n",
    "#     print(f\"Date of Birth (D.o.B): {dob}, Gender: {gender}, Postcode: {postcode}, Count: {count}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1ce1de0-088c-4447-a432-f0dce3facfee",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This code will output the number of unique records for each age in each year for each donor gender in each donor postcode.\n",
    "# It calculates the age based on the current year and the birth year of each person in the records.\n",
    "# Then, it creates a unique key combining age, year, gender, and postcode, and increments the count for each key.\n",
    "# Finally, it prints the results showing the count of unique records for each combination.\n",
    "\n",
    "#from datetime import datetime\n",
    "\n",
    "# Sample data representing records with donor gender, donor postcode, and date of birth\n",
    "#records = [\n",
    "#    {\"donor_gender\": \"Male\", \"donor_postcode\": \"AB12 3CD\", \"date_of_birth\": \"1999-05-15\"},\n",
    "#    {\"donor_gender\": \"Female\", \"donor_postcode\": \"XY34 5YZ\", \"date_of_birth\": \"1994-08-20\"},\n",
    "#    {\"donor_gender\": \"Male\", \"donor_postcode\": \"CD56 7EF\", \"date_of_birth\": \"1996-02-10\"},\n",
    "#    {\"donor_gender\": \"Male\", \"donor_postcode\": \"FG78 9HI\", \"date_of_birth\": \"2000-11-30\"},\n",
    "#    {\"donor_gender\": \"Female\", \"donor_postcode\": \"JK90 1LM\", \"date_of_birth\": \"1987-03-25\"},\n",
    "#    {\"donor_gender\": \"Male\", \"donor_postcode\": \"OP23 4QR\", \"date_of_birth\": \"1993-09-05\"}\n",
    "#]\n",
    "\n",
    "# Function to calculate the number of unique records by age, year, gender, and postcode\n",
    "#def calculate_unique_records_by_age_year_gender_postcode(records):\n",
    "    # Get the current year\n",
    "#    current_year = datetime.now().year\n",
    "    \n",
    "    # Create a dictionary to store counts for each age, year, gender, and postcode combination\n",
    "#    age_year_gender_postcode_counts = {}\n",
    "    \n",
    "    # Iterate over each record\n",
    "#    for record in records:\n",
    "        # Extract the year of birth from the date_of_birth\n",
    "#        birth_year = int(record[\"date_of_birth\"].split(\"-\")[0])\n",
    "        \n",
    "        # Calculate the age of the person\n",
    "#        age = current_year - birth_year\n",
    "        \n",
    "        # Extract the year from the date_of_birth\n",
    "#        year = birth_year\n",
    "        \n",
    "        # Extract gender and postcode\n",
    "#        gender = record[\"donor_gender\"]\n",
    "#        postcode = record[\"donor_postcode\"]\n",
    "        \n",
    "        # Create a unique key combining age, year, gender, and postcode\n",
    "#        key = (age, year, gender, postcode)\n",
    "        \n",
    "        # Increment the count for the key\n",
    "#        age_year_gender_postcode_counts[key] = age_year_gender_postcode_counts.get(key, 0) + 1\n",
    "        \n",
    "#    return age_year_gender_postcode_counts\n",
    "\n",
    "# Call the function and print the results\n",
    "#unique_records_by_age_year_gender_postcode = calculate_unique_records_by_age_year_gender_postcode(records)\n",
    "#print(\"Number of unique records by age, year, gender, and postcode:\")\n",
    "#for key, count in unique_records_by_age_year_gender_postcode.items():\n",
    "#    age, year, gender, postcode = key\n",
    "#    print(f\"Age: {age}, Year: {year}, Gender: {gender}, Postcode: {postcode}, Count: {count}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07154148-04e8-4096-918b-625a4fed04b8",
   "metadata": {},
   "source": [
    "# Missing age imutation\n",
    "\n",
    "There are two issues with the age:\n",
    "\n",
    "1. The donor_gender might be missing or entered incorrectly\n",
    "\n",
    "2. The derieved age might be higher than 126 years old\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68ed109d-7ce6-4e76-a6f2-0712c10f1cc2",
   "metadata": {},
   "outputs": [],
   "source": [
    "lpa_data_sample_imputed = lpa_df\n",
    "\n",
    "# Filter rows with negative or greater than 126 age values\n",
    "criteria = lpa_data_sample_imputed[(lpa_data_sample_imputed['age'] < 0) | (lpa_data_sample_imputed['age'] > 126)]\n",
    "\n",
    "# Display the filtered rows\n",
    "print(criteria)\n",
    "\n",
    "# Replace age values with NULL (NaN) in the filtered rows\n",
    "lpa_data_sample_imputed.loc[criteria.index, 'age'] = np.nan #None\n",
    "\n",
    "# Display the updated DataFrame\n",
    "print(lpa_data_sample_imputed)\n",
    "\n",
    "# Group by year and count age groups\n",
    "age_distribution = lpa_data_sample_imputed.groupby('year')['age'].value_counts()\n",
    "\n",
    "# Fill missing ages with the most common age for each year\n",
    "most_common_age = lpa_data_sample_imputed.groupby('year')['age'].apply(lambda x: x.mode().iloc[0])\n",
    "lpa_data_sample_imputed['age'] = lpa_data_sample_imputed.apply(lambda row: most_common_age[row['year']] if pd.isna(row['age']) else row['age'], axis=1)\n",
    "\n",
    "# Display the age distribution after filling missing ages\n",
    "print(\"\\nAge distribution by year (including filled missing ages):\")\n",
    "print(age_distribution)\n",
    "\n",
    "# Display the final DataFrame\n",
    "print(\"\\nFinal DataFrame:\")\n",
    "print(lpa_data_sample_imputed)\n",
    "\n",
    "# Save the dataframe with imputed ages\n",
    "lpa_data_sample_imputed.to_csv('lpa_data_sample_imputed.csv', index=False)\n",
    "\n",
    "# Print a success message\n",
    "print(\"The missing age data has been successfully imputed and saved to lpa_data_sample_imputed.csv file.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5dedfc27-b664-4e22-b06a-ef0bf64c9765",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# lpa_data_sample_imputed = lpa_df\n",
    "\n",
    "\n",
    "# # Identify the rows with missing age (represented as negative numbers)\n",
    "# ## 1. The donor_gender might be missing or entered incorrectly:  < 0\n",
    "# ## 2. The derieved age might be higher than 126 years old > 126\n",
    "# lpa_data_sample_imputed['missing_age'] = (lpa_data_sample_imputed['age'] < 0) | (lpa_data_sample_imputed['age'] > 126)\n",
    "\n",
    "# # Replace negative ages with NaN\n",
    "# lpa_data_sample_imputed.loc[missing_age, 'age'] = np.nan\n",
    "\n",
    "# # Calculate the age distribution for each year excluding missing ages\n",
    "# age_distribution = lpa_data_sample_imputed.loc[~missing_age].groupby('year')['age'].value_counts(normalize=True)\n",
    "\n",
    "# # Calculate the age distribution for each year\n",
    "# age_distribution_per_year = lpa_data_sample_imputed.groupby('year')['age'].value_counts(normalize=True)\n",
    "\n",
    "# # Apply the age distribution to the total number of donors in each year\n",
    "# for year in df['year'].unique():\n",
    "#     # Calculate the number of missing ages in the current year\n",
    "#     num_missing = missing_age & (df['year'] == year)\n",
    "    \n",
    "#     # If there are missing ages in the current year\n",
    "#     if num_missing.sum() > 0:\n",
    "#         # Generate ages according to the age distribution of the current year\n",
    "#         imputed_ages = np.random.choice(age_distribution[year].index, \n",
    "#                                         p=age_distribution[year].values, \n",
    "#                                         size=num_missing.sum())\n",
    "        \n",
    "#         # Assign the generated ages to the missing ages\n",
    "#         df.loc[num_missing, 'age'] = imputed_ages\n",
    "\n",
    "\n",
    "# # Apply the age distribution to the missing ages\n",
    "# for year in lpa_data_sample_imputed['year'].unique():\n",
    "#     missing_age_indices = lpa_data_sample_imputed[(lpa_data_sample_imputed['year'] == year) & (lpa_data_sample_imputed['age'].isna())].index\n",
    "#     if not missing_age_indices.empty:\n",
    "#         imputed_ages = np.random.choice(age_distribution_per_year[year].index, \n",
    "#                                         p=age_distribution_per_year[year].values, \n",
    "#                                         size=len(missing_age_indices))\n",
    "#         lpa_data_sample_imputed.loc[missing_age_indices, 'age'] = imputed_ages\n",
    "\n",
    "      \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0916770-95b6-4bff-8b14-b1d103bc585a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a unique identifier based on multiple columns:\n",
    "# lpa_unique_key = lpa_df\n",
    "\n",
    "\n",
    "# #df1.set_index(['donor_postcode', 'donor_gender', 'age']).index.factorize()[0]+1\n",
    "# lpa_unique_key.insert(loc = 0, column='ukey', value = lpa_unique_key.set_index(['donor_postcode', 'donor_gender', 'age']).index.factorize()[0]+1)\n",
    "# #lpa_unique_key\n",
    "\n",
    "# #(lpa_unique_key.fillna({'donor_postcode':'', 'donor_gender':'', 'age':''})\n",
    "# #   .groupby(['donor_postcode', 'donor_gender', 'age'],sort=False).ngroup()+1)\n",
    "\n",
    "# #lpa_unique_key.loc[lpa_unique_key['type']=='lpa','ukey'].agg(['nunique','count','size'])\n",
    "# #lpa_unique_key.query('type == \"lpa\"')['ukey'].agg(['nunique','count','size'])\n",
    "# #lpa_unique_key.query('casesubtype == \"hw\"')['ukey'].agg(['nunique','count','size'])\n",
    "# #lpa_unique_key.query('casesubtype == \"pfa\"')['ukey'].agg(['nunique','count','size'])\n",
    "# #lpa_unique_key.groupby(['ukey']).count()\n",
    "# #lpa_unique_key['count_ukey'] = lpa_unique_key['ukey'].value_counts()\n",
    "# #lpa_unique_key\n",
    "\n",
    "\n",
    "\n",
    "# lpa_unique_key['CountbyUkey'] = lpa_unique_key.groupby(['donor_postcode', 'donor_gender']).age.transform('count')\n",
    "# lpa_unique_key['CountbyAge'] = lpa_unique_key.groupby('year').age.transform('count').sum()\n",
    "\n",
    "# # Perform the GROUP BY operation and calculate the sum\n",
    "# lpa_age = lpa_unique_key.groupby(['donor_postcode', 'donor_gender', 'age']) \\\n",
    "#     .agg({'CountbyAge': 'sum'}) \\\n",
    "#     .reset_index()\n",
    "\n",
    "# print(lpa_age)\n",
    "# #lpa_unique_key['month'] = lpa_unique_key['ArrivalDate'].dt.month\n",
    "\n",
    "\n",
    "# # Cases_by_year_age\n",
    "\n",
    "# #lpa_by_year_age = lpa_unique_key[['receiptdate', 'uid', 'type', 'casesubtype', 'status', 'donor_postcode', 'donor_gender', 'age']] \\\n",
    "# #                    .groupby(['donor_postcode', 'donor_gender', 'age'])  \\\n",
    "# #                    .agg('count')#.sum()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90dd8f3a-f1d4-4753-b55a-15cb1620ffcf",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Number of LPA reciepts\n",
    "lpa_reciepts = lpa_df\n",
    "# Extract month letter and year \n",
    "###lpa_reciepts['month_year'] = lpa_reciepts['receiptdate'].dt.strftime('%b-%y')\n",
    "# Calculate the number of unique records by month and year\n",
    "#count_reciepts_month = lpa_reciepts.groupby(['year', 'month_year'])['receiptdate'].nunique().reset_index(name='count')\n",
    "###Count_daily_reciepts = lpa_reciepts.groupby(['receiptdate']).count()\n",
    "\n",
    "#unique_records = df.groupby('unique_key').agg('count').reset_index()  #.groupby(['year'])['unique_key'].nunique().reset_index(name='count')\n",
    "###Count_daily_reciepts = Count_daily_reciepts.rename(columns={\"count\": \"Count_of_daily_reciepts\"})\n",
    "\n",
    "# Display the result\n",
    "###print(Count_daily_reciepts)\n",
    "\n",
    "# Save the result into a csv file\n",
    "lpa_reciepts.to_csv(r'lpa_reciepts.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "607f2311-c45a-4016-ac0e-f2ca2a88c588",
   "metadata": {},
   "source": [
    "# Generate a Unique key by combining age, donor_gender, and donor_postcode\n",
    "\n",
    "For ages over 19 years old:\n",
    "Unique case reference for each donor = [donor_dob + donor_postcode + donor_gender]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d42c4dde-0614-45ab-91a1-254117ca8f41",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# DataFrame with the count of unique records for each combination of age and year. \n",
    "# Generate a unique key by combining age, donor_gender, and donor_postcode, \n",
    "# and then calculate the number of unique records by age and year.\n",
    "\n",
    "lpa_unique = lpa_data_sample_imputed\n",
    "\n",
    "# Remove spaces from the donor postcodes\n",
    "lpa_unique['donor_postcode'] = lpa_unique['donor_postcode'].str.strip()\n",
    "lpa_unique['donor_postcode'] = lpa_unique['donor_postcode'].str.replace(' ', '')\n",
    "\n",
    "# Generate a unique key by combining age, donor_gender, and donor_postcode\n",
    "lpa_unique['unique_key'] = lpa_unique['donor_dob'].astype(str) \\\n",
    "+ lpa_unique['donor_gender'] + lpa_unique['donor_postcode']\n",
    "\n",
    "# lpa_by_year_age = lpa_unique_key\n",
    "\n",
    "# lpa_by_year_age.to_csv(r'lpa_by_year_age.csv')\n",
    "\n",
    "# remove duplicate rows based on Id values(unique_key) and \n",
    "# keep only the row that don't have 0 value in all the fields.\n",
    "\n",
    "\n",
    "duplicateMask = lpa_unique.duplicated('unique_key', keep=False)\n",
    "\n",
    "lpa_unique = pd.concat([lpa_unique.loc[duplicateMask & lpa_unique[['age', 'donor_gender', 'donor_postcode']].ne(0).any(axis=1)], \\\n",
    "               lpa_unique[~duplicateMask]])\n",
    "\n",
    "#lpa_df['zero']=lpa_df.select_dtypes(['int','float']).eq(0).sum(axis=1)\n",
    "#df=df.sort_values(['zero','Id']).drop_duplicates(subset=['Id']).drop(columns='zero')df['zero']=df.select_dtypes(['int','float']).eq(0).sum(axis=1)\n",
    "#df=df.sort_values(['zero','Id']).drop_duplicates(subset=['Id']).drop(columns='zero')\n",
    "\n",
    "#lpa_unique = lpa_unique.drop_duplicates(subset=\"unique_key\")\n",
    "lpa_unique"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c2235a2-7a56-4d54-b10d-e0b17c6aa8ff",
   "metadata": {},
   "source": [
    "# Save the LPA data with new unique keys (as a unique ID)\n",
    "\n",
    "Sort by the unique id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51f4fb88-82cb-4844-bcae-72657302a02e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sort the rows of dataframe by  'unique_key'  \n",
    "## column inplace\n",
    "\n",
    "#lpa_df_index = lpa_unique.sort_values(lpa_unique.columns[9])\n",
    "\n",
    "## Sort by 'unique_key' column in ascending order\n",
    "lpa_df_index = lpa_unique.sort_values(by='unique_key')\n",
    "\n",
    "\n",
    "#lpa_df_index = lpa_unique.sort_values['unique_key']\n",
    "#lpa_df_index = lpa_unique.sort_values(by = 'unique_key', axis = 1, inplace = True, ascending = True)\n",
    "#lpa_df_index = lpa_unique.reindex(sorted(lpa_unique.columns), axis=1)\n",
    "\n",
    "## Set index\n",
    "#df['receiptdate'] = pd.to_datetime(df['receiptdate'])\n",
    "\n",
    "#df = df.set_index('receiptdate').asfreq('D')\n",
    "#lpa_df_index['unique_key'] = \n",
    "\n",
    "## Set the unique key as an ID (index)\n",
    "lpa_df_index.set_index('unique_key', inplace = True)\n",
    "\n",
    "#df.index = df.index.to_period('D')\n",
    "                            \n",
    "###print(df.head())\n",
    "###print(df.tail())\n",
    "\n",
    "#Missing_data = lpa_df_index[(lpa_data_sample_imputed['age'] < 0 | lpa_data_sample_imputed['age'] > 126)]\n",
    "#print(Missing_data)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7aa7b267-6788-441b-8145-b15e38f934d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract and save data into a csv file\n",
    "lpa_data = lpa_df_index\n",
    "lpa_data.to_csv(r'lpa_data.csv')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4eed52ec-7239-4bd9-a37f-c7c5a59956e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract month letter and year \n",
    "lpa_data['month_year'] = lpa_data['receiptdate'].dt.strftime('%b-%y')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4441107-af2e-41fc-b26e-d97bebbefda1",
   "metadata": {},
   "source": [
    "# Work out how many people applied for lpa and recieved the power of atthorney and how many applications in a year/month/week by age group since 2007?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d91646b3-755c-405d-9cf0-ffbe25a26fee",
   "metadata": {},
   "outputs": [],
   "source": [
    "#count_unique_records = lpa_data.groupby('unique_key').agg('count').reset_index()\n",
    "#count_unique_records"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1ce0cc6-03b8-46d6-b28c-ce8a13503efc",
   "metadata": {},
   "outputs": [],
   "source": [
    "d = lpa_unique.sort_values(by='unique_key')\n",
    "count_unique_grouped_year = d.groupby(['year'])['unique_key'].nunique().reset_index(name='count')\n",
    "count_unique_grouped_year"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b12b129-dea6-44e6-9dc3-8052e77a37f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "d = lpa_unique.sort_values(by='unique_key')\n",
    "count_unique_grouped_age = d.groupby(['age'])['unique_key'].nunique().reset_index(name='count')\n",
    "count_unique_grouped_age"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33b3f949-855d-4ed5-aa5b-c5aa170cd6f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "g = lpa_unique\n",
    "# Group by 'item' and 'color', then count the occurrences\n",
    "count_unique_grouped_all = g.groupby(['receiptdate', 'uid', 'casesubtype', 'status', 'donor_dob', 'donor_postcode', 'donor_gender', 'age', 'year'])['unique_key'].count().reset_index(name='count')\n",
    "count_unique_grouped_all"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c18be8a5-3357-4f91-9d26-af397dabcc02",
   "metadata": {},
   "outputs": [],
   "source": [
    "g = lpa_unique\n",
    "# Group by 'item' and 'color', then count the occurrences\n",
    "count_unique_grouped_age_year = g.groupby(['age', 'year'])['unique_key'].count().reset_index(name='count')\n",
    "count_unique_grouped_age_year = count_unique_grouped_age_year.rename(columns={\"count\": \"Count_of_CASEID\"})\n",
    "count_unique_grouped_age_year\n",
    "\n",
    "# Save the result into a csv file\n",
    "count_unique_grouped_age_year.to_csv(r'count_unique_grouped_age_year.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4771cf2-a4e5-45af-9cae-f90e72d616df",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract the year of reciept from the receiptdate\n",
    "#receipt_year = int(record[\"receiptdate\"].split(\"-\")[0])\n",
    "\n",
    "# Calculate the number of unique records by age and year\n",
    "#####count_unique_records = lpa_data.reset_index(name='count')\n",
    "\n",
    "\n",
    "#unique_records = df.groupby('unique_key').agg('count').reset_index()  #.groupby(['year'])['unique_key'].nunique().reset_index(name='count')\n",
    "####count_unique_records = count_unique_records.rename(columns={\"count\": \"Count_of_CASEID\"})\n",
    "\n",
    "# Display the result\n",
    "####print(count_unique_records)\n",
    "\n",
    "# Save the result into a csv file\n",
    "####count_unique_records.to_csv(r'count_unique_records.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a6408eb-1305-4ed9-8701-40a6249f2883",
   "metadata": {},
   "source": [
    "# Dermine Whether the application type [casesubtype] is hw=health and welfare or pfa=property and finance"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "066db357-3848-4fdc-86fe-12294383e61a",
   "metadata": {},
   "source": [
    "# How many certificate provider (cp) for each lpa application?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06ebb3ab-4ae6-405f-95de-6c7b259c344a",
   "metadata": {},
   "source": [
    "# Location based data and geographical data for the donor can be used to identify the financial situation and wherether they are located in England or Wales"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6889b3cc-d296-4eed-8270-17ca0c3f9d4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract the year from the receiptdate\n",
    "#receipt_year = birth_year        \n",
    "\n",
    "# Calculate the age of the person\n",
    "#age = receiptdate - birth_year\n",
    "#lpa_df['a'] = \n",
    "############(pd.to_datetime(lpa_df['receiptdate'], errors = 'coerce').dt.day - pd.to_datetime(lpa_df['donor_dob'], errors = 'coerce').dt.day) # / 365.25\n",
    "#lpa_df\n",
    "\n",
    "# Create an Excel writer\n",
    "writer = pd.ExcelWriter('LPA_Data_actuals_Years.xlsx', engine='xlsxwriter')\n",
    "\n",
    "# Iterate through unique years and save data to separate sheets\n",
    "for year in count_unique_grouped_age_year['year'].unique():\n",
    "    year_data = count_unique_grouped_age_year[count_unique_grouped_age_year['year'] == year]\n",
    "    year_data.to_excel(writer, sheet_name=str(year), index=False)\n",
    "\n",
    "\n",
    "# Save the Excel file\n",
    "writer.save()\n",
    "writer.close()  # Close the ExcelWriter\n",
    "\n",
    "year_data\n",
    "# # Iterate through unique years and save data to separate sheets\n",
    "# for year in lpa_df['year'].unique():\n",
    "#     year_data = lpa_df[lpa_df['year'] == year]\n",
    "#     chunk_size = 100000  # Adjust as needed\n",
    "#     num_chunks = len(year_data) // chunk_size + 1\n",
    "#     for i in range(num_chunks):\n",
    "#         start_idx = i * chunk_size\n",
    "#         end_idx = (i + 1) * chunk_size\n",
    "#         chunk_data = lpa_df.iloc[start_idx:end_idx]\n",
    "#         chunk_data.to_excel(writer, sheet_name=f'Sheet{i}', index=False)\n",
    "\n",
    "# # Save the Excel file\n",
    "# writer.save()\n",
    "# writer.close()  # Close the ExcelWriter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1881181d-7089-429b-8237-005d6314f8bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pivot table with count aggregation\n",
    "pivot_table = pd.pivot_table(lpa_unique,\n",
    "                              values='unique_key',\n",
    "                              index='age',\n",
    "                              columns='year',\n",
    "                              aggfunc='count')\n",
    "\n",
    "# Replace NaN values with zeros\n",
    "pivot_table_filled = pivot_table.fillna(0)\n",
    "\n",
    "print(pivot_table_filled)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce3a3741-b69e-48c3-b9b6-94fc816c8de1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the result into a csv file\n",
    "pivot_table_filled.to_csv(r'count_unique_records.csv')\n",
    "\n",
    "# Define the source path of the CSV file (assuming it's in the current directory)\n",
    "source_csv_path = \"count_unique_records.csv\"\n",
    "\n",
    "# Define the target directory where the CSV file should be placed\n",
    "target_directory = \"csv_files\"\n",
    "\n",
    "# Create the target directory if it doesn't exist\n",
    "if not os.path.exists(target_directory):\n",
    "    os.makedirs(target_directory)\n",
    "\n",
    "# Move the CSV file to the target directory\n",
    "shutil.move(source_csv_path, os.path.join(target_directory, \"count_unique_records.csv\"))\n",
    "\n",
    "# Print a success message\n",
    "print(f\"The CSV file {source_csv_path} was successfully moved to {target_directory}.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77c7b3db-0582-4c65-8ca5-f682932c572d",
   "metadata": {},
   "source": [
    "# Mortality Statistics\n",
    "## Source Data For Mortality Statistics and Modelled Age Specific Survival Rates (Model Input Set By Control Assumptions)\n",
    "\n",
    "# LPA Control Assumptions\n",
    "## Specific Key Assumptions that control expected demand , LPA market size and saturation.\n",
    "\n",
    "\n",
    "# Meta data and Variable selection and Data Cleaning for the Mortality statastics data based on population projections:\n",
    "\n",
    "## Goal: \n",
    "### What proportion of the UK population are likely to buy LPA and still alive?\n",
    "*How many people are still alive (Living Donors bought LPA)*\n",
    "*Based on ONS Data of Population of Engalnd and Wales, how many people are still alive and how many of them are dead?*\n",
    "*e.g., if there are 1000 people and 100 of them are still alive and bought LPA,\n",
    "so there are 900 of them still didn't buy LPA.\n",
    "\n",
    "\n",
    "\n",
    "**1. These rates are standardised to the 2013 European Standard Population, expressed per million population; \n",
    "they allow comparisons between populations with different age structures, including between males and females and over time. \n",
    "**2.  Deaths per 1,000 live births. \n",
    "**3.  Death figures are based on deaths registered rather than deaths occurring in a calendar year.\n",
    "\n",
    "### For information on registration delays for a range of causes, see: \n",
    "    https://webarchive.nationalarchives.gov.uk/ukgwa/20160106020016/http://www.ons.gov.uk/ons/guide-method/user-guidance/health-and-life-events/impact-of-registration-delays-on-mortality-statistics/index.html\n",
    "\n",
    "A limiting factor in modelling numbers of surving LPA holders aged 90+ has been the absence of single age specific mortality rates \n",
    "for this group. Estimates* suggested that previously applied mortality rates were too low increasing the apparent numbers of \n",
    "surviving LPA holder saged 90+ and therefore over-estimating the \"sauration of this market.\n",
    "\n",
    "For the 2018 LPA forecast , Age specific mortality rates for those aged 90+ have therefore been extrapolated based on \n",
    "a standard log power law that best fits existing mortality rates to age. \n",
    "\n",
    "*numbers of surviving LPA holders were estimated to exceed the total projected  population in each age group which was \n",
    "clearly not possible.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b287ad4-7294-4a75-a9e7-71d4f397b8da",
   "metadata": {},
   "source": [
    "# LPA SURVIVAL TABLES:\n",
    " LPA MODEL/LPA SURVIVAL TABLES\n",
    "percentage of people are died in one year\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8aa6b618-1161-4001-94c2-df536c5a7953",
   "metadata": {},
   "source": [
    "# if a 1000 40 years old male bought an LPA in 2008, what proportion of are still alove today?\n",
    "\n",
    "# The model taking each age categories (categorical variable) and assumed that they are \n",
    "# singe age-specifics in the age category 18 to 90 and provide figure what percentage of people for male died within one year?\n",
    "\n",
    "## e.g., in the 15-19 age category, 0.3 percent of males died within one year in the UK and 0.03 per 1000\n",
    "## e.g., in the 25-29 age category, 0.6 percent of males died within one year in the UK and 0.06 per 1000\n",
    "## e.g., in the 70-74 age category, 23.7 percent of males died within one year in the UK or 2.37 per 1000\n",
    "\n",
    "## if you started at age 18, 7 years and become 25 years old ahead, \n",
    "## as the ages goes up you will fall into a higher mortality category (from 0.3 to 0.6)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96ae6aa8-4340-45f6-8ba4-d231f3a29a5e",
   "metadata": {},
   "source": [
    "# calculate naïve extrapolation for demand forecasting and calculate low planning estimate, centeral planning estimate, and high planning estimate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4fd8723-2c72-44a2-8278-a3d1ddfe5907",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract the month of reciept from the receiptdate\n",
    "#lpa_data_no_index['month'] = lpa_data_no_index['receiptdate'].dt.month\n",
    "#lpa_data_no_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a60479e1-8b7e-4565-af0d-4f59f6b33678",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# Calculate the number of unique records by month and year\n",
    "count_unique_month = lpa_data.groupby(['year', 'month_year', 'age'])['uid'].nunique().reset_index(name='count')\n",
    "\n",
    "\n",
    "#unique_records = df.groupby('unique_key').agg('count').reset_index()  #.groupby(['year'])['unique_key'].nunique().reset_index(name='count')\n",
    "count_unique_month = count_unique_month.rename(columns={\"count\": \"Count_of_CASEID_month\"})\n",
    "\n",
    "# Display the result\n",
    "print(count_unique_month)\n",
    "\n",
    "# Save the result into a csv file\n",
    "#count_unique_month.to_csv(r'count_unique_month.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa18b9fd-73d7-46d1-9616-3cb0b6229292",
   "metadata": {},
   "outputs": [],
   "source": [
    "#df_count_unique_month = pd.DataFrame(count_unique_month)\n",
    "age_lower_limit = 50\n",
    "age_upper_limit = 70\n",
    "count_unique_month1 = count_unique_month.loc[(count_unique_month[\"age\"] >= age_lower_limit) &\n",
    "                 (count_unique_month[\"age\"] <= age_upper_limit)]\n",
    "\n",
    "#age_range = [50:70]\n",
    "#count_unique_month.loc[count_unique_month[\"month_year\"].isin(age_range)]\n",
    "\n",
    "count_unique_month2 = count_unique_month1[count_unique_month1['month_year'].isin(['Jan-24', 'Feb-24', 'Mar-24'])]\n",
    "#count_unique_month2 = count_unique_month1[count_unique_month1['month_year'].isin([50:70])]\n",
    "#df_count_unique_month['month_year'] = pd.to_datetime(df_count_unique_month['month_year'], format='%b-%y')\n",
    "#df_count_unique_month = df_count_unique_month.sort_values(df_count_unique_month.columns[1])\n",
    "#df_count_unique_month = df_count_unique_month.set_index(['month_year'])\n",
    "\n",
    "count_unique_month_age = count_unique_month2\n",
    "# Save the result into a csv file\n",
    "count_unique_month_age.to_csv(r'count_unique_month_age.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7546fbde-74e9-40f6-b03b-fea16e93e1af",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# Calculate the naïve forecast (previous month's sales)\n",
    "count_unique_month_age['Naive_Forecast'] = count_unique_month_age['Count_of_CASEID_month'].shift(1)\n",
    "\n",
    "# Define planning estimate factors\n",
    "low_factor = 0.9\n",
    "high_factor = 1.1\n",
    "\n",
    "# Calculate planning estimates\n",
    "count_unique_month_age['Low_Planning_Estimate'] = count_unique_month_age['Naive_Forecast'] * low_factor\n",
    "count_unique_month_age['Central_Planning_Estimate'] = count_unique_month_age['Naive_Forecast']\n",
    "count_unique_month_age['High_Planning_Estimate'] = count_unique_month_age['Naive_Forecast'] * high_factor\n",
    "\n",
    "# Calculate MAPE (Mean Absolute Percentage Error)\n",
    "count_unique_month_age['Absolute_Percentage_Error'] = abs(count_unique_month_age['Count_of_CASEID_month'] - count_unique_month_age['Naive_Forecast']) / count_unique_month_age['Count_of_CASEID_month']\n",
    "mape = count_unique_month_age['Absolute_Percentage_Error'].mean() * 100\n",
    "\n",
    "# Calculate MAD (Mean Absolute Deviation)\n",
    "count_unique_month_age['Absolute_Deviation'] = abs(count_unique_month_age['Count_of_CASEID_month'] - count_unique_month_age['Naive_Forecast'])\n",
    "mad = count_unique_month_age['Absolute_Deviation'].mean()\n",
    "\n",
    "# Display results\n",
    "print(count_unique_month_age)\n",
    "print(f\"MAPE: {mape:.2f}%\")\n",
    "print(f\"MAD: {mad:.2f}\")\n",
    "print(\"\\nPlanning Estimates:\")\n",
    "print(f\"Low Planning Estimate: {count_unique_month_age['Low_Planning_Estimate'].iloc[-1]:.2f}\")\n",
    "print(f\"Central Planning Estimate: {count_unique_month_age['Central_Planning_Estimate'].iloc[-1]:.2f}\")\n",
    "print(f\"High Planning Estimate: {count_unique_month_age['High_Planning_Estimate'].iloc[-1]:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99ce5ae4-fa0c-499e-aaee-9c38e47bd090",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a line plot\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.plot(df_count_unique_month['Count_of_CASEID_month'], df_count_unique_month['Low_Planning_Estimate'], label='Low Estimate', marker='o')\n",
    "plt.plot(df_count_unique_month['Count_of_CASEID_month'], df_count_unique_month['Central_Planning_Estimate'], label='Central Estimate', marker='s')\n",
    "plt.plot(df_count_unique_month['Count_of_CASEID_month'], df_count_unique_month['High_Planning_Estimate'], label='High Estimate', marker='^')\n",
    "\n",
    "# Add labels and title\n",
    "plt.xlabel('Month')\n",
    "plt.ylabel('Sales Estimate')\n",
    "plt.title('Demand Forecasting Estimates')\n",
    "plt.grid(True)\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe089b8b-61ca-424d-b7dd-adb9092ca2b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Number of LPA reciepts\n",
    "lpa_reciepts = lpa_unique\n",
    "# Extract month letter and year \n",
    "lpa_reciepts['month_year'] = lpa_reciepts['receiptdate'].dt.strftime('%b-%y')\n",
    "# Calculate the number of unique records by month and year\n",
    "count_reciepts_month = lpa_reciepts.groupby(['year', 'month_year'])['receiptdate'].nunique().reset_index(name='count')\n",
    "\n",
    "\n",
    "#unique_records = df.groupby('unique_key').agg('count').reset_index()  #.groupby(['year'])['unique_key'].nunique().reset_index(name='count')\n",
    "count_reciepts_month = count_reciepts_month.rename(columns={\"count\": \"Count_of_reciepts_month\"})\n",
    "\n",
    "# Display the result\n",
    "print(count_reciepts_month)\n",
    "\n",
    "# Save the result into a csv file\n",
    "count_reciepts_month.to_csv(r'count_reciepts_month.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c1c8fe0-7a2d-4cd8-bccc-4b302daaaa1c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
