#==============================================================================
# OPG Sirius Data Processing (OPG Analytical)
# @author: Richard Ingley
# MoJ Data Science Hub
#==============================================================================

# OPG Analytical have specified a schema that satisfies most of their 
# requirements for Sirius data cuts. This code to pull that data from Athena, 
# chunk, export to CSV + feather, and push to dedicated warehouse bucket(s). 

import os
import sys
import shutil
import psutil
import pydbtools
import boto3
import numpy
import pandas
import datetime
import uuid
import lovely_logger as log
from arrow_pd_parser import reader, writer

# Unique identifier to tag files
uuid_tag = str(uuid.uuid4())
timestring = str(datetime.datetime.today()).replace(' ','_').replace(':','-').split('.')[0]
name = 'opg-analytical'
test_mode = True

# Clear a tmp directory
shutil.rmtree('tmp',ignore_errors=True)
os.makedirs('tmp', exist_ok=True)

#==============================================================================
# Log
#==============================================================================

try:
    logfile
except NameError:
    # Initialise log
    logfile = 'tmp/{}_{}.log'.format(name,uuid_tag)
    # log.FILE_FORMAT = "[%(asctime)s] [%(levelname)-8s] - %(message)s (%(filename)s:%(lineno)s)"
    log.FILE_FORMAT = "[%(asctime)s] [%(levelname)-8s] %(message)s"
    # log.CONSOLE_FORMAT = "[%(levelname)-8s] - %(message)s (%(filename)s:%(lineno)s)"
    log.CONSOLE_FORMAT = "[%(levelname)-8s] %(message)s"
    log.DATE_FORMAT = '%Y-%m-%d %H:%M:%S.uuu%z'
    log.init(logfile)
    log.info("*** Hello from {}.py ***".format(name))
    log.info("uuid: our tag is [{}]".format(uuid_tag))
    
#==============================================================================
# Pipeline 
#==============================================================================

# Make an array of new local files we're creating
files = []

# Keep track of files successfully written to S3
news3files = []

# For testing limit the number of records pulled (negative number for no limit)
sql_limit = -1

# Leave data where?
bucket_name = 'alpha-opg-analytical'
s3dir = 'sirius_data_cuts_3/'

# Return data for first how many attorneys?
max_attorneys = 4

# Once a multi-field column has been converted to separate columns, shall we keep the original?
keep_parsed_array_fields = False

# For console debugging set data frame display options
pandas.set_option('display.min_rows', 50)
pandas.set_option('display.max_rows', 20)
pandas.set_option('display.width', 1000)
pandas.set_option('display.max_columns', 500)

pandas.options.display.float_format = '{:.1f}'.format

# Resource and buckets
s3c = boto3.client('s3')
s3r = boto3.resource('s3')

#==============================================================================
# 
#==============================================================================

# Report memory footprint of a dataframe and the python process
def memuse(df): 
    mem_df = round(df.memory_usage(deep=True).sum()/(1024**3),1)
    mem_py = round(psutil.Process(os.getpid()).memory_info().rss/(1024**3),1)
    report = 'memory: dataframe is {}GB, process is {}GB'.format(mem_df,mem_py)
    return(report)

# For a SIRIUS nested field in a dataframe, what are the unnested columns? 
# Name them sensibly. 
def split_attorney_fields_2(data, field, max_attorneys):
    
    log.info("[p{}] split_attorney_fields: parsing [{}] array field, first [{}] attorneys, [{}] records".format(page,field,max_attorneys,len(data)))
    
    # e.g. we want 'replacement_attorney_postcodes' to become
    # 'replacement_attorney_1_postcode', 'replacement_attorney_2_postcode', etc.
    new_field_name_head = '{}attorney_'.format(field.split('attorney_',1)[0])
    new_field_name_tail = '_{}'.format(field.split('attorney_',1)[1].replace('s_','_').rstrip('s'))

    # Remove leading and trailing brackets
    df = data[field].str.slice(
        1, -1
        
    # Split into separate columns for each attorney
    ).str.split(
        pat=', ', n = -1, expand=True
        
    # Return only the first N columns. Note this gives 'None' if we run out of attorneys.   
    # An empty string means there was an attorney, but data was missing. 
    ).iloc[
        :, : max_attorneys
        
    # Rename the columns (e.g.: "A1_postcode", "A2_postcode", ... etc.)
    ].rename(
        columns=dict(
            zip(numpy.arange(0,max_attorneys,1), 
                [new_field_name_head + n + new_field_name_tail for n in numpy.arange(1,max_attorneys+1,1).astype(str)])
        )
    )
    
    return(df)

# What do the codes in an NSPL column translate to?
def translate_nspl_codes(data, column, dict_s3bucket, dict_s3key, dict_from_col, dict_to_col, page): 

    # Read translation keys and values from S3 document to data frame
    log.info("translate_nspl_codes: [{}] reading {} to {} dictionary".format(page, dict_from_col,dict_to_col))
#     replace_df = pandas.read_csv('s3://{}/{}'.format(dict_s3bucket,dict_s3key), 
#                          usecols=[dict_from_col, dict_to_col], 
#                          low_memory=False, 
#                          dtype=str)
    replace_df = reader.read(f's3://{dict_s3bucket}/{dict_s3key}', 
                            usecols = [dict_from_col, dict_to_col], 
                            low_memory=False)

    # Convert to dict for series replace
    replace_dict = dict(zip(replace_df[dict_from_col], replace_df[dict_to_col]))

    log.info("translate_nspl_codes: [{}] translating {} codes to names".format(page, dict_from_col))
    names = data[column].replace(to_replace=replace_dict)

    return(names)


# File export handler
def my_export(df, fileformat, filename, page='?'):
    log.info('[p{}] export-data: Writing {} file [{}]'.format(page,fileformat,filename))
    if fileformat == "csv": 
        df.to_csv(filename, sep=',', index = False)
    elif fileformat == "feather":
        feather.write_dataframe(df=df, dest=filename)
    elif fileformat == "xlsx": 
        writer = pandas.ExcelWriter(filename, engine='xlsxwriter')
        df.to_excel(writer, sheet_name='Cases {} (N={})'.format(df['reg_year'].iloc[0],len(df)), index=False)
        writer.save()
    statinfo = os.stat(filename)
    log.info("[p{}] export-data: Done ({}MB)".format(page,round(statinfo.st_size/1024**2)))    
    return(statinfo)

def categorise_fields(df, fields): 
    '''
    Report memory usage while categorising
    '''
    # Drop any fields not in the data
    fields = list(set(fields).intersection(list(df.columns)))
    for field in fields: 
        MB_to_save = round((df[field].memory_usage(deep=True) - 
                            df[field].astype('category').memory_usage(deep=True))/1024**2)
        log.info('categorise_fields: converting {} to categorical to save {}MB'.format(field, MB_to_save))
        df[field] = df[field].astype("category")
    return(df)

#==============================================================================
# NSPL region data
#==============================================================================

## Read NSPL data

nspl_bucket_name = 'alpha-opg-data-processing'
nspl_bucket = s3r.Bucket(nspl_bucket_name)
nspl_key = 'ons/NSPL_FEB_2021_UK/Data/NSPL_FEB_2021_UK.csv'

usecols = ['pcds','oa11','cty','laua','ctry','rgn','lsoa11','msoa11','ccg','ru11ind','oac11','imd']

log.info("pandas: reading NSPL database")
#nspl = pandas.read_csv('s3://' + nspl_bucket_name + '/' + nspl_key, usecols=usecols, low_memory=False)
nspl = reader.read(f's3://{nspl_bucket_name}/{nspl_key}', file_format='csv', usecols=usecols, low_memory=False)

log.info("pandas: renaming NSPL fields")
cols_old = list(nspl.columns)
cols_new = ['donor_nspl_' + col for col in cols_old]
cols_dic = dict(zip(cols_old, cols_new))
nspl = nspl.rename(columns = cols_dic)

log.info("pandas: squashing NSPL postcodes")
nspl['pcdsquish'] = nspl['donor_nspl_pcds'].str.replace(" ","")

#==============================================================================
# Stats Wales WIMD data
#==============================================================================

wimd_bucket_name = 'alpha-opg-data-processing'
wimd_bucket = s3r.Bucket(wimd_bucket_name)
wimd_key = 'govwales/welsh-index-multiple-deprivation-2019-index-and-domain-ranks-by-small-area.ods'
usecols = ['LSOA Code','LSOA Name (Eng)','WIMD 2019 Overall Decile']

log.info("pandas: reading WIMD data")
# wimd = pandas.read_excel('s3://' + wimd_bucket_name + '/' + wimd_key,  
#                          engine="odf", 
#                          sheet_name = 'Deciles_quintiles_quartiles', 
#                          usecols = ['LSOA Code','LSOA Name (Eng)','WIMD 2019 Overall Decile'])
s3r.Object(wimd_bucket_name, wimd_key).download_file(f"tmp/{wimd_key.split('/')[1]}")
wimd = pandas.read_excel(f"tmp/{wimd_key.split('/')[1]}", sheet_name = 'Deciles_quintiles_quartiles', usecols = usecols)

log.info("pandas: renaming WIMD fields")
cols_old = list(wimd.columns)
cols_new = ['donor_wimd_lsoa', 'donor_wimd_lsoa_name', 'donor_wimd_decile']
cols_dic = dict(zip(cols_old, cols_new))
wimd = wimd.rename(columns = cols_dic)

#==============================================================================
# ONS English IMD data
#==============================================================================

imd_bucket_name = 'alpha-opg-data-processing'
imd_bucket = s3r.Bucket(nspl_bucket_name)
imd_key = 'ons/English indices of deprivation 2019 /File_7_-_All_IoD2019_Scores__Ranks__Deciles_and_Population_Denominators_3-1.csv'
usecols = ['LSOA code (2011)','LSOA name (2011)','Index of Multiple Deprivation (IMD) Decile (where 1 is most deprived 10% of LSOAs)']

log.info("pandas: reading IMD data")
# imd = pandas.read_csv('s3://' + imd_bucket_name + '/' + imd_key,  
#                        usecols = ['LSOA code (2011)','LSOA name (2011)','Index of Multiple Deprivation (IMD) Decile (where 1 is most deprived 10% of LSOAs)'])
imd = reader.read(f's3://{imd_bucket_name}/{imd_key}', file_format='csv', usecols=usecols, low_memory=False)
log.info("pandas: renaming IMD fields")
cols_old = list(imd.columns)
cols_new = ['donor_imd_lsoa', 'donor_imd_lsoa_name', 'donor_imd_decile']
cols_dic = dict(zip(cols_old, cols_new))
imd = imd.rename(columns = cols_dic)

#==============================================================================
# TransUnion CAMEO data
#==============================================================================

cameo_bucket_name = 'alpha-opg-data-processing'
cameo_bucket = s3r.Bucket(cameo_bucket_name)
cameo_key = 'transunion/cameo/CAMEO Analysis.csv'

log.info("pandas: reading CAMEO data")
usecols = ["Pcd", "CAMEO_UKP", "CAMEO_UKPG", "Age_Score", "Age_Band", 
           "Tenr_Score", "Tenr_Band", "Comp_Score", "Comp_Band", 
           "Econ_Score", "Econ_Band", "Life_Score", "Life_Band", 
           "CAMEOINTL"]
# cameo = pandas.read_csv('s3://' + cameo_bucket_name + '/' + cameo_key,  
#                        usecols = usecols)
cameo = reader.read(f's3://{cameo_bucket_name}/{cameo_key}', file_format='csv', usecols=usecols, low_memory=False)
log.info("pandas: squashing CAMEO postcodes")
cameo['pcdsquish'] = cameo['Pcd'].str.replace(" ","")

log.info("pandas: renaming CAMEO fields")
cols_old = list(cameo.columns)
cols_new = ['cameo_' + col for col in cols_old]
cols_dic = dict(zip(cols_old, cols_new))
cameo = cameo.rename(columns = cols_dic)

#==============================================================================
# Select glue exports
#==============================================================================

# Database sirius_stacked has all historical exports piled on top of 
# each other, discerned by glueexporteddate for each table. 

# Choose a version of the source data 
# target_date = '2022-01-28' # YYYY-MM-DD, or: 

# Choose the most recent available source data
target_date = pydbtools.read_sql_query('SELECT max(glueexporteddate) AS glueexporteddate FROM opg_sirius_prod.cases')['glueexporteddate'][0]
# target_date = target_date.split('T')[0]
log.info('pydbtools: Last glue export date seems to be {}'.format(target_date))

# What is the most recent glue export datetime for each table of interest?
glueexporteddates = {}
tables = ['cases', 'persons', 'addresses', 'person_caseitem']
log.info('pydbtools: looking for contemporary glue exports of {} tables ...'.format(len(tables)))
for table in tables: 
    query = f"SELECT MAX(glueexporteddate) AS glueexporteddate FROM opg_sirius_prod.{table} WHERE glueexporteddate <= DATE('{target_date}')"
    glueexporteddate = pydbtools.read_sql_query(query)['glueexporteddate'][0]
    log.info("pydbtools: using '{}' table glue export from {} in Sirius".format(table, glueexporteddate))
    glueexporteddates[table] = glueexporteddate

    
# For testing these dates will work 
example_working_glueexporteddates = {'cases': datetime.date(2024, 2, 1),
                                     'persons': datetime.date(2024, 2, 1),
                                     'addresses': datetime.date(2024, 2, 1),
                                     'person_caseitem': datetime.date(2024, 2, 1)}
# glueexporteddates = example_working_glueexporteddates

#==============================================================================
# Select attorney fields
#==============================================================================

## Which attorney/replacement attorney data do we want?

# SQL query varies depending on how many attorneys we want to recover data for 
# (max_attorneys), and which fields have been requested. Dictionary 'sql_subs' 
# below contains substrings of SQL to pass to Athena e.g. attorney ids in as many 
# fields as needed. 

# Make a dictionary with values containing SQL substrings which can be dropped 
# into our Athena query, e.g. TRY(<table>.attorney_dobs[N]) for N = 1 to 4). 
# We need TRY() in case there aren't that many attorneys for any specific case. 
# We cast to varchar to stop crazy dates in Sirius breaking Athena. Max 
# attorneys is set at top in pipeline config. 

attorney_indices = list(range(1, max_attorneys + 1))       # e.g. [1,2,3,4]
sql_subs = {}

# Attorney data 
for feature in ['dobs', 'postcodes']: 
    sql_subs['a_{}'.format(feature)] = ", ".join(['TRY(CAST(attorneys.{}[{}] as varchar)) AS A{}_{}'.format(feature,n,n,feature[:-1]) for n in attorney_indices])

# Replacement attorney data 
for feature in ['dobs']: 
    sql_subs['r_{}'.format(feature)] = ", ".join(['TRY(CAST(replacement_attorneys.{}[{}] as varchar)) AS R{}_{}'.format(feature,n,n,feature[:-1]) for n in attorney_indices])

#==============================================================================
# Request to add cert provider data
#==============================================================================

## Use opg_sirius_prod temp tables

def create_temp_table_ranked_person_address(glueexportdate): 
    
    '''
    Some (~30K) persons have more than one address attached to them. Sirius
    only uses the first one. Rank addresses for each person in the addresses 
    table.
    '''
    
    name = f"temp_ranked_addresses"
    
    q = (f"""
        SELECT
            ROW_NUMBER() OVER (PARTITION BY addresses.person_id ORDER BY addresses.id ASC) AS rank,
            addresses.*
        FROM opg_sirius_prod.addresses
        WHERE addresses.glueexporteddate = DATE('{glueexportdate}')
    """)
    
    q = ' '.join(q.split())
    
    pydbtools.create_temp_table(q, name)
    
    return name

def create_temp_table_ranked_uid_actor(actor_type, glueexportdate): 
    
    '''
    Rank certificate providers for each case in the person_caseitem table
    '''
    
    name = f"temp_ranked_{actor_type}"
    
    q = (f"""
        SELECT
        cases.uid as case_uid, 
        ROW_NUMBER() OVER (PARTITION BY cases.uid ORDER BY persons.id ASC) AS rank,
        persons.*

        FROM opg_sirius_prod.cases

        LEFT JOIN opg_sirius_prod.person_caseitem
        ON person_caseitem.glueexporteddate = DATE('{glueexportdate}')
        AND person_caseitem.caseitem_id = cases.id

        RIGHT JOIN opg_sirius_prod.persons
        ON persons.glueexporteddate = DATE('{glueexportdate}')
        AND persons.id = person_caseitem.person_id
        AND persons.type = '{actor_type}'

        WHERE cases.glueexporteddate = DATE('{glueexportdate}')
    """)
    
    q = ' '.join(q.split())
    
    pydbtools.create_temp_table(q, name)
    
    return name

# Rank addresses for each person in the addresses table
# gdate = glueexporteddates['cases'].split('T')[0]
gdate = glueexporteddates['cases'].strftime('%Y-%m-%d')
temp_table_adr_ranked = create_temp_table_ranked_person_address(gdate)
# Rank certificate providers for each case in the person_caseitem table
temp_cp_table = create_temp_table_ranked_uid_actor('lpa_certificate_provider', gdate)
    
#==============================================================================
# Configure SQL paging
#==============================================================================

## All the data at once exceeds our RAM limit. Page everything from here 
## until we can move all logic into Athena

# How many pages shall we divide work over?
n_pages = 10

# Treat SIRIUS UID as primary key. Categorise UIDs by page number.
log.info('pydbtools: requesting SIRIUS UIDs ...')
query = (f"""
         SELECT uid 
         FROM opg_sirius_prod.cases 
         WHERE cases.glueexporteddate = DATE('{glueexporteddates['cases']}')
         """)
query = ' '.join(query.split())
uids = pydbtools.read_sql_query(query)
log.info('page_config: discretising {} UIDs into {} pages'.format(len(uids),n_pages))
uids['uid'] = pandas.to_numeric(uids['uid'])
uids['page'] = pandas.qcut(uids['uid'], n_pages, labels = False)

for page in range(n_pages): 
    uid_min = uids.loc[uids['page'] == page, 'uid'].min()
    uid_max = uids.loc[uids['page'] == page, 'uid'].max()
    log.info('page_config: starting page {} UIDs {} - {} (N={})'.format(page,uid_min,uid_max,len(uids.loc[uids['page'] == page])))
    
    #==============================================================================
    # Read Athena data
    #==============================================================================

    query = (f"""

    WITH 
        attorneys AS 
        (SELECT 
            person_caseitem.caseitem_id AS caseitem_id, 
            COALESCE(CARDINALITY(ARRAY_AGG(persons.id)),0) AS count, 
            ARRAY_AGG(persons.dob) AS dobs, 
            ARRAY_AGG(addresses.postcode) AS postcodes
        FROM opg_sirius_prod.person_caseitem person_caseitem
        INNER JOIN opg_sirius_prod.persons persons
            ON persons.id = person_caseitem.person_id
            AND persons.type = 'lpa_attorney'
            AND persons.glueexporteddate = DATE('{glueexporteddates['persons']}')
        LEFT JOIN opg_sirius_prod.addresses addresses
            ON addresses.person_id = persons.id
            AND addresses.glueexporteddate = DATE('{glueexporteddates['addresses']}')
        WHERE person_caseitem.glueexporteddate = DATE('{glueexporteddates['person_caseitem']}')
        GROUP BY person_caseitem.caseitem_id), 

        replacement_attorneys AS
        (SELECT 
            person_caseitem.caseitem_id AS caseitem_id, 
            COALESCE(CARDINALITY(ARRAY_AGG(persons.id)),0) AS count, 
            ARRAY_AGG(persons.dob) AS dobs
        FROM opg_sirius_prod.person_caseitem person_caseitem
        INNER JOIN opg_sirius_prod.persons persons
            ON persons.id = person_caseitem.person_id
            AND persons.type = 'lpa_replacement_attorney'
            AND persons.glueexporteddate = DATE('{glueexporteddates['persons']}')
        WHERE person_caseitem.glueexporteddate = DATE('{glueexporteddates['person_caseitem']}')
        GROUP BY person_caseitem.caseitem_id)

    SELECT
        CAST(cases.uid AS varchar) AS uid, 
        cases.type AS type, 
        cases.casesubtype AS casesubtype, 
        cases.status AS status, 
        CAST(cases.receiptdate AS varchar) AS receiptdate, 
        CAST(cases.registrationdate AS varchar) AS registrationdate,
        CAST(cases.lpadonorsignaturedate AS varchar) AS lpadonorsignaturedate, 
        CASE WHEN cases.applicationtype = 0 THEN 'Classic' ELSE 'Online' END AS applicationtype, 
        cases.repeatapplication,     
        cases.attorneyactdecisions, 

        CASE
            WHEN cases.haveappliedforfeeremission = 0 THEN 'Not Set'
            WHEN cases.haveappliedforfeeremission = 1 THEN 'False'
            WHEN cases.haveappliedforfeeremission = 2 THEN 'True'
            ELSE null END AS haveappliedforfeeremission, 

        CASE
            WHEN cases.paymentremission = 0 THEN 'Not Set' 
            WHEN cases.paymentremission = 1 THEN 'False'
            WHEN cases.paymentremission = 2 THEN 'True'
            ELSE null END AS paymentremission, 

        CASE
            WHEN cases.paymentexemption = 0 THEN 'Not Set'
            WHEN cases.paymentexemption = 1 THEN 'False'
            WHEN cases.paymentexemption = 2 THEN 'True'
            ELSE null END AS paymentexemption, 

        CASE 
            WHEN cases.paymentbycheque = 0 THEN 'Not Set' 
            WHEN cases.paymentbycheque = 1 THEN 'False' 
            WHEN cases.paymentbycheque = 2 THEN 'True' 
            ELSE null END AS paymentbycheque, 

        cases.lifesustainingtreatment, 

        CASE 
            WHEN cases.caseattorneysingular AND NOT cases.caseattorneyjointly AND NOT cases.caseattorneyjointlyandseverally AND NOT cases.caseattorneyjointlyandjointlyandseverally THEN 'S' 
            WHEN cases.caseattorneyjointly AND NOT cases.caseattorneysingular AND NOT cases.caseattorneyjointlyandseverally AND NOT cases.caseattorneyjointlyandjointlyandseverally THEN 'J' 
            WHEN cases.caseattorneyjointlyandseverally AND NOT cases.caseattorneysingular AND NOT cases.caseattorneyjointly AND NOT cases.caseattorneyjointlyandjointlyandseverally THEN 'JS' 
            WHEN cases.caseattorneyjointlyandjointlyandseverally AND NOT cases.caseattorneysingular AND NOT cases.caseattorneyjointly AND NOT cases.caseattorneyjointlyandseverally THEN 'JJS' 
            ELSE null END AS how_attorneys_appointed, 

        CASE 
            WHEN cases.applicationhasguidance AND NOT cases.applicationhasrestrictions THEN 'PREF' 
            WHEN cases.applicationhasrestrictions AND NOT cases.applicationhasguidance THEN 'INST' 
            WHEN cases.applicationhasguidance AND cases.applicationhasrestrictions THEN 'BOTH' 
            WHEN NOT cases.applicationhasguidance AND NOT cases.applicationhasrestrictions THEN 'NONE' 
            ELSE null END AS instructions_or_preferences,  

        CAST(donors.dob AS varchar) AS donor_dob,
        LOWER(donors.salutation) AS donor_salutation, 
        add_d.postcode AS donor_postcode,

--        CASE WHEN 
--            LOWER(add_c.address_lines) LIKE '% solicitor%' 
--                OR LOWER(add_c.address_lines) LIKE '% llp%'  
--                OR LOWER(add_c.address_lines) LIKE '% ltd%' 
--                OR LOWER(add_c.address_lines) LIKE '% limited%' 
--                OR LOWER(add_c.address_lines) LIKE '% and co%' 
--                OR LOWER(add_c.address_lines) LIKE '% & co%' 
--                OR LOWER(add_c.address_lines) LIKE '% partners%' 
--                OR LOWER(add_c.address_lines) LIKE '% associates%' 
--                OR LOWER(add_c.address_lines) LIKE '% legal%' 
--                OR LOWER(add_c.address_lines) LIKE '% law %' 
--                OR correspondents.companyname != ''
--            THEN True
--        ELSE False END AS professional_correspondent, 

        COALESCE(attorneys.count,0) AS attorney_count, 
        COALESCE(replacement_attorneys.count,0) AS replacement_attorney_count, 

        {', '.join(sql_subs.values())}, 
        
        cases.glueexporteddate AS cases_glueexporteddate, 
        
        TRY(LOWER(cp1.firstname)) AS cp1_firstname, 
        TRY(LOWER(cp1.surname)) AS cp1_surname, 
        LOWER(TRY(cp1adr.address_lines[1])) AS cp1_adr_lines_1,
        LOWER(TRY(cp1adr.address_lines[2])) AS cp1_adr_lines_2,
        LOWER(TRY(cp1adr.address_lines[3])) AS cp1_adr_lines_3, 
        cp1adr.postcode AS cp1_pcode,
        
        TRY(LOWER(cp2.firstname)) AS cp2_firstname, 
        TRY(LOWER(cp2.surname)) AS cp2_surname, 
        LOWER(TRY(cp2adr.address_lines[1])) AS cp2_adr_lines_1,
        LOWER(TRY(cp2adr.address_lines[2])) AS cp2_adr_lines_2,
        LOWER(TRY(cp2adr.address_lines[3])) AS cp2_adr_lines_3, 
        cp2adr.postcode AS cp2_pcode
        
    FROM opg_sirius_prod.cases

    LEFT JOIN opg_sirius_prod.persons donors 
        ON donors.id = cases.donor_id
        AND donors.glueexporteddate = DATE('{glueexporteddates['persons']}')
    LEFT JOIN opg_sirius_prod.persons correspondents
        ON correspondents.id = cases.correspondent_id
        AND correspondents.glueexporteddate = DATE('{glueexporteddates['persons']}')
    LEFT JOIN opg_sirius_prod.addresses add_d
        ON add_d.person_id = cases.donor_id
        AND add_d.glueexporteddate = DATE('{glueexporteddates['addresses']}')
    LEFT JOIN opg_sirius_prod.addresses add_c
        ON add_c.person_id = cases.correspondent_id 
        AND add_c.glueexporteddate = DATE('{glueexporteddates['addresses']}')
    LEFT JOIN attorneys
        ON attorneys.caseitem_id = cases.id
    LEFT JOIN replacement_attorneys
        ON replacement_attorneys.caseitem_id = cases.id
        
    LEFT JOIN {temp_cp_table} cp1
    ON cp1.rank = 1
    AND cp1.case_uid = cases.uid
    
    LEFT JOIN {temp_cp_table} cp2
    ON cp2.rank = 2
    AND cp2.case_uid = cases.uid
    
    LEFT JOIN {temp_table_adr_ranked} cp1adr
    ON cp1adr.rank = 1
    AND cp1adr.person_id = cp1.id
    
    LEFT JOIN {temp_table_adr_ranked} cp2adr
    ON cp2adr.rank = 1
    AND cp2adr.person_id = cp2.id

    WHERE cases.glueexporteddate = DATE('{glueexporteddates['cases']}')
        AND CAST(cases.uid AS BIGINT) BETWEEN {uid_min} AND {uid_max} 
        AND cases.type IN ('lpa','epa')

    {'LIMIT ' + str(sql_limit) if sql_limit >= 0 else ''}
    """)

    # Query whitespace
    # query = " ".join(query.split())

    log.info("[p{}] pydbtools: our SQL query is: '{}'".format(page,query))

    # Send query to Athena, capture response in a pandas data frame
    if "LIMIT" in query: log.warning("pydbtools: *** SQL LIMIT IMPOSED [{}] ***".format(sql_limit))
    log.info("[p{}] pydbtools: requesting data ... ".format(page))
    cases = pydbtools.read_sql_query(query) 
    log.info("[p{}] pydbtools: received {} records".format(page,cases.index.max()))

    #==============================================================================
    # Cleaning: Categorise
    #==============================================================================

    categoricals = ['type','casesubtype','status', 'applicationtype', 
                    'attorneyactdecisions', 'haveappliedforfeeremission', 
                    'paymentremission', 'paymentexemption', 'paymentbycheque', 
                    'lifesustainingtreatment', 'how_attorneys_appointed', 
                    'instructions_or_preferences']

    cases = categorise_fields(cases, categoricals)

    #==============================================================================
    # Donor gender tagging 
    #==============================================================================

    log.info('[p{}] pandas: tagging donor genders'.format(page))

    cases['donor_salutation'] = cases['donor_salutation'].str.lower()

    # Specify gender specific titles to tag with a gender
    titles_f = ['mrs','miss','ms','missus','mistress','baroness', 'bness', 'countess', 'dame', 'lady', 'mm', 'mme', 'madam', 'madame', 'sister', 'viscountess', 'rev mrs']
    titles_m = ['mr','master','baron', 'earl', 'esq', 'father', 'his honour', 'lord', 'sir','viscount']

    # Tag everything as unknown initially, then match customer desired tags
    cases['donor_gender'] = 'Other'
    cases.loc[cases['donor_salutation'].isin(titles_m), 'donor_gender'] = 'Male'
    cases.loc[cases['donor_salutation'].isin(titles_f), 'donor_gender'] = 'Female'
    cases.loc[cases['donor_salutation'].isnull(), 'donor_gender'] = numpy.NaN
    cases.loc[cases['donor_salutation'] == '', 'donor_gender'] = numpy.NaN

    # Convert to categorical
    cases['donor_gender'] = cases['donor_gender'].astype('category')
    
    # Drop salutations
    cases = cases.drop(columns = 'donor_salutation')

    #==============================================================================
    # Cleaning: Dates
    #==============================================================================

    for field in ["receiptdate","registrationdate","lpadonorsignaturedate"]: 
        log.info("[p{}] pandas: converting {} to datetimes".format(page, field))
        cases[field] = pandas.to_datetime(cases[field], errors = 'coerce')

    for field in ["cases_glueexporteddate"]:
        log.info("[p{}] pandas: converting {} to dates".format(page, field))
        cases[field] = pandas.to_datetime(cases[field], errors = 'coerce').dt.date
      
    #==============================================================================
    # Merge with NSPL data
    #==============================================================================

    log.info("[p{}] pandas: squishing SIRIUS postcodes".format(page))
    cases['pcdsquish'] = cases['donor_postcode'].str.replace(" ","")

    log.info("[p{}] pandas: merging NSPL and SIRIUS databases".format(page))
    cases = cases.merge(nspl, on='pcdsquish', how='left')
    n_no_nspl_match = len(cases.loc[cases['donor_nspl_pcds'].isnull()])
    log.info('[p{}] pandas: failed to match {} records ({}%) to NSPL data'.format(page, n_no_nspl_match, round(100*n_no_nspl_match/len(cases),1)))
    cases = cases.drop(['donor_nspl_pcds'], axis = 1)
    
    # Make new fields categorical
    categoricals = ['donor_nspl_rgn','donor_nspl_laua','donor_nspl_ctry',
                    'donor_nspl_ccg','donor_nspl_oac11']
    cases = categorise_fields(cases, categoricals)

    # Regions and country pseudo-regions
    cases['donor_nspl_rgn_name'] = translate_nspl_codes(data = cases, 
                                                        column = 'donor_nspl_rgn', 
                                                        dict_s3bucket = nspl_bucket_name, 
                                                        dict_s3key = 'ons/NSPL_FEB_2021_UK/Documents/Region names and codes EN as at 12_10 (GOR).csv', 
                                                        dict_from_col = 'GOR10CD', dict_to_col = 'GOR10NM', page = page)

    # Local authority / unitary authority
    cases['donor_nspl_laua_name'] = translate_nspl_codes(data = cases, 
                                                         column = 'donor_nspl_laua', 
                                                         dict_s3bucket = nspl_bucket_name, 
                                                         dict_s3key = 'ons/NSPL_FEB_2021_UK/Documents/LA_UA names and codes UK as at 04_20.csv', 
                                                         dict_from_col = 'LAD20CD', dict_to_col = 'LAD20NM', page = page)

    # UK Country
    cases['donor_nspl_ctry_name'] = translate_nspl_codes(data = cases, 
                                                         column = 'donor_nspl_ctry', 
                                                         dict_s3bucket = nspl_bucket_name, 
                                                         dict_s3key = 'ons/NSPL_FEB_2021_UK/Documents/Country names and codes UK as at 08_12.csv', 
                                                         dict_from_col = 'CTRY12CD', dict_to_col = 'CTRY12NM', page = page)

    # Clinical commissioning group or national equivalent
    cases['donor_nspl_ccg_name'] = translate_nspl_codes(data = cases, 
                                                        column = 'donor_nspl_ccg', 
                                                        dict_s3bucket = nspl_bucket_name, 
                                                        dict_s3key = 'ons/NSPL_FEB_2021_UK/Documents/CCG names and codes UK as at 04_20.csv', 
                                                        dict_from_col = 'CCG20CD', dict_to_col = 'CCG20NM', 
                                                        page = page)

    # OAC11 supergroups, groups and subgroups
    for to_col in ['Supergroup','Group','Subgroup']: 
        cases['donor_nspl_oac11_{}'.format(to_col.lower())] = translate_nspl_codes(data = cases, 
                                                             column = 'donor_nspl_oac11', 
                                                             dict_s3bucket = nspl_bucket_name, 
                                                             dict_s3key = 'ons/NSPL_FEB_2021_UK/Documents/2011 Census Output Area Classification Names and Codes UK.csv', 
                                                             dict_from_col = 'OAC11', dict_to_col = to_col, 
                                                                                   page = page)

    # All new fields should be categories to save memory 
    categories = ['donor_nspl_oac11', 'donor_nspl_cty',
                  'donor_nspl_laua', 'donor_nspl_ctry',
                  'donor_nspl_rgn', 'donor_nspl_lsoa11',
                  'donor_nspl_msoa11', 'donor_nspl_ccg',
                  'donor_nspl_ru11ind', 'donor_nspl_oac11', 
                  'donor_nspl_rgn_name', 'donor_nspl_laua_name', 
                  'donor_nspl_ctry_name', 'donor_nspl_ccg_name'] + ['donor_nspl_oac11_{}'.format(to_col.lower()) for to_col in ['Supergroup','Group','Subgroup']]
    cases = categorise_fields(cases, categories)


    #==============================================================================
    # Merge with WIMD dataframe
    #==============================================================================

    log.info("[p{}] pandas: merging SIRIUS and WIMD data".format(page))
    cases = cases.merge(wimd, left_on='donor_nspl_lsoa11', right_on = 'donor_wimd_lsoa', how='left')
    n_wimd_match = len(cases.loc[~cases['donor_wimd_lsoa'].isnull()])
    log.info('[p{}] pandas: matched {} records ({}%) to WIMD data'.format(page, n_wimd_match, 
                                                                    round(100*n_wimd_match/len(cases),1)))
    cases = cases.drop(['donor_wimd_lsoa','donor_wimd_lsoa_name'], axis = 1)

    # wip 
    
    #==============================================================================
    # Merge with IMD dataframe
    #==============================================================================
    
    log.info("[p{}] pandas: merging SIRIUS and IMD data".format(page))
    cases = cases.merge(imd, left_on='donor_nspl_lsoa11', right_on = 'donor_imd_lsoa', how='left')
    n_imd_match = len(cases.loc[~cases['donor_imd_lsoa'].isnull()])
    log.info('[p{}] pandas: matched {} records ({}%) to IMD data'.format(page,
                                                                          n_imd_match,
                                                                    round(100*n_imd_match/len(cases),1)))
    cases = cases.drop(['donor_imd_lsoa','donor_imd_lsoa_name'], axis = 1)
    
    #==============================================================================
    # Merge with CAMEO dataframe
    #==============================================================================
    
    log.info("[p{}] pandas: merging SIRIUS and CAMEO data".format(page))
    cases = cases.merge(cameo, left_on='pcdsquish', right_on = 'cameo_pcdsquish', how='left')
    n_cameo_match = len(cases.loc[~cases['cameo_Pcd'].isnull()])
    log.info('[p{}] pandas: matched {} records ({}%) to CAMEO data'.format(page,
                                                                          n_cameo_match, 
                                                                          round(100*n_cameo_match/len(cases),1)))
    
    cases = cases.drop(['cameo_Pcd','cameo_pcdsquish'], axis = 1)
    log.info("[p{}] {}".format(page,memuse(cases)))
    
    # Translate field codes
        
    cases['cameo_CAMEO_UKP_name'] = translate_nspl_codes(data = cases, 
                                                    column = 'cameo_CAMEO_UKP', 
                                                    dict_s3bucket = cameo_bucket_name, 
                                                    dict_s3key = 'transunion/cameo/CAMEO_UKP_category_names.csv', 
                                                    dict_from_col = 'CAMEO UK Category Code', dict_to_col = 'CAMEO UK Type', 
                                                    page = page)
    
    log.info("[p{}] {}".format(page,memuse(cases)))
    
    cases['cameo_CAMEO_UKPG_name'] = translate_nspl_codes(data = cases, 
                                                column = 'cameo_CAMEO_UKPG', 
                                                dict_s3bucket = cameo_bucket_name, 
                                                dict_s3key = 'transunion/cameo/CAMEO_UKPG_group_names.csv', 
                                                dict_from_col = 'CAMEO UK Group Code', dict_to_col = 'CAMEO UK Type', 
                                                page = page)
    
    log.info("[p{}] {}".format(page,memuse(cases)))
    
    for to_col in ['group','type']: 
        cases['cameo_CAMEOINTL_{}'.format(to_col.lower())] = translate_nspl_codes(data = cases, 
                                                             column = 'cameo_CAMEOINTL', 
                                                             dict_s3bucket = cameo_bucket_name, 
                                                             dict_s3key = 'transunion/cameo/CAMEOINTL_group_type_names.csv', 
                                                             dict_from_col = 'CAMEO International Code', dict_to_col = to_col, page = page)
        
    log.info("[p{}] {}".format(page,memuse(cases)))

    # Rename with donor prefix in case we add matches for other actors later
    cols_old = [col for col in list(cases.columns) if col.startswith('cameo_')]
    cols_new = ['donor_' + col for col in cols_old]
    cols_dic = dict(zip(cols_old, cols_new))
    cases = cases.rename(columns = cols_dic)
        
    #==============================================================================
    # Drop discolsive/unneeded fields
    #==============================================================================
        
    cases = cases.drop(['pcdsquish'], axis = 1)
    
    #==============================================================================
    # Export to local files
    #==============================================================================

    # Export data to required formats 
    
    date_tag_cases = 'S' + glueexporteddates['cases'].strftime('%Y-%m-%d')
    date_tag_pipeline = 'P' + datetime.datetime.now().strftime("%Y-%m-%d")
    
    for fileformat in ["csv"]:      
        filename = "tmp/{}_cases_{}_{}_{}_part-{}.{}".format(name,date_tag_pipeline, date_tag_cases, uuid_tag,str(page+1).zfill(2),fileformat)
        my_export(cases, fileformat, filename, page)
        files.append(filename)
        
    log.info("[p{}] {}".format(page,memuse(cases)))
    
    log.info('page_config: finished page {} UIDs {} - {} (N={})'.format(page,uid_min,uid_max,len(uids.loc[uids['page'] == page])))
    
#==============================================================================
# Push local files to bucket
#==============================================================================

# Send everything to analytical bucket. 

for filename in files:
    s3file = s3dir + filename.split("tmp/")[1]
    log.info("s3upload: writing CSV to S3 [" + bucket_name + '/' + s3file + "]")
    
    try: 
        response = s3r.Object(bucket_name,s3file).put(Body=open(filename, 'rb'))
        if('ResponseMetadata' in response): 
            if('HTTPStatusCode' in response['ResponseMetadata']): 
                if(response['ResponseMetadata']['HTTPStatusCode'] == 200): 
                    log.info('s3upload: HTTPStatusCode was 200')
                    s3filesize = s3r.Bucket(bucket_name).Object(s3file).content_length
                    if (s3filesize == os.stat(filename).st_size): 
                        log.info('s3upload: S3 and local file sizes match, assuming good upload')
                        news3files.append(s3file)
                    else:
                        log.error('s3upload: S3 and local file size mismatch')
                        sys.exit(500)
                else: 
                    log.error('s3upload: bad HTTPStatusCode')
                    sys.exit(501)
            else: 
                log.error('s3upload: No HTTPStatusCode')
                sys.exit(502)
        else: 
            log.error('s3upload: No ResponseMetadata')
            sys.exit(503)
    except: 
        log.error('s3upload: some unknown problem uploading file to S3')
        sys.exit(504)


#==============================================================================
# Remove old files from bucket
#==============================================================================

if(len(news3files) > 0): 
    log.info("cleanup: Clearing deprecated files from '{}/{}'".format(bucket_name, s3dir))
    bucket = s3r.Bucket(bucket_name)
    s3files = list(bucket.objects.filter(Prefix=s3dir))
    log.info("cleanup: Checking {} files".format(len(s3files)))
    
    for file in s3files:
        if (file.key != s3dir and 
            file.key not in news3files): 
            log.info("cleanup: Deleting deprecated file '{}' from bucket".format(file.key))
            file.delete()
        else:
            log.info("cleanup: Leaving '{}' in bucket".format(file.key))
else: 
    log.warning('cleanup: No good uploads; not touching existing files')

#==============================================================================
# Sign off and upload log
#==============================================================================
s3logfile = s3dir + logfile.split("tmp/")[1]
log.info("boto3: writing log to S3 [" + bucket_name + '/' + s3logfile + "]")
log.info("*** Bye from {}.py ***".format(name))
s3r.Object(bucket_name,s3logfile).put(Body=open(logfile, 'rb'))

#==============================================================================
# Scratch ... delete before deploy
#==============================================================================

# ...